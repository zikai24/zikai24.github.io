

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="肘子开">
  <meta name="keywords" content="">
  
    <meta name="description" content="12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210">
<meta property="og:type" content="article">
<meta property="og:title" content="SwinTransformer">
<meta property="og:url" content="http://example.com/2023/03/27/SwinTransformer/index.html">
<meta property="og:site_name" content="肘子开的博客">
<meta property="og:description" content="12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-03-27T03:55:05.000Z">
<meta property="article:modified_time" content="2023-03-29T06:40:19.001Z">
<meta property="article:author" content="肘子开">
<meta name="twitter:card" content="summary_large_image">
  
  
  <title>SwinTransformer - 肘子开的博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/nnfx-dark.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>肘子开的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg/bg2.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="SwinTransformer">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2023-03-27 11:55" pubdate>
        2023年3月27日 中午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      21k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      177 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">SwinTransformer</h1>
            
            <div class="markdown-body">
              <figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># --------------------------------------------------------</span><br><span class="hljs-comment"># Swin Transformer</span><br><span class="hljs-comment"># Copyright (c) 2021 Microsoft</span><br><span class="hljs-comment"># Licensed under The MIT License [see LICENSE for details]</span><br><span class="hljs-comment"># Written by Ze Liu</span><br><span class="hljs-comment"># --------------------------------------------------------</span><br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.utils.checkpoint <span class="hljs-keyword">as</span> checkpoint<br><span class="hljs-keyword">from</span> timm.models.layers <span class="hljs-keyword">import</span> DropPath, to_2tuple, trunc_normal_<br><br><span class="hljs-keyword">try</span>:<br>    <span class="hljs-keyword">import</span> os, sys<br><br>    kernel_path = os.path.abspath(os.path.join(<span class="hljs-string">&#x27;..&#x27;</span>))<br>    sys.path.append(kernel_path)<br>    <span class="hljs-keyword">from</span> kernels.window_process.window_process <span class="hljs-keyword">import</span> WindowProcess, WindowProcessReverse<br><br><span class="hljs-keyword">except</span>:<br>    WindowProcess = <span class="hljs-literal">None</span><br>    WindowProcessReverse = <span class="hljs-literal">None</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;[Warning] Fused window process have not been installed. Please refer to get_started.md for installation.&quot;</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Mlp</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_features, hidden_features=<span class="hljs-literal">None</span>, out_features=<span class="hljs-literal">None</span>, act_layer=nn.GELU, drop=<span class="hljs-number">0.</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        out_features = out_features <span class="hljs-keyword">or</span> in_features<br>        hidden_features = hidden_features <span class="hljs-keyword">or</span> in_features<br>        self.fc1 = nn.Linear(in_features, hidden_features)<br>        self.act = act_layer()<br>        self.fc2 = nn.Linear(hidden_features, out_features)<br>        self.drop = nn.Dropout(drop)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.fc1(x)<br>        x = self.act(x)<br>        x = self.drop(x)<br>        x = self.fc2(x)<br>        x = self.drop(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">window_partition</span>(<span class="hljs-params">x, window_size</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        x: (B, H, W, C)</span><br><span class="hljs-string">        window_size (int): window size</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        windows: (num_windows*B, window_size, window_size, C)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    B, H, W, C = x.shape<br>    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)<br>    windows = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>).contiguous().view(-<span class="hljs-number">1</span>, window_size, window_size, C)<br>    <span class="hljs-keyword">return</span> windows<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">window_reverse</span>(<span class="hljs-params">windows, window_size, H, W</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        windows: (num_windows*B, window_size, window_size, C)</span><br><span class="hljs-string">        window_size (int): Window size</span><br><span class="hljs-string">        H (int): Height of image</span><br><span class="hljs-string">        W (int): Width of image</span><br><span class="hljs-string">    Returns:</span><br><span class="hljs-string">        x: (B, H, W, C)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    B = <span class="hljs-built_in">int</span>(windows.shape[<span class="hljs-number">0</span>] / (H * W / window_size / window_size))<br>    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -<span class="hljs-number">1</span>)<br>    x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>).contiguous().view(B, H, W, -<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WindowAttention</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; Window based multi-head self attention (W-MSA) module with relative position bias.</span><br><span class="hljs-string">    It supports both of shifted and non-shifted window.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dim (int): Number of input channels.</span><br><span class="hljs-string">        window_size (tuple[int]): The height and width of the window.</span><br><span class="hljs-string">        num_heads (int): Number of attention heads.</span><br><span class="hljs-string">        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True</span><br><span class="hljs-string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set</span><br><span class="hljs-string">        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0</span><br><span class="hljs-string">        proj_drop (float, optional): Dropout ratio of output. Default: 0.0</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, window_size, num_heads, qkv_bias=<span class="hljs-literal">True</span>, qk_scale=<span class="hljs-literal">None</span>, attn_drop=<span class="hljs-number">0.</span>, proj_drop=<span class="hljs-number">0.</span></span>):<br><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dim = dim<br>        self.window_size = window_size  <span class="hljs-comment"># Wh, Ww</span><br>        self.num_heads = num_heads<br>        head_dim = dim // num_heads<br>        self.scale = qk_scale <span class="hljs-keyword">or</span> head_dim ** -<span class="hljs-number">0.5</span><br><br>        <span class="hljs-comment"># define a parameter table of relative position bias</span><br>        self.relative_position_bias_table = nn.Parameter(<br>            torch.zeros((<span class="hljs-number">2</span> * window_size[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>) * (<span class="hljs-number">2</span> * window_size[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span>), num_heads))  <span class="hljs-comment"># 2*Wh-1 * 2*Ww-1, nH</span><br><br>        <span class="hljs-comment"># get pair-wise relative position index for each token inside the window</span><br>        coords_h = torch.arange(self.window_size[<span class="hljs-number">0</span>])<br>        coords_w = torch.arange(self.window_size[<span class="hljs-number">1</span>])<br>        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  <span class="hljs-comment"># 2, Wh, Ww</span><br>        coords_flatten = torch.flatten(coords, <span class="hljs-number">1</span>)  <span class="hljs-comment"># 2, Wh*Ww</span><br>        relative_coords = coords_flatten[:, :, <span class="hljs-literal">None</span>] - coords_flatten[:, <span class="hljs-literal">None</span>, :]  <span class="hljs-comment"># 2, Wh*Ww, Wh*Ww</span><br>        relative_coords = relative_coords.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).contiguous()  <span class="hljs-comment"># Wh*Ww, Wh*Ww, 2</span><br>        relative_coords[:, :, <span class="hljs-number">0</span>] += self.window_size[<span class="hljs-number">0</span>] - <span class="hljs-number">1</span>  <span class="hljs-comment"># shift to start from 0</span><br>        relative_coords[:, :, <span class="hljs-number">1</span>] += self.window_size[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span><br>        relative_coords[:, :, <span class="hljs-number">0</span>] *= <span class="hljs-number">2</span> * self.window_size[<span class="hljs-number">1</span>] - <span class="hljs-number">1</span><br>        relative_position_index = relative_coords.<span class="hljs-built_in">sum</span>(-<span class="hljs-number">1</span>)  <span class="hljs-comment"># Wh*Ww, Wh*Ww</span><br>        self.register_buffer(<span class="hljs-string">&quot;relative_position_index&quot;</span>, relative_position_index)<br><br>        self.qkv = nn.Linear(dim, dim * <span class="hljs-number">3</span>, bias=qkv_bias)<br>        self.attn_drop = nn.Dropout(attn_drop)<br>        self.proj = nn.Linear(dim, dim)<br>        self.proj_drop = nn.Dropout(proj_drop)<br><br>        trunc_normal_(self.relative_position_bias_table, std=<span class="hljs-number">.02</span>)<br>        self.softmax = nn.Softmax(dim=-<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, mask=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Args:</span><br><span class="hljs-string">            x: input features with shape of (num_windows*B, N, C)</span><br><span class="hljs-string">            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        B_, N, C = x.shape<br>        qkv = self.qkv(x).reshape(B_, N, <span class="hljs-number">3</span>, self.num_heads, C // self.num_heads).permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>)<br>        q, k, v = qkv[<span class="hljs-number">0</span>], qkv[<span class="hljs-number">1</span>], qkv[<span class="hljs-number">2</span>]  <span class="hljs-comment"># make torchscript happy (cannot use tensor as tuple)</span><br><br>        q = q * self.scale<br>        attn = (q @ k.transpose(-<span class="hljs-number">2</span>, -<span class="hljs-number">1</span>))<br><br>        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-<span class="hljs-number">1</span>)].view(<br>            self.window_size[<span class="hljs-number">0</span>] * self.window_size[<span class="hljs-number">1</span>], self.window_size[<span class="hljs-number">0</span>] * self.window_size[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># Wh*Ww,Wh*Ww,nH</span><br>        relative_position_bias = relative_position_bias.permute(<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>).contiguous()  <span class="hljs-comment"># nH, Wh*Ww, Wh*Ww</span><br>        attn = attn + relative_position_bias.unsqueeze(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">if</span> mask <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            nW = mask.shape[<span class="hljs-number">0</span>]<br>            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(<span class="hljs-number">1</span>).unsqueeze(<span class="hljs-number">0</span>)<br>            attn = attn.view(-<span class="hljs-number">1</span>, self.num_heads, N, N)<br>            attn = self.softmax(attn)<br>        <span class="hljs-keyword">else</span>:<br>            attn = self.softmax(attn)<br><br>        attn = self.attn_drop(attn)<br><br>        x = (attn @ v).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>).reshape(B_, N, C)<br>        x = self.proj(x)<br>        x = self.proj_drop(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extra_repr</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&#x27;dim=<span class="hljs-subst">&#123;self.dim&#125;</span>, window_size=<span class="hljs-subst">&#123;self.window_size&#125;</span>, num_heads=<span class="hljs-subst">&#123;self.num_heads&#125;</span>&#x27;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flops</span>(<span class="hljs-params">self, N</span>):<br>        <span class="hljs-comment"># calculate flops for 1 window with token length of N</span><br>        flops = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># qkv = self.qkv(x)</span><br>        flops += N * self.dim * <span class="hljs-number">3</span> * self.dim<br>        <span class="hljs-comment"># attn = (q @ k.transpose(-2, -1))</span><br>        flops += self.num_heads * N * (self.dim // self.num_heads) * N<br>        <span class="hljs-comment">#  x = (attn @ v)</span><br>        flops += self.num_heads * N * N * (self.dim // self.num_heads)<br>        <span class="hljs-comment"># x = self.proj(x)</span><br>        flops += N * self.dim * self.dim<br>        <span class="hljs-keyword">return</span> flops<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SwinTransformerBlock</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; Swin Transformer Block.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dim (int): Number of input channels.</span><br><span class="hljs-string">        input_resolution (tuple[int]): Input resulotion.</span><br><span class="hljs-string">        num_heads (int): Number of attention heads.</span><br><span class="hljs-string">        window_size (int): Window size.</span><br><span class="hljs-string">        shift_size (int): Shift size for SW-MSA.</span><br><span class="hljs-string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span><br><span class="hljs-string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span><br><span class="hljs-string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.</span><br><span class="hljs-string">        drop (float, optional): Dropout rate. Default: 0.0</span><br><span class="hljs-string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span><br><span class="hljs-string">        drop_path (float, optional): Stochastic depth rate. Default: 0.0</span><br><span class="hljs-string">        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU</span><br><span class="hljs-string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span><br><span class="hljs-string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, input_resolution, num_heads, window_size=<span class="hljs-number">7</span>, shift_size=<span class="hljs-number">0</span>,</span><br><span class="hljs-params">                 mlp_ratio=<span class="hljs-number">4.</span>, qkv_bias=<span class="hljs-literal">True</span>, qk_scale=<span class="hljs-literal">None</span>, drop=<span class="hljs-number">0.</span>, attn_drop=<span class="hljs-number">0.</span>, drop_path=<span class="hljs-number">0.</span>,</span><br><span class="hljs-params">                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,</span><br><span class="hljs-params">                 fused_window_process=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dim = dim<br>        self.input_resolution = input_resolution<br>        self.num_heads = num_heads<br>        self.window_size = window_size<br>        self.shift_size = shift_size<br>        self.mlp_ratio = mlp_ratio<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">min</span>(self.input_resolution) &lt;= self.window_size:<br>            <span class="hljs-comment"># if window size is larger than input resolution, we don&#x27;t partition windows</span><br>            self.shift_size = <span class="hljs-number">0</span><br>            self.window_size = <span class="hljs-built_in">min</span>(self.input_resolution)<br>        <span class="hljs-keyword">assert</span> <span class="hljs-number">0</span> &lt;= self.shift_size &lt; self.window_size, <span class="hljs-string">&quot;shift_size must in 0-window_size&quot;</span><br><br>        self.norm1 = norm_layer(dim)<br>        self.attn = WindowAttention(<br>            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,<br>            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)<br><br>        self.drop_path = DropPath(drop_path) <span class="hljs-keyword">if</span> drop_path &gt; <span class="hljs-number">0.</span> <span class="hljs-keyword">else</span> nn.Identity()<br>        self.norm2 = norm_layer(dim)<br>        mlp_hidden_dim = <span class="hljs-built_in">int</span>(dim * mlp_ratio)<br>        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)<br><br>        <span class="hljs-keyword">if</span> self.shift_size &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-comment"># calculate attention mask for SW-MSA</span><br>            <span class="hljs-comment"># 判断是否使用SW-MSA，如果使用窗口移位，会使用mask windows</span><br>            <span class="hljs-comment"># 解析输入图像的分辨率</span><br>            H, W = self.input_resolution<br>            img_mask = torch.zeros((<span class="hljs-number">1</span>, H, W, <span class="hljs-number">1</span>))  <span class="hljs-comment"># 1 H W 1</span><br>            <span class="hljs-comment"># 开始进行切片</span><br>            h_slices = (<span class="hljs-built_in">slice</span>(<span class="hljs-number">0</span>, -self.window_size),<br>                        <span class="hljs-built_in">slice</span>(-self.window_size, -self.shift_size),<br>                        <span class="hljs-built_in">slice</span>(-self.shift_size, <span class="hljs-literal">None</span>))<br>            w_slices = (<span class="hljs-built_in">slice</span>(<span class="hljs-number">0</span>, -self.window_size),<br>                        <span class="hljs-built_in">slice</span>(-self.window_size, -self.shift_size),<br>                        <span class="hljs-built_in">slice</span>(-self.shift_size, <span class="hljs-literal">None</span>))<br>            cnt = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">for</span> h <span class="hljs-keyword">in</span> h_slices:<br>                <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> w_slices:<br>                    <span class="hljs-comment"># 开始切片，并且将相同的位置附上相同的数值 </span><br>                    img_mask[:, h, w, :] = cnt<br>                    cnt += <span class="hljs-number">1</span><br><br>            mask_windows = window_partition(img_mask, self.window_size)  <span class="hljs-comment"># nW, window_size, window_size, 1</span><br>            mask_windows = mask_windows.view(-<span class="hljs-number">1</span>, self.window_size * self.window_size)<br>            attn_mask = mask_windows.unsqueeze(<span class="hljs-number">1</span>) - mask_windows.unsqueeze(<span class="hljs-number">2</span>)<br>            attn_mask = attn_mask.masked_fill(attn_mask != <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(-<span class="hljs-number">100.0</span>)).masked_fill(attn_mask == <span class="hljs-number">0</span>, <span class="hljs-built_in">float</span>(<span class="hljs-number">0.0</span>))<br>        <span class="hljs-keyword">else</span>:<br>            attn_mask = <span class="hljs-literal">None</span><br><br>        self.register_buffer(<span class="hljs-string">&quot;attn_mask&quot;</span>, attn_mask)<br>        self.fused_window_process = fused_window_process<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        H, W = self.input_resolution<br>        B, L, C = x.shape<br>        <span class="hljs-keyword">assert</span> L == H * W, <span class="hljs-string">&quot;input feature has wrong size&quot;</span><br><br>        shortcut = x<br>        x = self.norm1(x)<br>        x = x.view(B, H, W, C)<br><br>        <span class="hljs-comment"># cyclic shift</span><br>        <span class="hljs-keyword">if</span> self.shift_size &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.fused_window_process:<br>                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>                <span class="hljs-comment"># partition windows</span><br>                x_windows = window_partition(shifted_x, self.window_size)  <span class="hljs-comment"># nW*B, window_size, window_size, C</span><br>            <span class="hljs-keyword">else</span>:<br>                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)<br>        <span class="hljs-keyword">else</span>:<br>            shifted_x = x<br>            <span class="hljs-comment"># partition windows</span><br>            x_windows = window_partition(shifted_x, self.window_size)  <span class="hljs-comment"># nW*B, window_size, window_size, C</span><br><br>        x_windows = x_windows.view(-<span class="hljs-number">1</span>, self.window_size * self.window_size, C)  <span class="hljs-comment"># nW*B, window_size*window_size, C</span><br><br>        <span class="hljs-comment"># W-MSA/SW-MSA</span><br>        attn_windows = self.attn(x_windows, mask=self.attn_mask)  <span class="hljs-comment"># nW*B, window_size*window_size, C</span><br><br>        <span class="hljs-comment"># merge windows</span><br>        attn_windows = attn_windows.view(-<span class="hljs-number">1</span>, self.window_size, self.window_size, C)<br><br>        <span class="hljs-comment"># reverse cyclic shift</span><br>        <span class="hljs-keyword">if</span> self.shift_size &gt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.fused_window_process:<br>                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  <span class="hljs-comment"># B H&#x27; W&#x27; C</span><br>                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))<br>            <span class="hljs-keyword">else</span>:<br>                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)<br>        <span class="hljs-keyword">else</span>:<br>            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  <span class="hljs-comment"># B H&#x27; W&#x27; C</span><br>            x = shifted_x<br>        x = x.view(B, H * W, C)<br>        x = shortcut + self.drop_path(x)<br><br>        <span class="hljs-comment"># FFN</span><br>        x = x + self.drop_path(self.mlp(self.norm2(x)))<br><br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extra_repr</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;dim=<span class="hljs-subst">&#123;self.dim&#125;</span>, input_resolution=<span class="hljs-subst">&#123;self.input_resolution&#125;</span>, num_heads=<span class="hljs-subst">&#123;self.num_heads&#125;</span>, &quot;</span> \<br>               <span class="hljs-string">f&quot;window_size=<span class="hljs-subst">&#123;self.window_size&#125;</span>, shift_size=<span class="hljs-subst">&#123;self.shift_size&#125;</span>, mlp_ratio=<span class="hljs-subst">&#123;self.mlp_ratio&#125;</span>&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flops</span>(<span class="hljs-params">self</span>):<br>        flops = <span class="hljs-number">0</span><br>        H, W = self.input_resolution<br>        <span class="hljs-comment"># norm1</span><br>        flops += self.dim * H * W<br>        <span class="hljs-comment"># W-MSA/SW-MSA</span><br>        nW = H * W / self.window_size / self.window_size<br>        flops += nW * self.attn.flops(self.window_size * self.window_size)<br>        <span class="hljs-comment"># mlp</span><br>        flops += <span class="hljs-number">2</span> * H * W * self.dim * self.dim * self.mlp_ratio<br>        <span class="hljs-comment"># norm2</span><br>        flops += self.dim * H * W<br>        <span class="hljs-keyword">return</span> flops<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PatchMerging</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; Patch Merging Layer.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        input_resolution (tuple[int]): Resolution of input feature.</span><br><span class="hljs-string">        dim (int): Number of input channels.</span><br><span class="hljs-string">        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_resolution, dim, norm_layer=nn.LayerNorm</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.input_resolution = input_resolution<br>        self.dim = dim<br>        self.reduction = nn.Linear(<span class="hljs-number">4</span> * dim, <span class="hljs-number">2</span> * dim, bias=<span class="hljs-literal">False</span>)<br>        self.norm = norm_layer(<span class="hljs-number">4</span> * dim)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        x: B, H*W, C</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        H, W = self.input_resolution<br>        B, L, C = x.shape<br>        <span class="hljs-keyword">assert</span> L == H * W, <span class="hljs-string">&quot;input feature has wrong size&quot;</span><br>        <span class="hljs-keyword">assert</span> H % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> W % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>, <span class="hljs-string">f&quot;x size (<span class="hljs-subst">&#123;H&#125;</span>*<span class="hljs-subst">&#123;W&#125;</span>) are not even.&quot;</span><br><br>        x = x.view(B, H, W, C)<br><br>        x0 = x[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x1 = x[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x2 = x[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x3 = x[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>, :]  <span class="hljs-comment"># B H/2 W/2 C</span><br>        x = torch.cat([x0, x1, x2, x3], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># B H/2 W/2 4*C</span><br>        x = x.view(B, -<span class="hljs-number">1</span>, <span class="hljs-number">4</span> * C)  <span class="hljs-comment"># B H/2*W/2 4*C</span><br><br>        x = self.norm(x)<br>        x = self.reduction(x)<br><br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extra_repr</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;input_resolution=<span class="hljs-subst">&#123;self.input_resolution&#125;</span>, dim=<span class="hljs-subst">&#123;self.dim&#125;</span>&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flops</span>(<span class="hljs-params">self</span>):<br>        H, W = self.input_resolution<br>        flops = H * W * self.dim<br>        flops += (H // <span class="hljs-number">2</span>) * (W // <span class="hljs-number">2</span>) * <span class="hljs-number">4</span> * self.dim * <span class="hljs-number">2</span> * self.dim<br>        <span class="hljs-keyword">return</span> flops<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicLayer</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; A basic Swin Transformer layer for one stage.</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        dim (int): Number of input channels.</span><br><span class="hljs-string">        input_resolution (tuple[int]): Input resolution.</span><br><span class="hljs-string">        depth (int): Number of blocks.</span><br><span class="hljs-string">        num_heads (int): Number of attention heads.</span><br><span class="hljs-string">        window_size (int): Local window size.</span><br><span class="hljs-string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.</span><br><span class="hljs-string">        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True</span><br><span class="hljs-string">        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.</span><br><span class="hljs-string">        drop (float, optional): Dropout rate. Default: 0.0</span><br><span class="hljs-string">        attn_drop (float, optional): Attention dropout rate. Default: 0.0</span><br><span class="hljs-string">        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0</span><br><span class="hljs-string">        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm</span><br><span class="hljs-string">        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None</span><br><span class="hljs-string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.</span><br><span class="hljs-string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, dim, input_resolution, depth, num_heads, window_size,</span><br><span class="hljs-params">                 mlp_ratio=<span class="hljs-number">4.</span>, qkv_bias=<span class="hljs-literal">True</span>, qk_scale=<span class="hljs-literal">None</span>, drop=<span class="hljs-number">0.</span>, attn_drop=<span class="hljs-number">0.</span>,</span><br><span class="hljs-params">                 drop_path=<span class="hljs-number">0.</span>, norm_layer=nn.LayerNorm, downsample=<span class="hljs-literal">None</span>, use_checkpoint=<span class="hljs-literal">False</span>,</span><br><span class="hljs-params">                 fused_window_process=<span class="hljs-literal">False</span></span>):<br><br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dim = dim<br>        self.input_resolution = input_resolution<br>        self.depth = depth<br>        self.use_checkpoint = use_checkpoint<br><br>        <span class="hljs-comment"># build blocks</span><br>        self.blocks = nn.ModuleList([<br>            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,<br>                                 num_heads=num_heads, window_size=window_size,<br>                                 shift_size=<span class="hljs-number">0</span> <span class="hljs-keyword">if</span> (i % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) <span class="hljs-keyword">else</span> window_size // <span class="hljs-number">2</span>,<br>                                 mlp_ratio=mlp_ratio,<br>                                 qkv_bias=qkv_bias, qk_scale=qk_scale,<br>                                 drop=drop, attn_drop=attn_drop,<br>                                 drop_path=drop_path[i] <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(drop_path, <span class="hljs-built_in">list</span>) <span class="hljs-keyword">else</span> drop_path,<br>                                 norm_layer=norm_layer,<br>                                 fused_window_process=fused_window_process)<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(depth)])<br><br>        <span class="hljs-comment"># patch merging layer</span><br>        <span class="hljs-keyword">if</span> downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)<br>        <span class="hljs-keyword">else</span>:<br>            self.downsample = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blocks:<br>            <span class="hljs-keyword">if</span> self.use_checkpoint:<br>                x = checkpoint.checkpoint(blk, x)<br>            <span class="hljs-keyword">else</span>:<br>                x = blk(x)<br>        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            x = self.downsample(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extra_repr</span>(<span class="hljs-params">self</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">f&quot;dim=<span class="hljs-subst">&#123;self.dim&#125;</span>, input_resolution=<span class="hljs-subst">&#123;self.input_resolution&#125;</span>, depth=<span class="hljs-subst">&#123;self.depth&#125;</span>&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flops</span>(<span class="hljs-params">self</span>):<br>        flops = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> self.blocks:<br>            flops += blk.flops()<br>        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            flops += self.downsample.flops()<br>        <span class="hljs-keyword">return</span> flops<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">PatchEmbed</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; Image to Patch Embedding</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        img_size (int): Image size.  Default: 224.</span><br><span class="hljs-string">        patch_size (int): Patch token size. Default: 4.</span><br><span class="hljs-string">        in_chans (int): Number of input image channels. Default: 3.</span><br><span class="hljs-string">        embed_dim (int): Number of linear projection output channels. Default: 96.</span><br><span class="hljs-string">        norm_layer (nn.Module, optional): Normalization layer. Default: None</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, img_size=<span class="hljs-number">224</span>, patch_size=<span class="hljs-number">4</span>, in_chans=<span class="hljs-number">3</span>, embed_dim=<span class="hljs-number">96</span>, norm_layer=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        img_size = to_2tuple(img_size)<br>        patch_size = to_2tuple(patch_size)<br>        patches_resolution = [img_size[<span class="hljs-number">0</span>] // patch_size[<span class="hljs-number">0</span>], img_size[<span class="hljs-number">1</span>] // patch_size[<span class="hljs-number">1</span>]]<br>        self.img_size = img_size<br>        self.patch_size = patch_size<br>        self.patches_resolution = patches_resolution<br>        self.num_patches = patches_resolution[<span class="hljs-number">0</span>] * patches_resolution[<span class="hljs-number">1</span>]<br><br>        self.in_chans = in_chans<br>        self.embed_dim = embed_dim<br><br>        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)<br>        <span class="hljs-keyword">if</span> norm_layer <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            self.norm = norm_layer(embed_dim)<br>        <span class="hljs-keyword">else</span>:<br>            self.norm = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        B, C, H, W = x.shape<br>        <span class="hljs-comment"># FIXME look at relaxing size constraints</span><br>        <span class="hljs-keyword">assert</span> H == self.img_size[<span class="hljs-number">0</span>] <span class="hljs-keyword">and</span> W == self.img_size[<span class="hljs-number">1</span>], \<br>            <span class="hljs-string">f&quot;Input image size (<span class="hljs-subst">&#123;H&#125;</span>*<span class="hljs-subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="hljs-subst">&#123;self.img_size[<span class="hljs-number">0</span>]&#125;</span>*<span class="hljs-subst">&#123;self.img_size[<span class="hljs-number">1</span>]&#125;</span>).&quot;</span><br>        x = self.proj(x).flatten(<span class="hljs-number">2</span>).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># B Ph*Pw C</span><br>        <span class="hljs-keyword">if</span> self.norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            x = self.norm(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flops</span>(<span class="hljs-params">self</span>):<br>        Ho, Wo = self.patches_resolution<br>        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[<span class="hljs-number">0</span>] * self.patch_size[<span class="hljs-number">1</span>])<br>        <span class="hljs-keyword">if</span> self.norm <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            flops += Ho * Wo * self.embed_dim<br>        <span class="hljs-keyword">return</span> flops<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SwinTransformer</span>(nn.Module):<br>    <span class="hljs-string">r&quot;&quot;&quot; Swin Transformer</span><br><span class="hljs-string">        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -</span><br><span class="hljs-string">          https://arxiv.org/pdf/2103.14030</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">        img_size (int | tuple(int)): Input image size. Default 224</span><br><span class="hljs-string">        patch_size (int | tuple(int)): Patch size. Default: 4</span><br><span class="hljs-string">        in_chans (int): Number of input image channels. Default: 3</span><br><span class="hljs-string">        num_classes (int): Number of classes for classification head. Default: 1000</span><br><span class="hljs-string">        embed_dim (int): Patch embedding dimension. Default: 96</span><br><span class="hljs-string">        depths (tuple(int)): Depth of each Swin Transformer layer.</span><br><span class="hljs-string">        num_heads (tuple(int)): Number of attention heads in different layers.</span><br><span class="hljs-string">        window_size (int): Window size. Default: 7</span><br><span class="hljs-string">        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4</span><br><span class="hljs-string">        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True</span><br><span class="hljs-string">        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None</span><br><span class="hljs-string">        drop_rate (float): Dropout rate. Default: 0</span><br><span class="hljs-string">        attn_drop_rate (float): Attention dropout rate. Default: 0</span><br><span class="hljs-string">        drop_path_rate (float): Stochastic depth rate. Default: 0.1</span><br><span class="hljs-string">        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.</span><br><span class="hljs-string">        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False</span><br><span class="hljs-string">        patch_norm (bool): If True, add normalization after patch embedding. Default: True</span><br><span class="hljs-string">        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False</span><br><span class="hljs-string">        fused_window_process (bool, optional): If True, use one kernel to fused window shift &amp; window partition for acceleration, similar for the reversed part. Default: False</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, img_size=<span class="hljs-number">224</span>, patch_size=<span class="hljs-number">4</span>, in_chans=<span class="hljs-number">3</span>, num_classes=<span class="hljs-number">1000</span>,</span><br><span class="hljs-params">                 embed_dim=<span class="hljs-number">96</span>, depths=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">2</span>], num_heads=[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">12</span>, <span class="hljs-number">24</span>],</span><br><span class="hljs-params">                 window_size=<span class="hljs-number">7</span>, mlp_ratio=<span class="hljs-number">4.</span>, qkv_bias=<span class="hljs-literal">True</span>, qk_scale=<span class="hljs-literal">None</span>,</span><br><span class="hljs-params">                 drop_rate=<span class="hljs-number">0.</span>, attn_drop_rate=<span class="hljs-number">0.</span>, drop_path_rate=<span class="hljs-number">0.1</span>,</span><br><span class="hljs-params">                 norm_layer=nn.LayerNorm, ape=<span class="hljs-literal">False</span>, patch_norm=<span class="hljs-literal">True</span>,</span><br><span class="hljs-params">                 use_checkpoint=<span class="hljs-literal">False</span>, fused_window_process=<span class="hljs-literal">False</span>, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>        self.num_classes = num_classes<br>        self.num_layers = <span class="hljs-built_in">len</span>(depths)<br>        self.embed_dim = embed_dim<br>        self.ape = ape<br>        self.patch_norm = patch_norm<br>        self.num_features = <span class="hljs-built_in">int</span>(embed_dim * <span class="hljs-number">2</span> ** (self.num_layers - <span class="hljs-number">1</span>))<br>        self.mlp_ratio = mlp_ratio<br><br>        <span class="hljs-comment"># split image into non-overlapping patches</span><br>        self.patch_embed = PatchEmbed(<br>            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,<br>            norm_layer=norm_layer <span class="hljs-keyword">if</span> self.patch_norm <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>)<br>        num_patches = self.patch_embed.num_patches<br>        patches_resolution = self.patch_embed.patches_resolution<br>        self.patches_resolution = patches_resolution<br><br>        <span class="hljs-comment"># absolute position embedding</span><br>        <span class="hljs-keyword">if</span> self.ape:<br>            self.absolute_pos_embed = nn.Parameter(torch.zeros(<span class="hljs-number">1</span>, num_patches, embed_dim))<br>            trunc_normal_(self.absolute_pos_embed, std=<span class="hljs-number">.02</span>)<br><br>        self.pos_drop = nn.Dropout(p=drop_rate)<br><br>        <span class="hljs-comment"># stochastic depth</span><br>        dpr = [x.item() <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> torch.linspace(<span class="hljs-number">0</span>, drop_path_rate, <span class="hljs-built_in">sum</span>(depths))]  <span class="hljs-comment"># stochastic depth decay rule</span><br><br>        <span class="hljs-comment"># build layers</span><br>        self.layers = nn.ModuleList()<br>        <span class="hljs-keyword">for</span> i_layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.num_layers):<br>            layer = BasicLayer(dim=<span class="hljs-built_in">int</span>(embed_dim * <span class="hljs-number">2</span> ** i_layer),<br>                               input_resolution=(patches_resolution[<span class="hljs-number">0</span>] // (<span class="hljs-number">2</span> ** i_layer),<br>                                                 patches_resolution[<span class="hljs-number">1</span>] // (<span class="hljs-number">2</span> ** i_layer)),<br>                               depth=depths[i_layer],<br>                               num_heads=num_heads[i_layer],<br>                               window_size=window_size,<br>                               mlp_ratio=self.mlp_ratio,<br>                               qkv_bias=qkv_bias, qk_scale=qk_scale,<br>                               drop=drop_rate, attn_drop=attn_drop_rate,<br>                               drop_path=dpr[<span class="hljs-built_in">sum</span>(depths[:i_layer]):<span class="hljs-built_in">sum</span>(depths[:i_layer + <span class="hljs-number">1</span>])],<br>                               norm_layer=norm_layer,<br>                               downsample=PatchMerging <span class="hljs-keyword">if</span> (i_layer &lt; self.num_layers - <span class="hljs-number">1</span>) <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>                               use_checkpoint=use_checkpoint,<br>                               fused_window_process=fused_window_process)<br>            self.layers.append(layer)<br><br>        self.norm = norm_layer(self.num_features)<br>        self.avgpool = nn.AdaptiveAvgPool1d(<span class="hljs-number">1</span>)<br>        self.head = nn.Linear(self.num_features, num_classes) <span class="hljs-keyword">if</span> num_classes &gt; <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> nn.Identity()<br><br>        self.apply(self._init_weights)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_init_weights</span>(<span class="hljs-params">self, m</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>            trunc_normal_(m.weight, std=<span class="hljs-number">.02</span>)<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear) <span class="hljs-keyword">and</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.LayerNorm):<br>            nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>            nn.init.constant_(m.weight, <span class="hljs-number">1.0</span>)<br><br><span class="hljs-meta">    @torch.jit.ignore</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">no_weight_decay</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;absolute_pos_embed&#x27;</span>&#125;<br><br><span class="hljs-meta">    @torch.jit.ignore</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">no_weight_decay_keywords</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> &#123;<span class="hljs-string">&#x27;relative_position_bias_table&#x27;</span>&#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward_features</span>(<span class="hljs-params">self, x</span>):<br>        x = self.patch_embed(x)<br>        <span class="hljs-keyword">if</span> self.ape:<br>            x = x + self.absolute_pos_embed<br>        x = self.pos_drop(x)<br><br>        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.layers:<br>            x = layer(x)<br><br>        x = self.norm(x)  <span class="hljs-comment"># B L C</span><br>        x = self.avgpool(x.transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>))  <span class="hljs-comment"># B C 1</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.forward_features(x)<br>        x = self.head(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">flops</span>(<span class="hljs-params">self</span>):<br>        flops = <span class="hljs-number">0</span><br>        flops += self.patch_embed.flops()<br>        <span class="hljs-keyword">for</span> i, layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(self.layers):<br>            flops += layer.flops()<br>        flops += self.num_features * self.patches_resolution[<span class="hljs-number">0</span>] * self.patches_resolution[<span class="hljs-number">1</span>] // (<span class="hljs-number">2</span> ** self.num_layers)<br>        flops += self.num_features * self.num_classes<br>        <span class="hljs-keyword">return</span> flops<br></code></pre></div></td></tr></table></figure>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/">Deep Learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/cv/">cv</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/05/22/math/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">math</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/03/20/SIoU/">
                        <span class="hidden-mobile">SIoU</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                

              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
