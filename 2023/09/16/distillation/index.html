

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="肘子开">
  <meta name="keywords" content="">
  
    <meta name="description" content="Classification Knowledge Distillation via the Target-aware Transformer (CVPR 2022) 目前不足：  感受野对模型表征能力影响十分重要，这种差异是目前一对一匹配蒸馏法导致次优结果的潜在原因。  改进：  我们提出通过target-aware transformer（TaT）进行知识提炼，使全体学生分别">
<meta property="og:type" content="article">
<meta property="og:title" content="distillation">
<meta property="og:url" content="http://example.com/2023/09/16/distillation/index.html">
<meta property="og:site_name" content="肘子开的博客">
<meta property="og:description" content="Classification Knowledge Distillation via the Target-aware Transformer (CVPR 2022) 目前不足：  感受野对模型表征能力影响十分重要，这种差异是目前一对一匹配蒸馏法导致次优结果的潜在原因。  改进：  我们提出通过target-aware transformer（TaT）进行知识提炼，使全体学生分别">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/cv/image-20230916163256829.png">
<meta property="og:image" content="http://example.com/img/cv/image-20231011102343580.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230916170214352.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230916172809521.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230916173404192.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230916173923018.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230918105121610.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230918172153838.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230918174811736.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230918175405238.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230919163725146.png">
<meta property="og:image" content="http://example.com/img/cv/image-20230922150349750.png">
<meta property="og:image" content="http://example.com/img/cv/image-20231008104938634.png">
<meta property="og:image" content="http://example.com/img/cv/image-20231010163904386.png">
<meta property="article:published_time" content="2023-09-16T08:27:59.000Z">
<meta property="article:modified_time" content="2023-10-11T08:56:13.355Z">
<meta property="article:author" content="肘子开">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/cv/image-20230916163256829.png">
  
  
  <title>distillation - 肘子开的博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/nnfx-dark.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>肘子开的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg/bg2.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="distillation">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2023-09-16 16:27" pubdate>
        2023年9月16日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      20k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      163 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">distillation</h1>
            
            <div class="markdown-body">
              <h1 id="classification">Classification</h1>
<h2
id="knowledge-distillation-via-the-target-aware-transformer-cvpr-2022">Knowledge
Distillation via the Target-aware Transformer (CVPR 2022)</h2>
<p>目前不足：</p>
<ul>
<li>感受野对模型表征能力影响十分重要，这种差异是目前一对一匹配蒸馏法导致次优结果的潜在原因。</li>
</ul>
<p>改进：</p>
<ul>
<li>我们提出通过<em>target-aware
transformer</em>（TaT）进行知识提炼，使全体学生分别模仿教师的各个空间组件（one-to-all）。这样，我们就能提高匹配能力，从而改善知识提炼的性能。</li>
<li>我们提出了分层蒸馏法，以转移局部特征和全局依赖性，而不是原始特征图。这样，我们就能将提出的方法应用于因特征图规模庞大而计算负担沉重的应用中。</li>
</ul>
<p><img src="/img/cv/image-20230916163256829.png" srcset="/img/loading.gif" lazyload /></p>
<p>由于教师网络的特征图通常包含更多的层和更大的特征通道，与学生网络相比，同一像素位置的空间信息包含更丰富的语义信息。为此，我们提出了一个一对一的空间匹配知识提炼管道，让教师的每个特征位置都能以动态的方式教授整个学生的特征。
<span class="math display">\[
\begin{aligned}
W^{i} &amp; =\sigma\left(\left\langle f_{1}^{s},
f_{i}^{t}\right\rangle,\left\langle f_{2}^{s}, f_{i}^{t}\right\rangle,
\ldots,\left\langle f_{N}^{s}, f_{i}^{t}\right\rangle\right) \\
&amp; =\left[w_{1}^{i}, w_{2}^{i}, \ldots, w_{N}^{i}\right]
\end{aligned}
\]</span> 将这些相关语义汇总到所有组件中，我们就得到了结果： <span
class="math display">\[
f_{i}^{s^{\prime}}=w_{1}^{i} \times f_{1}^{s}+w_{2}^{i} \times
f_{2}^{s}+\cdots+w_{N}^{i} \times f_{N}^{s}
\]</span> 两个公式可以合为一个矩阵乘法：<span
class="math inline">\(f^{s^{&#39;}}_i=\sigma(f^s\cdot f^t_i)\cdot
f^s\)</span></p>
<p>为了方便训练，我们引入了参数方法，对学生特征和教师特征进行了额外的线性变换。我们发现，在消融研究中，参数版本比非参数版本表现更好。
<span class="math display">\[
f^{s^{\prime}}=\sigma\left(\gamma\left(f^s\right) \cdot
\theta\left(f^t\right)^{\top}\right) \cdot \phi\left(f^s\right)
\]</span> 最后TaT的损失函数就为： <span class="math display">\[
\mathcal{L}_{\mathrm{TaT}}=\left\|f^{s^{\prime}}-f^t\right\|_2
\]</span> 但是TaT的计算量还是很大，所以通过Patch-group
Distillation和Anchor-point Distillation减少计算量。</p>
<ul>
<li>Patch-group
Distillation通过将特征图分割成Patch然后n个Patch为一组，在各组之间计算TaT损失</li>
<li>上述算法对于远距离的感知性较差。Anchor-point
Distillation将特征图进行池化采样成小特征图进行计算TaT损失</li>
</ul>
<p>最后总的Loss为： <span class="math display">\[
\mathcal{L}_{\mathrm{Seg}}=\alpha \mathcal{L}_{\mathrm{CE}}+\delta
\mathcal{L}_{\mathrm{TaT}}^{\mathcal{P}}+\zeta
\mathcal{L}_{\mathrm{TaT}}^{\mathcal{A}}
\]</span></p>
<h2
id="a-fast-knowledge-distillation-framework-for-visual-recognitioneccv-2022">A
Fast Knowledge Distillation Framework for Visual Recognition(ECCV
2022)</h2>
<p>不足：</p>
<ul>
<li>蒸馏方法消耗资源大，因为每次都要遍历一遍教师网络。因为现在许多训练方式包含数据增强，我们无法将中间的软标签进行重复利用</li>
<li>ReLabel 被提出来存储来自预先训练的强教师的全局标签图注释，以便 RoI
align
重新利用，而无需重复通过教师网络。（1）但是全局标签图是通过输入全局图像获得的，它不能像在输入空间中采用随机裁剪-调整操作的蒸馏过程那样完全反映软分布。（2）RoI
对齐不能保证分布与教师转发的分布完全一致。</li>
</ul>
<p>创新点：</p>
<p>地址：https://github.com/szq0214/FKD</p>
<h2
id="improved-feature-distillation-via-projector-ensembleneurips">Improved
Feature Distillation via Projector Ensemble(NeurIPS)</h2>
<p>不足：</p>
<ul>
<li>不使用projector直接进行蒸馏学习的话，很容易造成学生的过拟合</li>
</ul>
<p>创新点：</p>
<ul>
<li>受使用projector可以提升性能的启发，作者使用多个映射器进行蒸馏学习</li>
</ul>
<p><img src="/img/cv/image-20231011102343580.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="feature-distillation-as-multi-task-learning">Feature
Distillation as Multi-task Learning</h3>
<p>由于蒸馏方法对超参数和老师-学生组合的变化很敏感，额外的目标会增加系数调整的训练成本。为了减轻这个问题，作者简单的使用如下Direction
Alignment（DA）损失进行特征蒸馏： <span class="math display">\[
\mathcal{L}_{DA}=\frac1{2b}\sum_{i=1}^b||\frac{g(s_i)}{||g(s_i)||_2}-\frac{t_i}{||t_i||_2}||_2^2=1-\frac1b\sum_{i=1}^b\frac{\langle
g(s_i),t_i\rangle}{||g(s_i)||_2||t_i||_2},
\]</span></p>
<h3 id="improved-feature-distillation-with-projector-ensemble">Improved
Feature Distillation with Projector Ensemble</h3>
<p>上一节主要讲述projector确实可以提升学生的学习能力，收到这个启发，作者使用projector组合进行进一步提升。主要有两个动机：</p>
<ul>
<li>多个projector具有不同的初始化会提供不同的转换特征，这有助于学生的泛化</li>
<li>因为在projector中使用ReLU，所以学生特征可能出现0，但是教师中使用了池化层，不太可能出现0。所以学生和教师的特征分布会有差距，所以使用多个projector可以进行权衡。</li>
</ul>
<p>使用了多个projector后，Modified Direction Alignmen（MDA）loss如下：
<span class="math display">\[
\mathcal{L}_{MDA}=1-\frac1b\sum_{i=1}^b\frac{\langle
f(s_i),t_i\rangle}{||f(s_i)||_2||t_i||_2}
\]</span> 其中 <span class="math inline">\(f(\cdot)\)</span>
就是多个projector。最后总的loss如下： <span class="math display">\[
\mathcal{L}_{total}=\mathcal{L}_{CE}+\alpha\mathcal{L}_{MDA}
\]</span></p>
<h1 id="object-detection">Object Detection</h1>
<h2
id="improve-object-detection-with-feature-based-knowledge-distillation-towards-accurate-and-efficient-detectors-iclr-2021">IMPROVE
OBJECT DETECTION WITH FEATURE-BASED KNOWLEDGE DISTILLATION: TOWARDS
ACCURATE AND EFFICIENT DETECTORS (ICLR 2021)</h2>
<p>KD在目标检测方向的应用受限主要是因为如下两个原因</p>
<ul>
<li>前后背景的像素点数量不平衡</li>
<li>像素间的关联性缺少一定的蒸馏</li>
</ul>
<p>对应的解决策略如下：</p>
<ul>
<li>既然存在不平衡关系，那我直接只针对这些bbox的区域做不就好了吗。有些是直接二值mask过滤、有的是将二值soft成高斯分布的形式</li>
<li>采用Non-Local算子产生attention map，针对attention
map计算L2损失回归</li>
</ul>
<p><img src="/img/cv/image-20230916170214352.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="attention-guided-distillation">ATTENTION-GUIDED
DISTILLATION</h3>
<p>那么，空间注意力图和通道注意力图的生成就等同于找到映射函数$^s: ^{C,
H, W} ^{H, W} <span class="math inline">\(和\)</span>^c: ^{C, H, W}
^C$。由于特征中每个元素的绝对值都意味着其重要性，因此我们通过对通道维度的绝对值求和来构建
<span
class="math inline">\(\mathcal{G}^s\)</span>，并通过对宽度和高度维度的绝对值求和来构建
<span class="math inline">\(\mathcal{G}^c\)</span>，具体公式为<span
class="math inline">\(\mathcal{G}^c(A)=\frac{1}{H W} \sum_H^{i=1}
\sum_W^{j=1}\left|A_{\cdot, i, j}\right|\)</span>和<span
class="math inline">\(\mathcal{G}^s(A)=\frac{1}{C}
\sum_C^{k=1}\left|A_{k,,
\cdot}\right|\)</span>。然后，将教师和学生Detector的注意力图相加，就可以得到注意力引导蒸馏法中使用的空间注意力掩码
<span class="math inline">\(M^s\)</span> 和通道注意力掩码 <span
class="math inline">\(M^c\)</span>，公式为<span
class="math inline">\(M^s=H W \cdot
\operatorname{softmax}\left(\left(\mathcal{G}^s\left(A^{\mathcal{S}}\right)+\mathcal{G}^s\left(A^{\mathcal{T}}\right)\right)
/ T\right)\)</span>和<span
class="math inline">\(M^c=C\cdot\text{softmax}(\mathcal{G}^c(A^{\mathcal{S}})+\mathcal{G}^c(A^{\mathcal{T}}))/T)\)</span>。注意力引导蒸馏损失
<span class="math inline">\(\mathcal{L}_{AGD}\)</span>
由两部分组成--注意力转移损失 <span
class="math inline">\(\mathcal{L}_{AT}\)</span> 和注意力屏蔽损失 <span
class="math inline">\(\mathcal{L}_{AM}\)</span>。 <span
class="math inline">\(\mathcal{L}_{AT}\)</span>
鼓励学生模型模拟教师模型的空间和通道注意力: <span
class="math display">\[
\mathcal{L}_{AT}=\mathcal{L}_2(\mathcal{G}^s(A^{\mathcal{S}}),\mathcal{G}^s(A^{\mathcal{T}}))+\mathcal{L}_2(\mathcal{G}^c(A^{\mathcal{S}}),\mathcal{G}^c(A^{\mathcal{T}}))
\]</span> <span
class="math inline">\(\mathcal{L}_{AM}\)</span>鼓励学生模仿教师模型的特征:
<span class="math display">\[
\begin{aligned}\mathcal{L}_{AM}&amp;=\left(\sum_{k=1}^C\sum_{i=1}^H\sum_{j=1}^W(A_{k,i,j}^T-A_{k,i,j}^\mathcal{S})^2\cdot
M_{i,j}^s\cdot M_k^c\right)^{\frac{1}{2}}\end{aligned}
\]</span></p>
<h3 id="non-local-distillation">NON-LOCAL DISTILLATION</h3>
<p>使用NON-LOCAL模块来提取全局信息，最后Loss： <span
class="math display">\[
\mathcal{L}_{NLD}=\mathcal{L}_{2}(r^{\mathcal{S}},r^{\mathcal{T}})
\]</span></p>
<h3 id="overall-loss-function">OVERALL LOSS FUNCTION</h3>
<p><span class="math display">\[
\mathcal{L}_{Distill}(A^\mathcal{T},A^\mathcal{S})=\underbrace{\alpha\cdot\mathcal{L}_{AT}+\beta\cdot\mathcal{L}_{AM}}_{\text{Attention-guided
distillation}}+\underbrace{\gamma\cdot\mathcal{L}_{NLD}.}_{\text{Non-local
distillation}}
\]</span></p>
<h2
id="localization-distillation-for-object-detection-tpami">Localization
Distillation for Object Detection (TPAMI)</h2>
<p><img src="/img/cv/image-20230916172809521.png" srcset="/img/loading.gif" lazyload /></p>
<p>还没看懂，后面再看</p>
<h2
id="general-instance-distillation-for-object-detection-cvpr-2021">General
Instance Distillation for Object Detection (CVPR 2021)</h2>
<p>不足：</p>
<ul>
<li>此外，目前的检测蒸馏方法不能同时在多个检测框架中很好地发挥作用，例如两阶段、一阶段、无锚方法。</li>
</ul>
<p>贡献：</p>
<ul>
<li>将General
Instance（GI）定义为蒸馏目标，可有效提高检测模型的蒸馏效果。</li>
<li>在 GI
的基础上，我们首先引入基于关系的知识，对检测任务进行提炼，并将其与基于反应和特征的知识进行整合，从而使学生超越教师。</li>
<li>我们的方法对各种检测框架具有稳健的泛化性</li>
</ul>
<p><img src="/img/cv/image-20230916173404192.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="general-instance-selection-module">General Instance Selection
Module</h3>
<p><img src="/img/cv/image-20230916173923018.png" srcset="/img/loading.gif" lazyload /></p>
<p>为了量化每个实例的差异以及选择用于蒸馏的判别实例，提出了两个指标：GI
score和GI box。为了节省训练时的计算资源，我们只需计算分类得分的 L1
距离作为 GI score，并选择得分较高的方框作为 GI
box(针对每一个回归框，作者通过比较tea与stu回归框的所有分类得分差异最大的那个作为当前框的差异分数)。上图展示了生成
GI 的过程，每个预测实例 r 的得分和方框定义如下： <span
class="math display">\[
\begin{aligned}
P_{GI}^{r}&amp; =\max_{0&lt;c\leq C}\left|P_t^{rc}-P_s^{rc}\right|,  \\
B_{GI}^r&amp; =\left\{\begin{array}{cc}B_t^r,&amp;\max_{0&lt;c\leq
C}P_t^{rc}&gt;\max_{0&lt;c\leq C}P_s^{rc}\\B_s^r,&amp;\max_{0&lt;c\leq
C}P_t^{rc}\leq\max_{0&lt;c\leq C}P_s^{rc}\end{array}\right.,  \\
\text{GI}&amp; =NMS(P_{GI},B_{GI}),
\end{aligned}
\]</span> 其中<span class="math inline">\(P_{GI}\)</span>和<span
class="math inline">\(B_{GI}\)</span>分别为GI score和GI
box。对与one-stage来说<span class="math inline">\(P_t\)</span>和<span
class="math inline">\(P_s\)</span>就是教师和学生模型的预测分类得分，对于two-stage来说<span
class="math inline">\(P\)</span>就是RPN预测的目标得分。同时，<span
class="math inline">\(B_t\)</span>和<span
class="math inline">\(B_s\)</span>就是预测的目标框了。R
是预测框的数量，C 是类别的数量。r、c 是 R、C
维度中的索引。最后使用NMS来消除冗余的重叠实例。此外，在每幅图像中，我们只选择得分最高的
K 个实例作为蒸馏的最终 GI。</p>
<h3 id="feature-based-distillation">Feature-based Distillation</h3>
<p>由于 FPN 结合了多个骨干层的特点，我们直观地选择 FPN
进行蒸馏。具体来说，我们根据每个 GI 方框的不同大小，对匹配 FPN
层的特征进行裁剪。鉴于检测任务中目标大小的差异很大，直接进行像素化提炼会使模型更倾向于学习大型目标。因此，我们采用
ROIAlign 算法，将不同大小的 GI
特征调整为相同大小，然后进行蒸馏，对每个目标一视同仁。基于特征的蒸馏损失如下：
<span class="math display">\[
\begin{gathered}L_{Feature}=\frac1K\sum_{i=1}^K\left\|t_i-s_i^{\prime}\right\|_2^2,\\s^{\prime}=f_{adapt}(s),\end{gathered}
\]</span>
K是第一部分GISM挑选出来的GI数量。第二个公式是线性适应方法令学生特征图映射到和教师相同维度。</p>
<h3 id="relation-based-distillation">Relation-based Distillation</h3>
<p>在这里，我们使用欧氏距离来衡量实例的相关性，并使用 L1
距离来传递知识。我们还利用 GI
之间的相关性信息来提炼从教师到学生的知识。损失表达式如下： <span
class="math display">\[
\begin{aligned}
L_{Relation}&amp;
\begin{aligned}=\sum_{(i,j)\in\mathbb{K}^2}l(\frac{1}{\phi(t)}\|t_i-t_j\|_2,\frac{1}{\phi(s)}\|s&#39;_i-s&#39;_j\|_2),\end{aligned}  \\
\phi(x)&amp;
=\frac1{|\mathbb{K}^2|}\sum_{(i,j)\in\mathbb{K}^2}\|x_i-x_j\|_2,
\end{aligned}
\]</span> <span
class="math inline">\(\phi(\cdot)\)</span>是归一化因子。<span
class="math inline">\(l\)</span> 是 L1 loss。</p>
<h3 id="response-based-distillation">Response-based Distillation</h3>
<p>对检测头的整个输出进行提炼会损害学生模型的性能。
这可能是由于检测任务中正负样本的不平衡以及过多的负样本带来的噪声造成的。最近，一些检测蒸馏方法只对检测头的正样本进行蒸馏，而忽略了具有判别能力的负样本的正则化效应。因此，我们根据选定的
GI 为分类分支和回归分支设计了蒸馏掩码，事实证明这比仅使用 GT
标签作为蒸馏掩码更有效。</p>
<p>然而，由于不同模型对探测头输出的定义不同，我们提出了一个针对不同模型对探测头进行蒸馏的通用框架。首先，基于
GI 的蒸馏掩码计算如下： <span class="math display">\[
M=F_{Assign}(GIs),
\]</span> F是标签分配算法。例如，对于 RetinaNet，我们使用锚点和 GI
之间的 IoU 来确定是否屏蔽。对于 FCOS，GI
以外的所有输出都是屏蔽的。于是Loss为： <span class="math display">\[
\begin{aligned}L_{Response}=\frac{1}{N_m}\sum_{i=1}^{R}M_i\left(\alpha
L_{cls}\left(y_t^i,y_s^i\right)+\beta
L_{reg}\left(r_t^i,r_s^i\right)\right)\\
N_m=\sum_{i=1}^RM_i
\end{aligned}
\]</span> 其中y是分类头输出，r是回归头输出。</p>
<h3 id="overall-loss-function-1">Overall loss function</h3>
<p><span class="math display">\[
\begin{aligned}L&amp;=L_{GT}+\lambda_1L_{Feature}+\lambda_2L_{Relation}+\lambda_3L_{Response}\end{aligned}
\]</span></p>
<h2
id="distilling-object-detectors-via-decoupled-features-cvpr-2021">Distilling
Object Detectors via Decoupled Features (CVPR 2021)</h2>
<p>不足：</p>
<ul>
<li>以往的paper都没有使用背景区域进行蒸馏学习，只使用了目标区域</li>
</ul>
<p>创新点：</p>
<ul>
<li>使用解耦的目标区域和背景区域进行蒸馏学习</li>
</ul>
<p><img src="/img/cv/image-20230918105121610.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="decouple-intermediate-features-in-distillation">Decouple
Intermediate Features in Distillation</h3>
<p>我们得出的结论是，中间特征中的背景区域可以补充目标区域，进一步帮助学生检测器的训练。利用ground-truth来生成二值化的掩码，随后使用二值化的掩码来区分前景和背景，然后解耦的进行损失值的计算。
<span class="math display">\[
\begin{aligned}&amp;\mathcal{L}_{fea}=\frac{\alpha_{obj}}{2N_{obj}}\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C}M_{h,w}(\phi(\mathcal{S}_{h,w,c})-\mathcal{T}_{h,w,c})^2\\&amp;+\frac{\alpha_{bg}}{2N_{bg}}\sum_{h=1}^{H}\sum_{w=1}^{W}\sum_{c=1}^{C}(1-M_{h,w})(\phi(\mathcal{S}_{h,w,c})-\mathcal{T}_{h,w,c})^2,\end{aligned}
\]</span></p>
<h3 id="decouple-region-proposals-in-distillation">Decouple Region
Proposals in Distillation</h3>
<p>在蒸馏检测头时将区域建议分为positive建议和negative建议，所以还是对教师和学生的预测结果进行软化：
<span class="math display">\[
\begin{aligned}p^{s,T_{obj}}(c\mid\theta^s)&amp;=\frac{exp(z_c^s/T_{obj})}{\sum_{j=1}^Cexp(z_j^s/T_{obj})},c\in
Y\\p^{t,T_{obj}}(c\mid\theta^t)&amp;=\frac{exp(z_c^t/T_{obj})}{\sum_{j=1}^Cexp(z_j^t/T_{obj})},c\in
Y\end{aligned}
\]</span> 最后就进行positive和negative的损失计算： <span
class="math display">\[
\begin{gathered}
\begin{aligned}\mathcal{L}_{cls}&amp;=\frac{\beta_{obj}}{K_{obj}}\sum_{i=1}^{K}b_i\mathcal{L}_{KL}(p_i^{s,T_{obj}},p_i^{t,T_{obj}})\end{aligned}
\\
+\frac{\beta_{\boldsymbol{b}g}}{K_{\boldsymbol{b}g}}\sum_{i=1}^K{(1-b_i)\mathcal{L}_{KL}(p_i^{s,T_{\boldsymbol{b}g}},p_i^{t,T_{\boldsymbol{b}g}})}
\\
\begin{aligned}\mathcal{L}_{KL}(p^{s,T},p^{t,T})&amp;=T^2\sum_{c=1}^Cp^{t,T}(c|\theta^t)\log\frac{p^{t,T}(c|\theta^t)}{p^{s,T}(c|\theta^s)}\end{aligned}
\end{gathered}
\]</span></p>
<h2
id="instance-conditional-knowledge-distillation-for-object-detection-nips2021">Instance-Conditional
Knowledge Distillation for Object Detection (NIPS2021)</h2>
<p>不足：</p>
<ul>
<li>许多方法会忽略信息量大的上下文区域，或涉及缜密的决策</li>
<li>虽然注意力能为辨别区域提供固有提示，但激活与检测知识之间的关系仍不明确。</li>
</ul>
<p>创新点：</p>
<ul>
<li>我们设计了一个条件解码模块来定位知识，每个实例之间的相关性由实例感知注意力来计算</li>
</ul>
<p><img src="/img/cv/image-20230918172153838.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="overview">Overview</h3>
<p>为了促进KD，作者提出在教师和学生之间传输实例条件知识，Loss为： <span
class="math display">\[
\mathcal{L}_{distill}=\sum_{i=1}^N\mathcal{L}_d(\kappa_i^\mathcal{S},\kappa_i^\mathcal{T})
\]</span> 其中<span
class="math inline">\(\kappa_i^{\mathcal{T}}=\mathcal{G}(\mathcal{T},
y_i)\)</span>，<span
class="math inline">\(\mathcal{G}\)</span>是实例条件解码模块，由辅助loss进行优化。</p>
<h3 id="instance-conditional-knowledge">Instance-conditional
Knowledge</h3>
<p>实例条件知识由两部分计算得出(1) 无条件知识 (2) 实例条件</p>
<ul>
<li><p>无条件知识 <span class="math inline">\(\mathcal{T}\)</span> ,
表示教师探测器提供的所有信息。将多尺度特征表示为 <span
class="math inline">\(\mathcal{T}=\{X_p \in \mathbb{R}^{D\times
H_p\times W_p}\}_{p\in \mathcal{P}}\)</span> ，其中 <span
class="math inline">\(\mathcal{P}\)</span> 是空间上的分辨率，D
是维度。沿着空间维度方向对不同尺度的表征进行concat，我们就会获得 <span
class="math inline">\(A^T\in \mathbb{R}^{L \times D}\)</span> ，其中
<span class="math inline">\(L=\sum_{p \in \mathcal{P}}H_pW_p\)</span>
是各尺度像素总数之和</p></li>
<li><p>实例条件最初描述的是人类观察到的物体，用 <span
class="math inline">\(\mathcal{Y} = \{y_i\}^N_{i=1}\)</span> 表示，其中
N 是物体编号，<span class="math inline">\(y_i = (c_i, b_i)\)</span> 是第
i 个实例的注释，包括类别 <span class="math inline">\(c_i\)</span>
和框位置 <span class="math inline">\(b_i = (x_i, y_i, w_i,
h_i)\)</span>，其中指定了定位和尺寸信息。</p>
<p>为了为每个实例生成可学习的嵌入，注释被映射到隐藏空间中的查询特征向量
<span
class="math inline">\(q_i\)</span>，该向量指定了收集所需知识的条件：
<span class="math display">\[
\mathbf{q}_i=\mathcal{F}_q(\mathcal{E}(\mathrm{y}_i)),\mathrm{~}\mathbf{q}_i\in\mathbb{R}^D
\]</span> 其中 <span class="math inline">\(\mathcal{E} (\cdot)\)</span>
是一个编码方法，以及 <span class="math inline">\(\mathcal{F}_q\)</span>
是一个MLP</p>
<p>我们通过测量相关反应，从给定 <span
class="math inline">\(q_i\)</span>的 <span
class="math inline">\(\mathcal{T}\)</span>
中检索知识。通过多头注意力机制进行计算得出。 <span
class="math display">\[
\begin{aligned}
&amp;\mathrm{K}_{j}^{\mathcal{T}}&amp;&amp;
=\mathcal{F}_j^k(\mathrm{A}^{\mathcal{T}}+\mathcal{F}_{pe}(\mathrm{P})),\text{
K}_j^{\mathcal{T}}\in\mathbb{R}^{L\times d}  \\
&amp;\mathrm{V}_j^{\mathcal{T}}&amp;&amp;
=\mathcal{F}_j^v(\mathrm{A}^{\mathcal{T}}),\text{
V}_j^{\mathcal{T}}\in\mathbb{R}^{L\times d}  \\
&amp;\mathbf{q}_{ij}&amp;&amp;
=\mathcal{F}_j^q(\mathbf{q}_i),\mathbf{q}_{ij}\in\mathbb{R}^d  \\
&amp;\mathbf{m}_{ij}&amp;&amp;
=softmax(\frac{\mathrm{K}_j^T\mathbf{q}_{ij}}{\sqrt{d}}),\mathbf{m}_{ij}\in\mathbb{R}^L
\end{aligned}
\]</span></p></li>
</ul>
<h3 id="auxiliary-task">Auxiliary Task</h3>
<p>等下次再看，不太懂</p>
<h2
id="focal-and-global-knowledge-distillation-for-detectors-cvpr2022">Focal
and Global Knowledge Distillation for Detectors (CVPR2022)</h2>
<p>不足：</p>
<ul>
<li>将前景和背景不进行解耦一起进行蒸馏结果是最差的，这一现象表明，特征图的不均匀差异会对蒸馏产生负面影响。</li>
<li>再往深处想，不仅前景和背景之间存在负面影响，像素和通道之间也存在负面影响。</li>
<li>缺乏对全局信息的提取。</li>
</ul>
<p>创新点：</p>
<ul>
<li>提出focal蒸馏。在分离前景和背景的同时，焦点提炼还能计算出教师特征中不同像素和通道的关注度，让学生关注教师的关键像素和通道。</li>
<li>提出global蒸馏。我们利用 GcBlock
提取不同像素之间的关系，然后将其从教师提炼到学生。</li>
</ul>
<p><img src="/img/cv/image-20230918174811736.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="focal-distillation">Focal Distillation</h3>
<p>针对前景和背景不平衡的问题，我们提出了焦点蒸馏法来分离图像，引导学生关注关键像素和通道。对于蒸馏区域的对比如下图所示:</p>
<p><img src="/img/cv/image-20230918175405238.png" srcset="/img/loading.gif" lazyload /></p>
<p>首先还是利用ground-truth来得到二值化掩码M。在不同的图像中，像素数量和前景与背景的比例差异很大。因此，为了对不同目标一视同仁，平衡前景和背景的损失，我们将比例掩码
S 设置为： <span class="math display">\[
\begin{aligned}S_{i,j}&amp;=\begin{cases}\frac1{H_rW_r},&amp;\mathrm{if~}\left(i,j\right)\in
r\\\frac1{N_{bg}},&amp;\mathrm{Otherwise}&amp;\end{cases}\\\\N_{bg}&amp;=\sum_{i=1}^H\sum_{j=1}^W(1-M_{i,j})\end{aligned}
\]</span>
其中r就是ground-truth。如果一个像素属于不同的目标，我们会选择最小的方框来计算
S。</p>
<p>随后先选择焦点像素和通道，然后得到相应的注意力掩码。我们分别计算不同像素和不同通道的绝对平均值：
<span class="math display">\[
\begin{aligned}G^S(F)&amp;=\frac1C\cdot\sum_{c=1}^C|F_c|\\\\G^C(F)&amp;=\frac1{HW}\cdot\sum_{i=1}^H\sum_{j=1}^W|F_{i,j}|\end{aligned}
\]</span> 然后得到注意力机制掩码： <span class="math display">\[
\begin{aligned}A^S(F)&amp;=H\cdot W\cdot
softmax\big(G^S(F)/T\big)\\\\A^C(F)&amp;=C\cdot
softmax\big(G^C(F)/T\big)\end{aligned}
\]</span> 得到特征损失函数： <span class="math display">\[
\begin{aligned}L_{fea}&amp;=\alpha\sum_{k=1}^C\sum_{i=1}^H\sum_{j=1}^WM_{i,j}S_{i,j}A_{i,j}^SA_k^C\left(F_{k,i,j}^T-f(F_{k,i,j}^S)\right)^2\\&amp;+\beta\sum_{k=1}^C\sum_{i=1}^H\sum_{j=1}^W(1-M_{i,j})S_{i,j}A_{i,j}^SA_k^C\left(F_{k,i,j}^T-f(F_{k,i,j}^S)\right)^2\end{aligned}
\]</span>
此外，我们还利用注意力损失使学生检测器模仿教师检测器的空间和通道注意力掩码，其公式为：
<span class="math display">\[
    L_{at}=\gamma\cdot\left(l(A_t^S,A_S^S)+l(A_t^C,A_S^C)\right)
\]</span> 最后的Focal loss为： <span class="math display">\[
L_{focal}=L_{fea}+L_{at}
\]</span></p>
<h3 id="global-distillation">Global Distillation</h3>
<p>利用 GcBlock
在单幅图像中捕捉全局关系信息，并迫使学生探测器从教师那里学习关系。全局Loss为：
<span class="math display">\[
\begin{aligned}
L_{global}=&amp;
\lambda\cdot\sum\left(\mathcal{R}(F^T)-\mathcal{R}(F^S)\right)^2  \\
\mathcal{R}(F)=&amp; \begin{aligned}F+W_{v2}(ReLU(LN(W_{v1}\end{aligned}
\begin{aligned}(\sum_{j=1}^{N_p}\frac{e^{W_kF_j}}{\sum_{m=1}^{N_p}e^{W_kF_M}}F_j))))\end{aligned}
\end{aligned}
\]</span></p>
<h3 id="overall-loss">Overall loss</h3>
<p>于是总的Loss为： <span class="math display">\[
L=L_{original}+L_{focal}+L_{global}
\]</span></p>
<h3 id="代码实现">代码实现</h3>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_fea_loss</span>(<span class="hljs-params">self, preds_S, preds_T, Mask_fg, Mask_bg, C_s, C_t, S_s, S_t</span>):<br>        loss_mse = nn.MSELoss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)<br><br>        Mask_fg = Mask_fg.unsqueeze(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># [N, 1, H, W]</span><br>        Mask_bg = Mask_bg.unsqueeze(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># [N, 1, H, W]</span><br><br>        C_t = C_t.unsqueeze(dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># [N, C, 1]</span><br>        C_t = C_t.unsqueeze(dim=-<span class="hljs-number">1</span>)  <span class="hljs-comment"># [N, C, 1, 1]</span><br><br>        S_t = S_t.unsqueeze(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># [N, 1, H, W]</span><br><br>        <span class="hljs-comment"># use `sqrt` because the feature loss is a mse loss!</span><br>        <span class="hljs-comment"># the following two sections are using to get attention-based feature map</span><br>        <span class="hljs-comment"># and then decouple the background and foreground.</span><br>        fea_t = torch.mul(preds_T, torch.sqrt(S_t))<br>        fea_t = torch.mul(fea_t, torch.sqrt(C_t))<br>        fg_fea_t = torch.mul(fea_t, torch.sqrt(Mask_fg))<br>        bg_fea_t = torch.mul(fea_t, torch.sqrt(Mask_bg))<br><br>        fea_s = torch.mul(preds_S, torch.sqrt(S_t))<br>        fea_s = torch.mul(fea_s, torch.sqrt(C_t))<br>        fg_fea_s = torch.mul(fea_s, torch.sqrt(Mask_fg))<br>        bg_fea_s = torch.mul(fea_s, torch.sqrt(Mask_bg))<br><br>        <span class="hljs-comment"># get loss of background and foreground feature map, len(maks_x) = batch size (N)</span><br>        fg_loss = loss_mse(fg_fea_s, fg_fea_t)/<span class="hljs-built_in">len</span>(Mask_fg)<br>        bg_loss = loss_mse(bg_fea_s, bg_fea_t)/<span class="hljs-built_in">len</span>(Mask_bg)<br><br>        <span class="hljs-keyword">return</span> fg_loss, bg_loss<br></code></pre></div></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_mask_loss</span>(<span class="hljs-params">self, C_s, C_t, S_s, S_t</span>):<br>      <span class="hljs-comment"># get L1 loss of attention map</span><br>      mask_loss = torch.<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">abs</span>((C_s-C_t)))/<span class="hljs-built_in">len</span>(C_s) + \<br>          torch.<span class="hljs-built_in">sum</span>(torch.<span class="hljs-built_in">abs</span>((S_s-S_t)))/<span class="hljs-built_in">len</span>(S_s)<br><br>      <span class="hljs-keyword">return</span> mask_loss<br></code></pre></div></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># GC block</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">spatial_pool</span>(<span class="hljs-params">self, x, in_type</span>):<br>        batch, channel, width, height = x.size()<br>        input_x = x<br>        <span class="hljs-comment"># [N, C, H * W]</span><br>        input_x = input_x.view(batch, channel, height * width)<br>        <span class="hljs-comment"># [N, 1, C, H * W]</span><br>        input_x = input_x.unsqueeze(<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># [N, 1, H, W]</span><br>        <span class="hljs-keyword">if</span> in_type == <span class="hljs-number">0</span>:<br>            context_mask = self.conv_mask_s(x)<br>        <span class="hljs-keyword">else</span>:<br>            context_mask = self.conv_mask_t(x)<br>        <span class="hljs-comment"># [N, 1, H * W]</span><br>        context_mask = context_mask.view(batch, <span class="hljs-number">1</span>, height * width)<br>        <span class="hljs-comment"># [N, 1, H * W]</span><br>        context_mask = F.softmax(context_mask, dim=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># [N, 1, H * W, 1]</span><br>        context_mask = context_mask.unsqueeze(-<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># [N, 1, C, 1]</span><br>        context = torch.matmul(input_x, context_mask)<br>        <span class="hljs-comment"># [N, C, 1, 1]</span><br>        context = context.view(batch, channel, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>        <span class="hljs-keyword">return</span> context<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_rela_loss</span>(<span class="hljs-params">self, preds_S, preds_T</span>):<br>        loss_mse = nn.MSELoss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)<br><br>        context_s = self.spatial_pool(preds_S, <span class="hljs-number">0</span>)<br>        context_t = self.spatial_pool(preds_T, <span class="hljs-number">1</span>)<br><br>        out_s = preds_S<br>        out_t = preds_T<br><br>        channel_add_s = self.channel_add_conv_s(context_s)<br>        out_s = out_s + channel_add_s<br><br>        channel_add_t = self.channel_add_conv_t(context_t)<br>        out_t = out_t + channel_add_t<br><br>        rela_loss = loss_mse(out_s, out_t)/<span class="hljs-built_in">len</span>(out_s)<br><br>        <span class="hljs-keyword">return</span> rela_loss<br></code></pre></div></td></tr></table></figure>
<h2
id="channel-wise-knowledge-distillation-for-dense-prediction-iccv-2021">Channel-wise
Knowledge Distillation for Dense Prediction (ICCV 2021)</h2>
<p>不足：</p>
<ul>
<li>以往蒸馏方法都是挖掘空间上的知识</li>
</ul>
<p>创新点：</p>
<ul>
<li>对通道方向上进行蒸馏</li>
</ul>
<p><img src="/img/cv/image-20230919163725146.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="channel-wise-distillation">Channel-wise Distillation</h3>
<p>整体公式如下所示： <span class="math display">\[
\varphi(\phi(y^T),\phi(y^S))=\varphi(\phi(y_c^T),\phi(y_c^S)).
\]</span> 其中 <span class="math inline">\(\phi (\cdot)\)</span>
代表在每个通道内进行softmax操作，<span
class="math inline">\(\varphi(\cdot))\)</span>
用于评估教师网络与学生网络通道分布之间的差异。然后使用KL散度： <span
class="math display">\[
\varphi\big(y^T,y^S\big)=\frac{\mathcal{T}^2}C\sum_{c=1}^C\sum_{i=1}^{W\cdot
H}\phi(y_{c,i}^T)\cdot\log\Big[\frac{\phi(y_{c,i}^T)}{\phi(y_{c,i}^S)}\Big].
\]</span>
这样可以让学生网络倾向于在前景显著性中产生相似的激活分布，而教师网络中对应于背景区域的激活对学习的影响较小。</p>
<h2
id="g-detkd-towards-general-distillation-framework-for-object-detectors-via-contrastive-and-semantic-guided-feature-imitationiccv-2021">G-DetKD:
Towards General Distillation Framework for Object Detectors via
Contrastive and Semantic-guided Feature Imitation(ICCV 2021)</h2>
<p>不足：</p>
<ul>
<li>以往的目标检测模型都是划分前景背景来进行蒸馏，但是都是使用在较老没有FPN的目标检测模型上</li>
<li>以往的目标检测蒸馏方法都不能使用在异构的目标检测器上</li>
<li>不加区分地对所有层级应用相同的掩码，可能会引入无响应特征层级的噪音</li>
</ul>
<p>创新点：</p>
<ul>
<li>带有语义感知软匹配机制的新型语义引导特征模仿方法（SGFI）</li>
<li>对比知识提炼（CKD），以捕捉教师不同特征区域之间的关系所编码的信息。</li>
</ul>
<h3 id="semantic-guided-feature-imitation-sgfi">Semantic-Guided Feature
Imitation (SGFI)</h3>
<p><img src="/img/cv/image-20230922150349750.png" srcset="/img/loading.gif" lazyload /></p>
<p>这部分就用一个类似注意力机制的办法来进行蒸馏学习语义信息。首先从启发式指定的金字塔级别中提取教师
<span class="math inline">\(T_i \in R^{H\times W \times C}\)</span>
，学生特征则是多层中提取的 <span class="math inline">\(S_i \in R^{L
\times H \times W \times C}\)</span>，整体如下所示， <span
class="math display">\[
\begin{gathered}
K_{s_{i}}=f_{embed}\left(f_{adap}\left(S_{i}\right)\right),K_{t_{i}}=f_{embed}\left(T_{i}\right)
\\
\begin{aligned}\alpha_i=softmax\left(\frac{K_{s_i}K_{t_i}^T}{\tau}\right)\end{aligned}
\\
\begin{aligned}S_{agg_i}&amp;=\sum_{l=1}^{L}\alpha_i^l\times
f_{adap}\left(S_i^l\right)\end{aligned} \\
L_{feat}=\frac{1}{N}\sum_{i=1}^{N}\left(MSE\left(S_{agg_{i}},T_{i}\right)\right)
\end{gathered}
\]</span></p>
<h3
id="exploiting-region-relationship-with-contrastive-kd-ckd">Exploiting
Region Relationship with Contrastive KD (CKD)</h3>
<p>首先给定一个集合B，其中包含N个RoI的bounding box（<span
class="math inline">\(B={bbox}_{1,2,...,N}\)</span>），他们的对应表示是<span
class="math inline">\(\{r^i_s, r^i_t\}_{1,...,N}\)</span>
，是在输出层前的嵌入层提取的。其中对比对(contrastive
pair)的定义为：positive为相同box，negative为不同box（<span
class="math inline">\(x_{pos}=\{ r_s^i, r_t^i\}, x_{neg}=\{r^i_s,
r^j_t\}(i \ne j)\)</span>），目标是从包含K个负数对的 <span
class="math inline">\(S=\begin{Bmatrix}x_{pos},x_{neg}^1,x_{neg}^2,...,x_{neg}^K\end{Bmatrix}\)</span>
中识别出正数对，以 InfoNCE 损失的形式实现： <span
class="math display">\[
\begin{aligned}L_{ckd}&amp;=\frac1N\sum_{i=1}^N-\log\frac{g\left(r_s^i,r_t^i\right)}{\sum_{j=0}^Kg\left(r_s^i,r_t^j\right)}\end{aligned}
\]</span> g
是估计为正对的概率的批判函数（主要思想就是使用cos相似度来计算概率）:
<span class="math display">\[
g\left(r_{s},r_{t}\right)=\exp\left(\frac{f_{\theta}\left(r_{s}\right)\cdot
f_{\theta}\left(r_{t}\right)}{\parallel
f_{\theta}\left(r_{s}\right)\parallel\cdot\parallel
f_{\theta}\left(r_{t}\right)\parallel}\cdot\frac1\gamma\right)
\]</span></p>
<h2
id="structural-knowledge-distillation-for-object-detectionneurlps-2022">Structural
Knowledge Distillation for Object Detection(NeurlPS 2022)</h2>
<p>创新点：</p>
<ul>
<li>不使用更复杂的采样机制去提升 l -正则化所带来的弊端</li>
<li>通过交叉使得学生能够获取更多的知识</li>
</ul>
<p><img src="/img/cv/image-20231008104938634.png" srcset="/img/loading.gif" lazyload /></p>
<h3 id="overview-1">Overview</h3>
<p>作者选择neck的输出端分别从教师和学生那里选择中间表征 $^{C,H,W} <span
class="math inline">\(和\)</span>^{C,H,W}$ ，整体蒸馏loss为： <span
class="math display">\[
\mathcal{L}_{feat}=\sum_{r=1}^R\frac1{N_r}\sum_{h=1}^H\sum_{w=1}^W\sum_{c=1}^C\mathcal{L}_{\varepsilon}\left(\nu\left(\phi\left(\mathcal{S}_{r,h,w,c}\right)\right),\nu\left(\mathcal{T}_{r,h,w,c}\right)\right)
\]</span> 其中R是neck的输出数量，<span
class="math inline">\(\nu(\cdot)\)</span>
是归一化方法，作者使用min-max缩放， <span
class="math inline">\(\phi(\cdot)\)</span>
是适应层，为了令T和S的维度相匹配，作者使用一个1x1的卷积。</p>
<h3 id="measuring-difference">Measuring Difference</h3>
<p>显然，L-norm
函数无法捕捉到特征之间的空间关系。为了捕捉二阶信息，我们需要至少设计两个特征位置，因此我们将问题从point-wise比较修改成patch-wise比较。对于每个patch，我们提取出三个基本属性：均值
<span class="math inline">\(\mu\)</span>，方差 <span
class="math inline">\(\sigma^2\)</span> 和交叉相关性，后者捕捉了 S 和 T
之间的关系。我们使用大小为11 x 11、<span
class="math inline">\(\sigma_F\)</span> =1.5
的高斯加权补丁进行计算。SSIM框架对每种属性进行了比较，由三个部分组成：亮度
<span class="math inline">\(l\)</span>，对比度 <span
class="math inline">\(c\)</span>，结构 <span
class="math inline">\(s\)</span>，其定义如下： <span
class="math display">\[
l=\frac{2\mu_{\mathcal{S}}\mu_{\mathcal{T}}+C_1}{\mu_{\mathcal{S}}^2+\mu_{\mathcal{T}}^2+C_1}\quad\mathrm{(3a)}\quad
c=\frac{2\sigma_{\mathcal{S}}\sigma_{\mathcal{T}}+C_2}{\sigma_{\mathcal{S}}^2+\sigma_{\mathcal{T}}^2+C_2}\quad\mathrm{(3b)}\quad
s=\frac{\sigma_{\mathcal{S}\mathcal{T}}+C_3}{\sigma_{\mathcal{S}}\sigma_{\mathcal{T}}+C_3}\quad\mathrm{(3c)}
\]</span> 为了防止不稳定性，<span
class="math inline">\(C_1=(K_1L)^2,C_2=(K_2L)^2,C_3=C_2/2\)</span>，其中
L 是特征图的动态范围，K1=0.01，K2=0.03。从公式中可以看到更加重视 l 和 c
的相对变化。于是最后可以将这三个部分结合到一起： <span
class="math display">\[
\ell_{\mathbf{SSIM}}:\mathcal{L}_{\varepsilon}=(1-\mathrm{SSIM})/2=(1-\left(l^\alpha\cdot
c^\beta\cdot s^\gamma\right))/2
\]</span> 其中 <span class="math inline">\(\alpha,\beta,\gamma\)</span>
的默认值为1.0。由于我们的方法纯粹基于特征，因此与头部或边界框标签的类型无关，我们只需使用加权因子
<span class="math inline">\(\lambda\)</span> 将 <span
class="math inline">\(\mathcal{L}_{feat}\)</span>
添加到现有的检测目标函数 <span
class="math inline">\(\mathcal{L}_{det}\)</span>（通常为 <span
class="math inline">\(\mathcal{L}_{cls}\)</span> 和 <span
class="math inline">\(\mathcal{L}_{reg}\)</span>）中，从而得到以下总体训练目标：
<span class="math display">\[
\mathcal{L}=\lambda\mathcal{L}_{feat}+\mathcal{L}_{det}
\]</span></p>
<h2
id="pkd-general-distillation-framework-for-object-detectors-via-pearson-correlation-coefficientneurlps-2022">PKD:
General Distillation Framework for Object Detectors via Pearson
Correlation Coefficient(NeurlPS 2022)</h2>
<p>不足：</p>
<ul>
<li>教师和学生之间的特征量级差异可能会对学生施加过于严格的限制</li>
<li>教师模型中特征量级较大的 FPN
层和通道可能会主导蒸馏损失的梯度，这将压倒 KD
中其他特征的效果，并引入大量噪声</li>
</ul>
<p>创新点：</p>
<ul>
<li>作者认为，即使师生探测器对是异质的，FPN
特征模仿也能成功地提炼知识。</li>
<li>建议用 PCC 来模仿 FPN
特征，以关注关系信息，并放宽学生特征大小的分布限制。</li>
</ul>
<p><img src="/img/cv/image-20231010163904386.png" srcset="/img/loading.gif" lazyload /></p>
<p>我们建议首先对教师和学生的特征进行归一化处理，使其均值为零，方差为单位，并最小化归一化特征之间的
MSE。此外，我们还希望归一化服从卷积属性--这样，同一特征图中位于不同位置的不同元素就会以相同的方式归一化。让
<span class="math inline">\(\mathbb{B}\)</span>
成为特征图中所有值的集合，既包括 mini-batch
元素，也包括空间位置。因此，对于大小为 b 的 mini-batch 和大小为 h × w
的特征图，我们使用大小为 <span
class="math inline">\(m=\|\mathbb{B}\|=b\cdot hw\)</span>
的有效迷你批。令 <span class="math inline">\(s^{(c)} \in \mathbb{R} ^
m\)</span>
作为FPN一个batch输出的第c个通道。之后我们得到来自学生和老师的归一化值
<span class="math inline">\(\hat{s}_{1...m}\)</span> 和 <span
class="math inline">\(\hat{t}_{1...m}\)</span>
。作者不使用掩码来选择重要特征，而是根据整个特征图进行操作。因此蒸馏loss就是：
<span class="math display">\[
\mathcal{L}_{FPN}=\frac1{2m}\sum_{i=1}^m(\hat{s}_i-\hat{t}_i)^2
\]</span>
进一步看，最小化上述损失等于最大化学生和教师的预归一化特征之间的皮尔逊系数(PCC)。PCC如下：
<span class="math display">\[
r(s,\boldsymbol{t})=\frac{\sum_{i=1}^m(s_i-\mu_s)(t_i-\mu_t)}{\sqrt{\sum_{i=1}^m(s_i-\mu_s)^2}\sqrt{\sum_{i=1}^m(t_i-\mu_t)^2}}.
\]</span> 因为 <span
class="math inline">\(\hat{s},\hat{t}\sim\mathcal{N}(0,1)\)</span>，我们可以得到
<span
class="math inline">\(\frac1{m-1}\sum_i\hat{s}_i^2=1,\frac1{m-1}\sum_i\hat{t}_i^2=1\)</span>。那么我们可以将公式重拟为：
<span class="math display">\[
\begin{aligned}
\mathcal{L}_{FPN}&amp;
=\frac1{2m}\left((2m-2)-2\sum_{i=1}^m\hat{s}_i\hat{t}_i\right)  \\
&amp;=\frac{2m-2}{2m}(1-r(s,\boldsymbol{t}))\approx1-r(s,\boldsymbol{t})
\end{aligned}
\]</span> PCC范围是[-1,
1]，如果PCC为1就代表二者具有线性关系在同一条支线上，s增加t也增加。</p>
<p>总之，PCC
专注于来自教师的关系信息，放松了对学生特征大小的分布限制。此外，它还消除了占主导地位的
FPN
阶段和通道的负面影响，从而获得更好的性能。因此，归一化机制弥合了学生和教师激活模式之间的差距（见图
2）。因此，使用 PCC
进行特征模仿适用于异质检测器对。最后模型训练的Loss为：</p>
<p><span class="math display">\[
\mathcal{L}=\mathcal{L}_{GT}+\alpha\mathcal{L}_{FPN},
\]</span></p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/">Deep Learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/cv/">cv</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/05/22/math/">
                        <span class="hidden-mobile">math</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                

              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
