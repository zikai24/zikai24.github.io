

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="肘子开">
  <meta name="keywords" content="">
  
    <meta name="description" content="1.梯度消失 1.1 梯度消失问题 梯度消失问题：当这些梯度很小的时候，反向传播的越深入，梯度信号就会变得越来越小  1.2 梯度消失证明简述 \[ \boldsymbol{h}^{(t)}&#x3D;\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\b">
<meta property="og:type" content="article">
<meta property="og:title" content="第七讲-梯度消失问题与RNN变种">
<meta property="og:url" content="http://example.com/2022/05/13/%E7%AC%AC%E4%B8%83%E8%AE%B2-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%E4%B8%8ERNN%E5%8F%98%E7%A7%8D/index.html">
<meta property="og:site_name" content="肘子开的博客">
<meta property="og:description" content="1.梯度消失 1.1 梯度消失问题 梯度消失问题：当这些梯度很小的时候，反向传播的越深入，梯度信号就会变得越来越小  1.2 梯度消失证明简述 \[ \boldsymbol{h}^{(t)}&#x3D;\sigma\left(\boldsymbol{W}_{h} \boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x} \boldsymbol{x}^{(t)}+\b">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/1.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/2.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/3.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/4.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/5.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/6.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/7.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/8.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/9.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/10.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/11.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/12.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/13.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/14.png">
<meta property="og:image" content="http://example.com/img/nlp/第七讲/15.png">
<meta property="article:published_time" content="2022-05-13T10:50:43.000Z">
<meta property="article:modified_time" content="2022-05-13T12:35:06.761Z">
<meta property="article:author" content="肘子开">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/nlp/第七讲/1.png">
  
  
  <title>第七讲-梯度消失问题与RNN变种 - 肘子开的博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/nnfx-dark.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>肘子开的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg/bg2.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="第七讲-梯度消失问题与RNN变种">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-05-13 18:50" pubdate>
        2022年5月13日 晚上
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.1k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      43 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">第七讲-梯度消失问题与RNN变种</h1>
            
            <div class="markdown-body">
              <h1 id="梯度消失">1.梯度消失</h1>
<h2 id="梯度消失问题">1.1 梯度消失问题</h2>
<p>梯度消失问题：当这些梯度很小的时候，反向传播的越深入，梯度信号就会变得越来越小</p>
<p><img src="/img/nlp/第七讲/1.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="梯度消失证明简述">1.2 梯度消失证明简述</h2>
<p><span class="math display">\[
\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h}
\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}
\boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)
\]</span></p>
<p>因此通过<strong>链式法则</strong>得到： <span class="math display">\[
\frac{\partial \boldsymbol{h}^{(t)}}{\partial
\boldsymbol{h}^{(t-1)}}=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h}
\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}
\boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right) \boldsymbol{W}_{h}
\]</span> 考虑第<span
class="math inline">\(i\)</span>步上的损失梯度<span
class="math inline">\(J^{(i)}(\theta)\)</span>，相对于第<span
class="math inline">\(j\)</span>步上的隐藏状态<span
class="math inline">\(h^{(j)}\)</span>。如果权重矩阵<span
class="math inline">\(W_h\)</span>很小，那么这一项也会随着<span
class="math inline">\(i\)</span>和<span
class="math inline">\(j\)</span>的距离越来越远而变得越来越小</p>
<p>考虑矩阵的 L2 范数 <span class="math display">\[
\left\|\frac{\partial J^{(i)}(\theta)}{\partial
\boldsymbol{h}^{(j)}}\right\| \leq\left\|\frac{\partial
J^{(i)}(\theta)}{\partial
\boldsymbol{h}^{(i)}}\right\|\left\|\boldsymbol{W}_{h}\right\|^{(i-j)}
\prod_{j&lt;t \leq
i}\left\|\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h}
\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}
\boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right)\right\|
\]</span> Pascanu et al 表明，如果<span
class="math inline">\(W_h\)</span>的<strong>最大特征值</strong>&lt;1，梯度<span
class="math inline">\(\left\|\frac{\partial J^{(i)}(\theta)}{\partial
\boldsymbol{h}^{(j)}}\right\|\)</span>将呈<strong>指数衰减</strong></p>
<ul>
<li>这里的界限是1因为我们使用的非线性函数是sigmoid</li>
</ul>
<p>有一个类似的证明将一个<strong>最大的特征值</strong> &gt;
1与<strong>梯度爆炸</strong>联系起来</p>
<h2 id="为什么梯度消失是个问题">1.3 为什么梯度消失是个问题？</h2>
<p>来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多。因此，模型权重只会根据近期效应而不是长期效应进行更新。</p>
<p><img src="/img/nlp/第七讲/2.png" srcset="/img/loading.gif" lazyload /></p>
<p>另一种解释：<strong>梯度</strong>可以被看作是<strong>过去对未来的影响</strong>的衡量标准</p>
<p>如果梯度在较长一段距离内(从时间步<span
class="math inline">\(t\)</span>到<span
class="math inline">\(t+n\)</span>)变得越来越小，那么我们就不能判断：</p>
<ul>
<li>在数据中，步骤<span class="math inline">\(t\)</span>和<span
class="math inline">\(t+n\)</span>之间<strong>没有依赖关系</strong></li>
<li>我们用<strong>错误的参数</strong>来捕获<span
class="math inline">\(t\)</span>和<span
class="math inline">\(t+n\)</span>之间的真正依赖关系</li>
</ul>
<h2 id="梯度消失对rnn语言模型的影响">1.4
梯度消失对RNN语言模型的影响</h2>
<p><code>LM task: When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ________</code></p>
<p>为了从这个训练示例中学习，RNN-LM需要对第7步的 <code>tickets</code>
和最后的目标单词 <code>tickets</code>
之间的<strong>依赖关系建模</strong></p>
<p>但是如果梯度很小，模型就<strong>不能学习这种依赖关系</strong></p>
<ul>
<li>因此模型无法在测试时<strong>预测类似的长距离依赖关系</strong></li>
</ul>
<p><img src="/img/nlp/第七讲/3.png" srcset="/img/loading.gif" lazyload /></p>
<p>Correct answer: The writer of the books <u>is</u> planning a
sequel</p>
<ul>
<li><strong>语法近因</strong>：The <u>writer</u> of the books <u>is</u>
（正确）</li>
<li><strong>顺序近因</strong>：The writer of the <u>books</u> <u>is</u>
（不正确）</li>
</ul>
<p>由于梯度的消失，RNN-LMs更善于从<strong>顺序近因</strong>学习而不是<strong>语法近因</strong>，所以他们犯这种错误的频率比我们希望的要高[Linzen
et al . 2016]</p>
<h2 id="如何解决梯度消失问题">1.5 如何解决梯度消失问题？</h2>
<p>主要问题是RNN很难学习在多个时间步长的情况下保存信息，在普通的RNN中，隐藏状态不断被重写。
<span class="math display">\[
\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h}
\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}
\boldsymbol{x}^{(t)}+\boldsymbol{b}\right)
\]</span> 有没有更好结构的RNN？</p>
<h1 id="梯度爆炸">2.梯度爆炸</h1>
<h2 id="为什么梯度爆炸是个问题">2.1 为什么梯度爆炸是个问题？</h2>
<p>如果梯度过大，则SGD更新步骤过大，这可能导致<strong>错误的更新</strong>：我们更新的太多，导致错误的参数配置(损失很大)。</p>
<p>在最坏的情况下，这将导致网络中的 <strong>Inf</strong> 或
<strong>NaN</strong>(然后你必须从较早的检查点重新启动训练)</p>
<h2 id="梯度剪裁梯度爆炸的解决方案">2.2
梯度剪裁：梯度爆炸的解决方案</h2>
<p><strong>梯度裁剪</strong>：如果梯度的范数大于某个阈值，在应用SGD更新之前将其缩小</p>
<p><img src="/img/nlp/第七讲/4.png" srcset="/img/loading.gif" lazyload /></p>
<p><strong>直觉</strong>：朝着同样的方向迈出一步，但要小一点</p>
<p>这显示了一个简单RNN的损失面(隐藏层状态是一个标量不是一个向量)，悬崖是危险的，因为有陡坡</p>
<p><img src="/img/nlp/第七讲/5.png" srcset="/img/loading.gif" lazyload /></p>
<p>在左边，由于陡坡，梯度下降有<strong>两个非常大的步骤</strong>，导致攀登悬崖然后向右射击(都是<strong>坏的更新</strong>)</p>
<p>在右边，梯度剪裁减少了这些步骤的大小，所以参数调整不会有剧烈的波动</p>
<h1 id="长短时记忆网络lstm">3.长短时记忆网络(LSTM)</h1>
<h2 id="长短时记忆lstm">3.1 长短时记忆(LSTM)</h2>
<p>Hochreiter和Schmidhuber在1997年提出了一种RNN，用于解决梯度消失问题。</p>
<p>在第<span
class="math inline">\(t\)</span>步，有一个<strong>隐藏状态</strong><span
class="math inline">\(h^{(t)}\)</span>和一个<strong>单元状态</strong><span
class="math inline">\(c^{(t)}\)</span></p>
<ul>
<li>都是长度为<span class="math inline">\(n\)</span>的向量</li>
<li>单元存储长期信息</li>
<li>LSTM可以从单元中<strong>擦除</strong>、<strong>写入</strong>和<strong>读取信息</strong></li>
</ul>
<p>信息被 擦除 / 写入 / 读取 的选择由三个对应的门控制</p>
<ul>
<li>门也是长度为<span class="math inline">\(n\)</span>的向量</li>
<li>在每个时间步长上，门的每个元素可以<strong>打开</strong>(1)、<strong>关闭</strong>(0)或介于两者之间</li>
<li><strong>门是动态的</strong>：它们的值是基于当前上下文计算的</li>
</ul>
<p><img src="/img/nlp/第七讲/6.png" srcset="/img/loading.gif" lazyload /></p>
<p>我们有一个输入序列<span
class="math inline">\(x^{(t)}\)</span>，我们将计算一个隐藏状态<span
class="math inline">\(h^{(t)}\)</span>和单元状态<span
class="math inline">\(c^{(t)}\)</span>的序列。在时间步<span
class="math inline">\(t\)</span>时</p>
<ul>
<li><p><strong>遗忘门</strong>：控制上一个单元状态的保存与遗忘</p></li>
<li><p><strong>输入门</strong>：控制写入单元格的新单元内容的哪些部分</p></li>
<li><p><strong>输出门</strong>：控制单元的哪些内容输出到隐藏状态</p></li>
<li><p><strong>新单元内容</strong>：这是要写入单元的新内容</p></li>
<li><p><strong>单元状态</strong>：删除(“忘记”)上次单元状态中的一些内容，并写入(“输入”)一些新的单元内容</p></li>
<li><p><strong>隐藏状态</strong>：从单元中读取(“output”)一些内容</p></li>
<li><p><strong>Sigmoid函数</strong>：所有的门的值都在0到1之间</p></li>
<li><p>通过逐元素的乘积来应用门</p></li>
<li><p>这些是长度相同(<span
class="math inline">\(n\)</span>)的向量</p></li>
</ul>
<p><img src="/img/nlp/第七讲/7.png" srcset="/img/loading.gif" lazyload /></p>
<p><img src="/img/nlp/第七讲/8.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="lstm如何解决梯度消失">3.2 LSTM如何解决梯度消失</h2>
<p>RNN的LSTM架构更容易保存许多时间步上的信息</p>
<ul>
<li>如果忘记门设置为记得每一时间步上的所有信息，那么单元中的信息被无限地保存</li>
<li>相比之下，普通RNN更难学习重复使用并且在隐藏状态中保存信息的矩阵<span
class="math inline">\(W_h\)</span></li>
</ul>
<p>LSTM并不保证没有<strong>梯度消失/爆炸</strong>，但它确实为模型提供了一种更容易的方法来学习远程依赖关系</p>
<h2 id="lstms现实世界的成功">3.3 LSTMs：现实世界的成功</h2>
<p>2013-2015年，LSTM开始实现最先进的结果</p>
<ul>
<li>成功的任务包括：手写识别、语音识别、机器翻译、解析、图像字幕</li>
<li><strong>LSTM成为主导方法</strong></li>
</ul>
<p>现在(2019年)，其他方法(<strong>如Transformers</strong>)在某些任务上变得更加主导</p>
<ul>
<li>例如在WMT(a MT conference + competition)中</li>
<li>在2016年WMT中，总结报告包含“RNN”44次</li>
<li>在2018年WMT中，总结报告包含“RNN”9次，“Transformers” 63次</li>
</ul>
<h1 id="gru网络">4.GRU网络</h1>
<h2 id="gated-recurrent-unitsgru">4.1 Gated Recurrent Units(GRU)</h2>
<p>Cho等人在2014年提出了LSTM的一个更简单的替代方案，在每个时间步<span
class="math inline">\(t\)</span>上，我们都有输入<span
class="math inline">\(x^{(t)}\)</span>和隐藏状态<span
class="math inline">\(h^{(t)}\)</span>(没有单元状态)</p>
<ul>
<li><p><strong>更新门</strong>：控制隐藏状态的哪些部分被更新，或者被保留</p></li>
<li><p><strong>重置门</strong>：控制之前隐藏状态的哪些部分被用于计算新内容</p></li>
<li><p><strong>新的隐藏状态内容</strong>：重置门选择之前隐藏状态的有用部分。使用这一部分和当前输入来计算新的隐藏状态内容</p></li>
<li><p><strong>隐藏状态</strong>：更新门同时控制从以前的隐藏状态保留的内容，以及更新到新的隐藏状态内容的内容</p></li>
</ul>
<p>这如何解决消失梯度？</p>
<ul>
<li>与LSTM类似，GRU使长期保存信息变得更容易(例如，将update
gate设置为0)</li>
</ul>
<p><img src="/img/nlp/第七讲/9.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="lstm-vs-gru">4.2 LSTM vs GRU</h2>
<p>研究人员提出了许多门控RNN变体，其中LSTM和GRU的应用最为广泛</p>
<p>最大的区别是<strong>GRU计算速度更快</strong>，参数更少。没有确凿的证据表明其中一个总是比另一个表现得更好。<strong>LSTM</strong>
是一个<strong>很好的默认选择</strong>(特别是当你的数据具有非常长的依赖关系，或者你有很多训练数据时)</p>
<p><strong>经验法则</strong>：从LSTM开始，但是如果你想要更有效率，就切换到GRU</p>
<h2 id="梯度消失爆炸只是rnn问题吗">4.3 梯度消失/爆炸只是RNN问题吗？</h2>
<p><strong>梯度消失/爆炸只是RNN问题吗</strong>？</p>
<p>并不是，这对于所有的神经结构(包括前馈和卷积网络)都是一个问题，尤其是对于深度结构</p>
<ul>
<li>由于链式法则/选择非线性函数，反向传播时梯度可以变得很小很小</li>
<li>因此，较低层次的学习非常缓慢(难以训练)</li>
<li>解决方案：大量新的深层前馈 /
卷积架构，<strong>添加更多的直接连接</strong>(从而使梯度可以流动)</li>
</ul>
<p>例如：</p>
<ul>
<li>残差连接又名“ResNet”,也称为跳转连接</li>
<li>默认情况下，标识连接保存信息</li>
<li>这使得深层网络更容易训练</li>
</ul>
<p>例如：</p>
<ul>
<li>密集连接又名“DenseNet”</li>
<li>直接将所有内容连接到所有内容</li>
</ul>
<p>例如：</p>
<ul>
<li>Highway连接又称“高速网络”</li>
<li>类似于残差连接，但标识连接与转换层由动态门控制</li>
<li>灵感来自LSTMs，但适用于深度前馈/卷积网络</li>
</ul>
<p>结论：虽然梯度消失/爆炸是一个普遍的问题，但由于重复乘以相同的权矩阵，RNN尤其不稳定[Bengio
et al, 1994]</p>
<h2 id="双向rnn动机">4.4 双向RNN：动机</h2>
<p><img src="/img/nlp/第七讲/10.png" srcset="/img/loading.gif" lazyload /></p>
<p>我们可以把这种隐藏状态看作是这个句子中单词“terribly”的一种表示。我们称之为上下文表示。</p>
<p>这些上下文表示只包含关于左上下文的信息(例如“the movie was”)。</p>
<p>那么正确的上下文呢?</p>
<ul>
<li>在这个例子中，“exciting”在右上下文中，它修饰了“terribly”的意思(从否定变为肯定)</li>
</ul>
<p><img src="/img/nlp/第七讲/11.png" srcset="/img/loading.gif" lazyload /></p>
<p>“terribly”的上下文表示同时具有左上下文和右上下文</p>
<p><img src="/img/nlp/第七讲/12.png" srcset="/img/loading.gif" lazyload /></p>
<p>这是一个表示“计算RNN的一个向前步骤”的通用符号——它可以是普通的、LSTM或GRU计算</p>
<p>我们认为这是一个双向RNN的“隐藏状态”。这就是我们传递给网络下一部分的东西</p>
<p>一般来说，这两个RNNs有各自的权重</p>
<p><img src="/img/nlp/第七讲/13.png" srcset="/img/loading.gif" lazyload /></p>
<p>双向箭头表示双向性，所描述的隐藏状态是正向+反向状态的连接</p>
<p>注意：双向RNNs只适用于访问整个输入序列的情况</p>
<ul>
<li>它们不适用于语言建模，因为在LM中，你只有左侧的上下文可用</li>
</ul>
<p>如果你有完整的输入序列(例如任何一种编码)，<strong>双向性是强大的</strong>(默认情况下你应该使用它)</p>
<p>例如，BERT(来自transformer的双向编码器表示)是一个基于双向性的强大的预训练的上下文表示系统</p>
<ul>
<li>你会在课程的后面学到更多关于BERT的知识!</li>
</ul>
<h2 id="深层rnn">4.5 深层RNN</h2>
<p>RNNs在一个维度上已经是“deep”(它们展开到许多时间步长)</p>
<p>我们还可以通过应用<strong>多个RNN</strong>使它们“深入”到另一个维度：这是一个多层RNN</p>
<p><strong>较低的RNN</strong>应该计算<strong>较低级别的特性</strong>，而<strong>较高的RNN</strong>应该计算<strong>较高级别的特性</strong></p>
<p>多层RNN也称为堆叠RNN</p>
<p><img src="/img/nlp/第七讲/14.png" srcset="/img/loading.gif" lazyload /></p>
<p>RNN层<span class="math inline">\(i\)</span>的隐藏状态是RNN层<span
class="math inline">\(i+1\)</span>的输入</p>
<h2 id="深层rnn在实践中的应用">4.6 深层RNN在实践中的应用</h2>
<p>高性能的RNNs通常是多层的(但没有卷积或前馈网络那么深)</p>
<p>例如：在2017年的一篇论文，Britz et al
发现在神经机器翻译中，2到4层RNN编码器是最好的,和4层RNN解码器</p>
<ul>
<li>但是，<strong>skip-connections</strong> /
<strong>dense-connections</strong> 需要训练更深RNNs(例如8层)</li>
<li>RNN无法并行化，计算代价过大，所以不会过深</li>
</ul>
<p>Transformer-based 的网络(如BERT)可以多达24层</p>
<ul>
<li>BERT 有很多skipping-like的连接</li>
</ul>
<h2 id="总结">4.7 总结</h2>
<ul>
<li>LSTM功能强大，但GRU速度更快</li>
<li>剪裁你的梯度</li>
<li>尽可能使用双向性</li>
<li>多层RNN功能强大，但如果很深可能需要跳接/密集连接</li>
</ul>
<p><img src="/img/nlp/第七讲/15.png" srcset="/img/loading.gif" lazyload /></p>
<p>参考：</p>
<p>http://www.showmeai.tech/article-detail/241</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/">Deep Learning</a>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/NLP/">NLP</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/05/14/%E7%AC%AC%E5%85%AB%E8%AE%B2-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E3%80%81seq2seq%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">第八讲-机器翻译、seq2seq与注意力机制</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/05/12/%E7%AC%AC%E5%85%AD%E8%AE%B2-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">
                        <span class="hidden-mobile">第六讲-循环神经网络与语言模型</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                

              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
