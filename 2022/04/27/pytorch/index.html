

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.png">
  <link rel="icon" href="/img/favicon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="肘子开">
  <meta name="keywords" content="">
  
    <meta name="description" content="PyTorch 1、DataSet DataSet：提供一种方法去获取数据及其label 使用的数据集为蜜蜂与蚂蚁的图像数据集，分别保存在dataset&#x2F;train&#x2F;bees以及dataset&#x2F;train&#x2F;ants DataSet需要通过继承重载才能使用，使用方法如下： 1234567891011121314151617181920212223242526272829303132from">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch">
<meta property="og:url" content="http://example.com/2022/04/27/pytorch/index.html">
<meta property="og:site_name" content="肘子开的博客">
<meta property="og:description" content="PyTorch 1、DataSet DataSet：提供一种方法去获取数据及其label 使用的数据集为蜜蜂与蚂蚁的图像数据集，分别保存在dataset&#x2F;train&#x2F;bees以及dataset&#x2F;train&#x2F;ants DataSet需要通过继承重载才能使用，使用方法如下： 1234567891011121314151617181920212223242526272829303132from">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/img/pytorch/1.png">
<meta property="og:image" content="http://example.com/img/pytorch/2.png">
<meta property="og:image" content="http://example.com/img/pytorch/4.png">
<meta property="og:image" content="http://example.com/img/pytorch/5.png">
<meta property="og:image" content="http://example.com/img/pytorch/3.png">
<meta property="og:image" content="http://example.com/img/pytorch/6.png">
<meta property="og:image" content="http://example.com/img/pytorch/7.png">
<meta property="og:image" content="http://example.com/img/pytorch/8.png">
<meta property="og:image" content="http://example.com/img/pytorch/9.png">
<meta property="og:image" content="http://example.com/img/pytorch/10.png">
<meta property="og:image" content="http://example.com/img/pytorch/11.png">
<meta property="og:image" content="http://example.com/img/pytorch/12.png">
<meta property="og:image" content="http://example.com/img/pytorch/13.png">
<meta property="og:image" content="http://example.com/img/pytorch/14.png">
<meta property="og:image" content="http://example.com/img/pytorch/15.png">
<meta property="article:published_time" content="2022-04-27T08:32:20.000Z">
<meta property="article:modified_time" content="2023-11-23T09:32:24.270Z">
<meta property="article:author" content="肘子开">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://example.com/img/pytorch/1.png">
  
  
  <title>pytorch - 肘子开的博客</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/nnfx-dark.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->


  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 6.1.0"></head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>肘子开的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/bg/bg2.png') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="pytorch">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-04-27 16:32" pubdate>
        2022年4月27日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      33k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      278 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">pytorch</h1>
            
            <div class="markdown-body">
              <h1 id="pytorch">PyTorch</h1>
<h2 id="dataset">1、DataSet</h2>
<p>DataSet：提供一种方法去获取数据及其label</p>
<p>使用的数据集为蜜蜂与蚂蚁的图像数据集，分别保存在dataset/train/bees以及dataset/train/ants</p>
<p>DataSet需要通过继承重载才能使用，使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root_dir, label_dir</span>):	<span class="hljs-comment"># read data &amp; preprocess</span><br>        self.root_dir = root_dir    <span class="hljs-comment"># 根目录</span><br>        self.label_dir = label_dir  <span class="hljs-comment"># 标记目录</span><br>        self.path = os.path.join(self.root_dir, self.label_dir)     <span class="hljs-comment"># 地址相加</span><br>        self.img_path = os.listdir(self.path)   <span class="hljs-comment"># 生成图片文件列表</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):	<span class="hljs-comment"># returns one sample at a time</span><br>        img_name = self.img_path[index]   <span class="hljs-comment"># 文件名</span><br>        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)<br>        img = Image.<span class="hljs-built_in">open</span>(img_item_path)<br>        label = self.label_dir<br>        <span class="hljs-keyword">return</span> img, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):	<span class="hljs-comment"># returns the size of the dataset</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_path)<br><br><br><span class="hljs-comment"># 读取蚂蚁和蜜蜂的图片数据集</span><br>root_dir = <span class="hljs-string">&quot;dataset/train&quot;</span><br>ants_label_dir = <span class="hljs-string">&quot;ants&quot;</span><br>bees_label_dir = <span class="hljs-string">&quot;bees&quot;</span><br>ants_dataset = MyData(root_dir, ants_label_dir)<br>bees_dataset = MyData(root_dir, bees_label_dir)<br><br><span class="hljs-comment"># 将两个数据集进行合并，成为训练数据集</span><br>train_dataset = ants_dataset + bees_dataset<br></code></pre></div></td></tr></table></figure>
<h2 id="dataloader">2、DataLoader</h2>
<p>DataSet就相当于是一整个数据集，而DataLoader是取出其中一部分到神经网络中进行使用。</p>
<p>使用的是torchvision所提供的数据集。DataLoader中的参数分别释义如下：</p>
<ul>
<li>dataset：我们所使用的数据集，即dataset类型数据</li>
<li>batch_size：一次抓取多少个数据</li>
<li>shuffle：抓取时是否打乱顺序</li>
<li>num_workers：代表创建了多少个worker进程，0表示只有主进程去加载batch数据，1表示有一个worker进程加载batch数据</li>
<li>drop_last：无法整除时，最后剩余的几条数据要不要去除</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 准备的测试数据集</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="hljs-literal">True</span>)<br><br>test_loader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">False</span>)<br><br>writer = SummaryWriter(<span class="hljs-string">&quot;dataloader&quot;</span>)<br>step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>    imgs, targets = data<br>    writer.add_images(<span class="hljs-string">&quot;test_data&quot;</span>, imgs, step)<br>    step = step + <span class="hljs-number">1</span><br><br>writer.close()<br></code></pre></div></td></tr></table></figure>
<p>最后显示结果如下：</p>
<p><img src="/img/pytorch/1.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="tensorboard">3、TensorBoard</h2>
<p>我们可以通过TensorBoard可以查看图像</p>
<h3 id="输出函数图像">3.1输出函数图像</h3>
<p>首先我们通过TensorBoard来绘制y=x的图像，我们需要先生成一个实例，随后通过add_scalar()方法来添加，参数分别为名称，y轴的值，x轴的值。</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-comment"># 创建一个实例,存储在logs文件夹下</span><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br><br><span class="hljs-comment"># y = x图像</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&quot;y=x&quot;</span>, i, i)<br><br>writer.close()<br></code></pre></div></td></tr></table></figure>
<p>完成后，我们可以通过控制台输入如下指令来观看图像</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensorboard --logdir=logs<br></code></pre></div></td></tr></table></figure>
<p>这时候我们可以通过6006端口来查看图像，但是如果有很多tensorboard都要查看呢，我们可以选择自定义端口查看，例如我们要选择6007端口来查看，指令如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensorboard --logdir=logs --port=6007<br></code></pre></div></td></tr></table></figure>
<p><img src="/img/pytorch/2.png" srcset="/img/loading.gif" lazyload /></p>
<p>这时候我们就可以看到y=x的图像</p>
<p>如果是y=2x的图像则需要修改代码为：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># y = 2x图像</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&quot;y=2x&quot;</span>, <span class="hljs-number">2</span>*i, i)<br></code></pre></div></td></tr></table></figure>
<h3 id="输出图片">3.2输出图片</h3>
<p>我们也可以通过TensorBoard来显示我们的图片，通过TensorBoard的add_image()方法，参数分别为名称、图片（需要为tensor类型或者numpy类型）、global_step、类型（因为默认的类型为是（3，H，W）即通道(channel)为3，H为高度，W为宽度），代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># 创建一个实例,存储在logs文件夹下</span><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>image_path = <span class="hljs-string">&quot;dataset/train/ants/0013035.jpg&quot;</span><br>img_PIL = Image.<span class="hljs-built_in">open</span>(image_path)<br>img_array = np.array(img_PIL)<br><br>writer.add_image(<span class="hljs-string">&quot;test&quot;</span>, img_array, <span class="hljs-number">1</span>, dataformats=<span class="hljs-string">&#x27;HWC&#x27;</span>)<br><br>writer.close()<br></code></pre></div></td></tr></table></figure>
<h2 id="神经网络的基本骨架">4、神经网络的基本骨架</h2>
<p>基本骨架是通过对nn.Module的继承重写实现的，大体实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = <span class="hljs-built_in">input</span> + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> output<br><br>res = MyModel()<br>x = torch.tensor(<span class="hljs-number">1.0</span>)<br>output = res(x)		<span class="hljs-comment"># 有__call__方法来调用，所以可以直接将x输入，不用我们来调用forward()方法</span><br><span class="hljs-built_in">print</span>(output)<br><br></code></pre></div></td></tr></table></figure>
<p>最后我们可以看到控制台输出为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor(2.)<br></code></pre></div></td></tr></table></figure>
<h2 id="卷积层">5、卷积层</h2>
<p>首先对于卷积层的学习，要了解卷积层的使用，我们先使用torch.nn.functional中的方法（一般都是使用torch.nn，其对functional进行了封装，这里是为了了解如何卷积）进行学习，首先要对conv2d的stride参数进行了解，stride时计算卷积时移动的步长，我们可以使用代码来了解：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                      [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br><br>kernel = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>                       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                       [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>kernel = torch.reshape(kernel, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br><br>output = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output)<br><br>output2 = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(output2)<br></code></pre></div></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[10, 12, 12],<br>          [18, 16, 16],<br>          [13,  9,  3]]]])<br>tensor([[[[10, 12],<br>          [13,  3]]]])<br></code></pre></div></td></tr></table></figure>
<p>图示为：</p>
<p><img src="/img/pytorch/4.png" srcset="/img/loading.gif" lazyload /></p>
<p>随后我们要学习的是conv2d中的padding参数，它是对输入图像的周围进行填充，并设置填充值：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">output3 = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output3)<br></code></pre></div></td></tr></table></figure>
<p>输出结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[ 1,  3,  4, 10,  8],<br>          [ 5, 10, 12, 12,  6],<br>          [ 7, 18, 16, 16,  8],<br>          [11, 13,  9,  3,  4],<br>          [14, 13,  9,  7,  4]]]])<br></code></pre></div></td></tr></table></figure>
<p>图示为：</p>
<p><img src="/img/pytorch/5.png" srcset="/img/loading.gif" lazyload /></p>
<p>现在对卷积层的知识进行了学习后正式来时学习卷积层是如何搭建的，我们要使用的是torch.nn中的conv2d方法，其方法的参数解释在官方文档中如下：</p>
<ul>
<li><strong>in_channels</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)
– Number of channels in the input image</li>
<li><strong>out_channels</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)
– Number of channels produced by the convolution</li>
<li><strong>kernel_size</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>
<em>or</em> <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>)
– Size of the convolving kernel</li>
<li><strong>stride</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>
<em>or</em> <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em>
<em>optional</em>) – Stride of the convolution. Default: 1</li>
<li><strong>padding</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em>
<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>
<em>or</em> <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em>
<em>optional</em>) – Padding added to all four sides of the input.
Default: 0</li>
<li><strong>padding_mode</strong> (*string**,* <em>optional</em>) –
<code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code>
or <code>'circular'</code>. Default: <code>'zeros'</code></li>
<li><strong>dilation</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>
<em>or</em> <a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em>
<em>optional</em>) – Spacing between kernel elements. Default: 1</li>
<li><strong>groups</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em>
<em>optional</em>) – Number of blocked connections from input channels
to output channels. Default: 1</li>
<li><strong>bias</strong> (<a
target="_blank" rel="noopener" href="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em>
<em>optional</em>) – If <code>True</code>, adds a learnable bias to the
output. Default: <code>True</code></li>
</ul>
<p><img src="/img/pytorch/3.png" srcset="/img/loading.gif" lazyload /></p>
<p>我们简单的对图像进行卷积处理：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br>dataloader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.conv1 = Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>myModel = MyModel()<br><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs, targets = data<br>    output = myModel(imgs)<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(output.shape)</span><br><br>    <span class="hljs-comment"># torch.Size([64, 3, 32, 32])</span><br>    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, imgs, step)<br>    <span class="hljs-comment"># torch.Size([64, 6, 32, 32]) -&gt; [xxx, 3, 32, 32]，因为out_channels为6没办法add进去</span><br>    output = torch.reshape(output, (-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>))<br>    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, output, step)<br><br>    step = step + <span class="hljs-number">1</span><br></code></pre></div></td></tr></table></figure>
<p>运行结果在tensorboard上显示为：</p>
<p><img src="/img/pytorch/6.png" srcset="/img/loading.gif" lazyload /></p>
<p><img src="/img/pytorch/7.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="池化层">6、池化层</h2>
<p>我们要使用的是torch.nn中的MaxPool2d方法，其中参数解释为：</p>
<ul>
<li><strong>kernel_size</strong> – the size of the window to take a max
over</li>
<li><strong>stride</strong> – the stride of the window. Default value is
<code>kernel_size</code></li>
<li><strong>padding</strong> – implicit zero padding to be added on both
sides</li>
<li><strong>dilation</strong> – a parameter that controls the stride of
elements in the window</li>
<li><strong>return_indices</strong> – if <code>True</code>, will return
the max indices along with the outputs. Useful for <a
target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d"><code>torch.nn.MaxUnpool2d</code></a>
later</li>
<li><strong>ceil_mode</strong> – when True, will use ceil instead of
floor to compute the output shape</li>
</ul>
<p>对于ceil_mode的两种情况如下图示意：</p>
<p><img src="/img/pytorch/8.png" srcset="/img/loading.gif" lazyload /></p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MaxPool2d<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                      [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.float32)<br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.maxpool1 = MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.maxpool1(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br><br>myModel = MyModel()<br>output = myModel(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></div></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[2., 3.],<br>          [5., 1.]]]])<br></code></pre></div></td></tr></table></figure>
<p>当我们将ceil_mode修改为False后：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">self.maxpool1 = MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">False</span>)<br></code></pre></div></td></tr></table></figure>
<p>结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[2.]]]])<br></code></pre></div></td></tr></table></figure>
<p>我们可以直观的感受一下最大池化后的结果，我们可以将输入换成我们的数据集，在tensorboard上查看输入输出的差别：</p>
<p><img src="/img/pytorch/9.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="非线性激活">7、非线性激活</h2>
<p>首先我们要使用的是torch.nn中的ReLU方法</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> ReLU<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">0.5</span>],<br>                      [-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>]])<br><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.relu1 = ReLU()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.relu1(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br><br>myModel = MyModel()<br>output = myModel(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></div></td></tr></table></figure>
<p>结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[ 1.0000, -0.5000],<br>          [-1.0000,  3.0000]]]])<br>tensor([[[[1., 0.],<br>          [0., 3.]]]])<br></code></pre></div></td></tr></table></figure>
<p>然后我们使用torch.nn中的Sigmoid方法更直观的来查看结果：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Sigmoid<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br>dataloader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.sigmoid1 = Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.sigmoid1(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br>myModel = MyModel()<br>writer = SummaryWriter(<span class="hljs-string">&quot;logs_sigmoid&quot;</span>)<br>step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs, target = data<br>    output = myModel(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, imgs, step)<br>    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, output, step)<br>    step = step + <span class="hljs-number">1</span><br><br>writer.close()<br></code></pre></div></td></tr></table></figure>
<p>在tensorboard中查看运行结果：</p>
<p><img src="/img/pytorch/10.png" srcset="/img/loading.gif" lazyload /></p>
<h2 id="线性层-及其他层">8、线性层 及其他层</h2>
<p>主要还是查看官方文档。</p>
<h2
id="搭建小实例和sequential的使用">9、搭建小实例和Sequential的使用</h2>
<p>我们使用之前学习过的基础来搭建一个简单的神经网络，例如我们来搭建cifar10的模型，其如下图所示</p>
<p><img src="/img/pytorch/11.png" srcset="/img/loading.gif" lazyload /></p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.conv1 = Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.maxpool1 = MaxPool2d(<span class="hljs-number">2</span>)<br>        self.conv2 = Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.maxpool2 = MaxPool2d(<span class="hljs-number">2</span>)<br>        self.conv3 = Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.maxpool3 = MaxPool2d(<span class="hljs-number">2</span>)<br>        self.flatten = Flatten()<br>        self.linear1 = Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>)<br>        self.linear2 = Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br><br>        self.model1 = Sequential(<br>            Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        x = self.maxpool1(x)<br>        x = self.conv2(x)<br>        x = self.maxpool2(x)<br>        x = self.conv2(x)<br>        x = self.maxpool2(x)<br>        x = self.flatten(x)<br>        x = self.linear1(x)<br>        x = self.linear2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></div></td></tr></table></figure>
<p>引入了Sequential方法后可以将我们搭建的模型简写出来</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.model1 = Sequential(<br>            Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Flatten(),<br>            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),<br>            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model1(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></div></td></tr></table></figure>
<h2 id="损失函数与反向传播">10、损失函数与反向传播</h2>
<p>我们可以使用torch.nn中的L1Loss以及MSELoss方法来构造损失函数：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.float32)<br>target = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=torch.float32)<br><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))<br>target = torch.reshape(target, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))<br><br>loss = L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>result = loss(<span class="hljs-built_in">input</span>, target)<br><br>loss_mse = nn.MSELoss()<br>result_mse = loss_mse(<span class="hljs-built_in">input</span>, target)<br><br><span class="hljs-built_in">print</span>(result)<br><span class="hljs-built_in">print</span>(result_mse)<br></code></pre></div></td></tr></table></figure>
<p>运行结果为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor(2.)<br>tensor(1.3333)<br></code></pre></div></td></tr></table></figure>
<p>我们可以利用交叉熵作为损失函数来进行反向传播，利用backward方法：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">myModel = MyModel()<br>loss = nn.CrossEntropyLoss()<br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs, targets = data<br>    output = myModel(imgs)<br>    result_loss = loss(output, targets)<br>    result_loss.backward()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ok&quot;</span>)<br></code></pre></div></td></tr></table></figure>
<h2 id="优化器">11、优化器</h2>
<p>使用torch.optim中的优化器:</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">myModel = MyModel()<br>loss = nn.CrossEntropyLoss()<br>optim = torch.optim.SGD(myModel.parameters(), lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    runing_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>        imgs, targets = data<br>        output = myModel(imgs)<br>        result_loss = loss(output, targets)<br>        optim.zero_grad()	<span class="hljs-comment"># 将梯度初始化为0</span><br>        result_loss.backward()<br>        optim.step()	<span class="hljs-comment"># 进行优化</span><br>        runing_loss = runing_loss + result_loss<br>    <span class="hljs-built_in">print</span>(runing_loss)<br></code></pre></div></td></tr></table></figure>
<p>运行结果：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">tensor(<span class="hljs-number">18666.8984</span>, grad_fn=&lt;AddBackward0&gt;)<br>tensor(<span class="hljs-number">16161.6846</span>, grad_fn=&lt;AddBackward0&gt;)<br>tensor(<span class="hljs-number">15338.8057</span>, grad_fn=&lt;AddBackward0&gt;)<br>...<br></code></pre></div></td></tr></table></figure>
<p><code>torch.optim.lr_scheduler</code>模块提供了一些根据epoch训练次数来调整学习率（learning
rate）的方法。一般情况下我们会设置随着epoch的增大而逐渐减小学习率从而达到更好的训练效果。</p>
<p><code>torch.optim.lr_scheduler.LambdaLR</code>中大部分调整学习率的方法都是根据epoch训练次数</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">torch</span>.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-<span class="hljs-number">1</span>)<br></code></pre></div></td></tr></table></figure>
<p>更新策略： <span class="math display">\[
new\_lr=\lambda \times initial\_lr
\]</span> 其中new_lr是得到的新的学习率，initial_lr是初始的学习率，<span
class="math inline">\(\lambda\)</span>是通过参数lr_lambda和epoch得到的。</p>
<p>参数：</p>
<p>optimizer （Optimizer）：要更改学习率的优化器； lr_lambda（function
or list）：根据epoch计算<span
class="math inline">\(\lambda\)</span>的函数；或者是一个list的这样的function，分别计算各个parameter
groups的学习率更新用到的<span class="math inline">\(\lambda\)</span>；
last_epoch
（int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">net_1 = model()<br><br>optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)<br>scheduler_1 = LambdaLR(optimizer_1, lr_lambda=<span class="hljs-keyword">lambda</span> epoch: <span class="hljs-number">1</span>/(epoch+<span class="hljs-number">1</span>))<br></code></pre></div></td></tr></table></figure>
<h2 id="现有网络模型的使用与修改">12、现有网络模型的使用与修改</h2>
<p>我们使用torchvision中现有的vgg16模型进行使用：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">vgg16_true = torchvison.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 添加层</span><br>vgg16_true.classifier.add_module(<span class="hljs-string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>))<br><span class="hljs-comment"># 修改层</span><br>vgg16_true.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br></code></pre></div></td></tr></table></figure>
<h2 id="网络模型的保存与读取">13、网络模型的保存与读取</h2>
<p>保存方法：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 保存方法1</span><br>torch.save(vgg16, <span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)<br><br><span class="hljs-comment"># 保存方法二，将模型参数保存为字典（官方推荐）</span><br>torch.save(vgg16.state_dict(), <span class="hljs-string">&quot;vgg16_method2.pth&quot;</span>)<br></code></pre></div></td></tr></table></figure>
<p>读取方法：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 方法1保存的模型进行读取</span><br>model = torch.load(<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)<br><br><span class="hljs-comment"># 方法2保存的模型进行读取</span><br>vgg16 = vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)<br>vgg16.load_state_dict(torch.load(<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>))<br></code></pre></div></td></tr></table></figure>
<h2 id="完整的模型训练">14、完整的模型训练</h2>
<p>model.py：用于保存网络模型</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-comment"># 验证</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    myModel = MyModel()<br>    <span class="hljs-built_in">input</span> = torch.ones((<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>    output = myModel(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-built_in">print</span>(output.shape)<br></code></pre></div></td></tr></table></figure>
<p>train.py：进行网络训练</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 准备数据集</span><br>train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 获取数据集长度</span><br>train_data_length = <span class="hljs-built_in">len</span>(train_data)<br>test_data_length = <span class="hljs-built_in">len</span>(test_data)<br><br><span class="hljs-comment"># 利用DataLoader加载数据</span><br>train_dataloader = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-comment"># 创建网络模型</span><br>myModel = MyModel()<br><br><span class="hljs-comment"># 损失函数</span><br>loss_fn = nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># 优化器</span><br>learning_rate = <span class="hljs-number">0.01</span><br>optimizer = torch.optim.SGD(myModel.parameters(), lr=learning_rate)<br><br><span class="hljs-comment"># 设置训练网络的一些参数</span><br><span class="hljs-comment"># 记录训练的次数</span><br>total_train_step = <span class="hljs-number">0</span><br><span class="hljs-comment"># 记录测试的次数</span><br>total_test_step = <span class="hljs-number">0</span><br><span class="hljs-comment"># 训练论数</span><br>epoch = <span class="hljs-number">10</span><br><br><span class="hljs-comment"># 添加tensorboard</span><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs_train&quot;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;------第%d轮训练开始------&quot;</span> % (i+<span class="hljs-number">1</span>))<br><br>    <span class="hljs-comment"># 训练步骤开始</span><br>    myModel.train()<br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_dataloader:<br>        imgs, targets = data<br>        outputs = myModel(imgs)<br>        loss = loss_fn(outputs, targets)<br><br>        <span class="hljs-comment"># 优化器优化模型</span><br>        <span class="hljs-comment"># 利用优化器进行梯度清零</span><br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># 反向传播</span><br>        loss.backward()<br>        optimizer.step()<br><br>        total_train_step = total_train_step + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step, loss.item()))<br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)<br><br>    <span class="hljs-comment"># 测试步骤开始</span><br>    myModel.<span class="hljs-built_in">eval</span>()<br>    total_test_loss = <span class="hljs-number">0</span><br>    total_accuracy = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader:<br>            imgs, targets = data<br>            outputs = myModel(imgs)<br>            loss = loss_fn(outputs, targets)<br>            total_test_loss = total_test_loss + loss.item()<br>            accuracy = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()<br>            total_accuracy = total_accuracy + accuracy<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuracy/test_data_length))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_length, total_test_step)<br>    total_test_step = total_test_step + <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 保存每轮的数据</span><br>    torch.save(myModel, <span class="hljs-string">&quot;myModel_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br></code></pre></div></td></tr></table></figure>
<h2 id="使用gpu训练">15、使用GPU训练</h2>
<h3 id="方法一">方法一</h3>
<p>我们需要对网络模型、数据、损失函数进行修改</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 构建模型</span><br>myModel = MyModel()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    myModel = myModel.cuda()<br>    <br><span class="hljs-comment"># 损失函数</span><br>loss_fn = nn.CrossEntropyLoss()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    loss_fn = loss_fn.cuda()<br>    <br>imgs, targets = data<br>	<span class="hljs-keyword">if</span> torch.cuda.is_available():<br>		imgs = imgs.cuda()<br>		targets = targets.cuda()<br></code></pre></div></td></tr></table></figure>
<h3 id="方法二">方法二</h3>
<p>需要在文件最开始定义训练的设备</p>
<p>此时为将设备设置为cpu</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 定义训练的设备</span><br>device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># 构建模型</span><br>myModel = MyModel()<br>myModel.to(device)	<span class="hljs-comment"># 模型和损失函数不用重新赋值</span><br><br><span class="hljs-comment"># 损失函数</span><br>loss_fn = nn.CrossEntropyLoss()<br>loss_fn.to(device)<br><br>imgs, targets = data<br>imgs = imgs.to(device)	<span class="hljs-comment"># 数据需要重新赋值</span><br>targets = targets.to(device)<br></code></pre></div></td></tr></table></figure>
<p>如果要使用gpu需要如下设置：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 定义训练的设备</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></div></td></tr></table></figure>
<p>如果有多个gpu，可以按照如下选择设置：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 定义训练的设备</span><br>device = torch.device(<span class="hljs-string">&quot;cuda：0&quot;</span>)<br></code></pre></div></td></tr></table></figure>
<h2 id="模型验证套路">16、模型验证套路</h2>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">image_path = <span class="hljs-string">&quot;./img/dog.png&quot;</span><br><br>image = Image.<span class="hljs-built_in">open</span>(image_path)<br>image = image.convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br><br>transform = torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),<br>                                            torchvision.transforms.ToTensor()])<br><br>image = transform(image)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>myModel = torch.load(<span class="hljs-string">&quot;myModel_0.pth&quot;</span>)<br><span class="hljs-built_in">print</span>(myModel)<br>image = torch.reshape(image, (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>myModel.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output = myModel(image)<br><span class="hljs-built_in">print</span>(output)<br><span class="hljs-built_in">print</span>(output.argmax(<span class="hljs-number">1</span>))<br></code></pre></div></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[ 0.3308,  0.0307,  0.9064,  0.9474,  0.2715,  0.8123, -0.4077,  0.2044,<br>         -0.4149, -1.0106]])<br>tensor([3])<br></code></pre></div></td></tr></table></figure>
<p>可以看到最后预测我们的图片属于第三类</p>
<h2 id="lstm">17、LSTM</h2>
<p><strong>输入的参数列表包括:</strong></p>
<ul>
<li>input_size
输入数据的特征维数，通常就是embedding_dim(词向量的维度)</li>
<li>hidden_size　LSTM中隐层的维度</li>
<li>num_layers　循环神经网络的层数</li>
<li>bias　用不用偏置，default=True</li>
<li>batch_first
这个要注意，通常我们输入的数据shape=(batch_size,seq_length,embedding_dim),而batch_first默认是False,所以我们的输入数据最好送进LSTM之前将batch_size与seq_length这两个维度调换</li>
<li>dropout　默认是0，代表不用dropout</li>
<li>bidirectional默认是false，代表不用双向LSTM</li>
</ul>
<p><strong>输入数据包括input,(h_0,c_0):</strong></p>
<ul>
<li>input就是shape=(seq_length,batch_size,input_size)的张量</li>
<li>h_0是shape=(num_layers*num_directions,batch_size,hidden_size)的张量，它包含了在当前这个batch_size中每个句子的初始隐藏状态。其中num_layers就是LSTM的层数。如果bidirectional=True,num_directions=2,否则就是１，表示只有一个方向。</li>
<li>c_0和h_0的形状相同，它包含的是在当前这个batch_size中的每个句子的初始细胞状态。h_0,c_0如果不提供，那么默认是０。</li>
</ul>
<p><strong>输出数据包括output,(h_n,c_n):</strong></p>
<ul>
<li>output的shape=(seq_length,batch_size,num_directions*hidden_size),它包含的是LSTM的最后一时间步的输出特征(h_t),ｔ是batch_size中每个句子的长度。</li>
<li>h_n.shape==(num_directions * num_layers,batch,hidden_size)</li>
<li>h_n包含的是句子的最后一个单词（也就是最后一个时间步）的隐藏状态，c_n包含的是句子的最后一个单词的细胞状态，所以它们都与句子的长度seq_length无关。</li>
<li>output[-1]与h_n是相等的，因为output[-1]包含的正是batch_size个句子中每一个句子的最后一个单词的隐藏状态，注意LSTM中的隐藏状态其实就是输出，cell
state细胞状态才是LSTM中一直隐藏的，记录着信息。</li>
</ul>
<h2 id="lstmcell">18、LSTMcell</h2>
<p>上述的nn.LSTM模块一次构造完若干层的LSTM,但是为了对模型有更加灵活的处nn中还有一个LSTMCell模块，是组成LSTM整个序列计算过程的基本组成单元，也就是进行sequence中一个word的计算。</p>
<p>同时它的参数也就只有3个：输入维度，隐节点数据维度，是否带有偏置。仅提供最基本一个LSTM单元结构，想要完整的进行一个序列的训练的要还要自己编写传播函数把cell间的输入输出连接起来。但是想要自定义不同cell隐节点维度的多层LSTM的话，这个模块还是挺有用的。</p>
<h2 id="pad_sequence">19、pad_sequence</h2>
<p><strong>sequences</strong>：表示输入样本序列，为 list 类型，list
中的元素为 tensor 类型。 tensor 的 size 为 L * F 。其中，L
为单个序列的长度，F 为序列中每个时间步（time
step）特征的个数，根据任务的不同 F 的维度会有所不同。</p>
<p><strong>batch_first</strong>：为 True 对应 [batch_size, seq_len,
feature]；False 对应[seq_len, batch_size,
feature]，从习惯上来讲一般设置为 True 比较符合我们的认知。</p>
<p><strong>padding_value</strong>：填充值，默认值为 0 。</p>
<p><strong>说明</strong></p>
<p>主要用来对样本进行填充，填充值一般为 0
。我们在训练网络时，一般会采用一个一个 mini-batch
的方式，将训练样本数据喂给网络。在 PyTorch 里面数据都是以 tensor
的形式存在，一个 mini-batch 实际上就是一个高维的 tensor
，每个序列数据的长度必须相同才能组成一个 tensor 。为了使网络可以处理
mini-batch 形式的数据，就必须对序列样本进行填充，保证一个 mini-batch
里面的数据长度是相同的。</p>
<p>在 PyTorch 里面一般是使用 DataLoader 进行数据加载，返回 mini-batch
形式的数据，再将此数据喂给网络进行训练。我们一般会自定义一个 collate_fn
函数，完成对数据的填充。</p>
<p><strong>示例</strong></p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">from</span> torch.nn.utils.rnn <span class="hljs-keyword">import</span> pad_sequence,pack_padded_sequence,pack_sequence,pad_packed_sequence<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>        self.data = data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.data[idx]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>    data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>, padding_value=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> data<br><br>a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br>b = torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>])<br>c = torch.tensor([<span class="hljs-number">7</span>,<span class="hljs-number">8</span>])<br>d = torch.tensor([<span class="hljs-number">9</span>])<br>train_x = [a, b, c, d]<br><br>data = MyData(train_x)<br>data_loader = DataLoader(data, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=collate_fn)<br><span class="hljs-comment"># 采用默认的 collate_fn 会报错</span><br><span class="hljs-comment">#data_loader = DataLoader(data, batch_size=2, shuffle=True) </span><br>batch_x = <span class="hljs-built_in">iter</span>(data_loader).<span class="hljs-built_in">next</span>()<br></code></pre></div></td></tr></table></figure>
<p>运行程序，得到 batch_x 的值：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>        [<span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br></code></pre></div></td></tr></table></figure>
<p>从 batch_x 的值可以看出，第二行填充了三个 0
，使其长度和第一行保持一致。</p>
<p>需要说明的是，对于长度不同的序列，使用默认的 collate_fn
函数，不自定义 collate_fn 函数完成对序列的填充，上面的程序就会报错。</p>
<h2 id="pack_padded_sequence">20、pack_padded_sequence</h2>
<p><strong>参数</strong></p>
<p><strong>input</strong>：经过 pad_sequence 处理之后的数据。</p>
<p><strong>lengths</strong>：mini-batch中各个序列的实际长度。</p>
<p><strong>batch_first</strong>：True 对应 [batch_size, seq_len,
feature] ；</p>
<p>False 对应 [seq_len, batch_size, feature] 。</p>
<p><strong>enforce_sorted</strong>：如果是 True
，则输入应该是按长度降序排序的序列。如果是 False
，会在函数内部进行排序。默认值为 True 。</p>
<p><strong>说明</strong></p>
<p>这个 pack 的意思可以理解为压紧或压缩
，因为数据在经过填充之后，会有很多冗余的
padding_value，所以需要压缩一下。</p>
<p>为什么要使用这个函数呢？</p>
<p>RNN 读取数据的方式：网络每次吃进去一组同样时间步 （time step）
的数据，也就是 mini-batch 的所有样本中下标相同的数据，然后获得一个
mini-batch 的输出；再移到下一个时间步 （time step），再读入 mini-batch
中所有该时间步的数据，再输出；直到处理完所有的时间步数据。</p>
<p>第一个时间步：</p>
<p><img src="/img/pytorch/12.png" srcset="/img/loading.gif" lazyload /></p>
<p>第二个时间步：</p>
<p><img src="/img/pytorch/13.png" srcset="/img/loading.gif" lazyload /></p>
<p>mini-batch 中的 0 只是用来做数据对齐的 padding_value ，如果进行
forward 计算时，把 padding_value
也考虑进去，可能会导致RNN通过了非常多无用的
padding_value，这样不仅浪费计算资源，最后得到的值可能还会存在误差。对于上面的序列
2 的数据，通过 RNN 网络：</p>
<p><img src="/img/pytorch/14.png" srcset="/img/loading.gif" lazyload /></p>
<p>实际上从第 2 个时间步开始一直到最后的计算都是多余的，输入都是无效的
padding_value 而已。</p>
<p>从上面的分析可以看出，为了使 RNN 可以高效的读取数据进行训练，就需要在
pad 之后再使用 pack_padded_sequence 对数据进行处理。</p>
<p>需要注意的是，默认条件下，我们必须把输入数据按照序列长度从大到小排列后才能送入
pack_padded_sequence ，否则会报错。</p>
<p><strong>示例</strong></p>
<p>只需要将上面的例子中的 collate_fn
函数稍作修改即可，其余部分保持不变。</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>    seq_len = [s.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data] <span class="hljs-comment"># 获取数据真实的长度</span><br>    data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>)    <br>    data = pack_padded_sequence(data, seq_len, batch_first=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> data<br></code></pre></div></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>PackedSequence(data=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), <br>               batch_sizes=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>               sorted_indices=<span class="hljs-literal">None</span>, unsorted_indices=<span class="hljs-literal">None</span>)<br></code></pre></div></td></tr></table></figure>
<p>可以看出，输出返回一个 PackedSequence 对象，它主要包含两部分：data 和
batch_sizes 。</p>
<p><img src="/img/pytorch/15.png" srcset="/img/loading.gif" lazyload /></p>
<p>填充值 0 就被跳过了。batch_size
中的值，实际上就是告诉网络每个时间步需要吃进去多少数据。</p>
<p>如果仔细看，其实输出的 PackedSequence 对象还包含两个部分
sorted_indices 和unsorted_indices 。前面说到 pack_padded_sequence
还有一个参数 <strong>enforce_sorted</strong> ，如果是 True
，则输入应该是按长度降序排序的序列。如果是 False
，会在函数内部进行排序。默认值为 True 。也就是说在输入
pack_padded_sequence 前，我们也可以不对数据进行排序。</p>
<p>现在我们将 enforce_sorted 设置为 False
，且输入数据不预先进行排序。</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">data = [torch.tensor([<span class="hljs-number">9</span>]), <br>        torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]),<br>        torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])]<br><br>seq_len = [s.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data]<br>data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>)    <br>data = pack_padded_sequence(data, seq_len, batch_first=<span class="hljs-literal">True</span>, enforce_sorted=<span class="hljs-literal">False</span>)<br></code></pre></div></td></tr></table></figure>
<p>输出：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">PackedSequence(data=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), <br>               batch_sizes=tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>               sorted_indices=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>]), <br>               unsorted_indices=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]))<br></code></pre></div></td></tr></table></figure>
<p>sorted_indices = tensor([1, 2, 0]，表示排序之后的结果与原始 data 中的
tensor 的下标对应关系。1 表示原始 data 中 第 1
行最长，排序之后排在最前面，其次是第 2 行、第 0 行。</p>
<p>假设排序之后的结果为：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">sort_data = [torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]),<br>             torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])<br>             torch.tensor([<span class="hljs-number">9</span>]), <br>             ]<br></code></pre></div></td></tr></table></figure>
<p>unsorted_indices = tensor([2, 0, 1]，表示未排序前结果。2 表示
sort_data 的第 2 行对应 data 中第 0 行；0 表示 sort_data 的第 0 行对应
data 中的第 1 行；1 表示 sort_data 的第 1 行对应 data 中的第 2 行。</p>
<h2 id="pack_sequence">21、pack_sequence</h2>
<p><strong>参数</strong></p>
<p><strong>sequences</strong>：输入样本序列，为 list 类型，list
中的元素为 tensor ；tensor 的 size 为 L * F，其中，L 为单个序列的长度，F
为序列中每个时间步（time step）特征的个数，根据任务的不同 F
的维度会有所不同。</p>
<p><strong>enforce_sorted</strong>：如果是 True
，则输入应该是按长度降序排序的序列。如果是 False
，会在函数内部进行排序。默认值为 True 。</p>
<p><strong>说明</strong></p>
<p>我们看看 PyTorch 中的源码：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pack_sequence</span>(<span class="hljs-params">sequences, enforce_sorted=<span class="hljs-literal">True</span></span>): <br>	lengths = torch.as_tensor([v.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sequences]) <br>	<span class="hljs-keyword">return</span> pack_padded_sequence(pad_sequence(sequences),lengths,enforce_sorted=enforce_sorted)<br></code></pre></div></td></tr></table></figure>
<p>可以看出 pack_sequence 实际上就是对 pad_sequence 和
pack_padded_sequence
操作的一个封装。通过一个函数完成了两步才能完成的工作。</p>
<p><strong>示例</strong></p>
<p>前面的 collate_fn 函数可以进一步修改为：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>   <br>    data = pack_sequence(data)<br>    <span class="hljs-comment">#seq_len = [s.size(0) for s in data]</span><br>    <span class="hljs-comment">#data = pad_sequence(data, batch_first=True)    </span><br>    <span class="hljs-comment">#data = pack_padded_sequence(data, seq_len, batch_first=True)</span><br>    <span class="hljs-keyword">return</span> data<br></code></pre></div></td></tr></table></figure>
<p>输出结果与前面相同：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>PackedSequence(data=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), <br>               batch_sizes=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>               sorted_indices=<span class="hljs-literal">None</span>, unsorted_indices=<span class="hljs-literal">None</span>)<br></code></pre></div></td></tr></table></figure>
<h2 id="pad_packed_sequence">22、pad_packed_sequence</h2>
<p><strong>参数</strong></p>
<p><strong>sequences</strong>：PackedSequence 对象，将要被填充的 batch
；</p>
<p><strong>batch_first</strong>：一般设置为 True，返回的数据格式为
[batch_size, seq_len, feature] ；</p>
<p><strong>padding_value</strong>：填充值；</p>
<p><strong>total_length</strong>：如果不是<code>None</code>，输出将被填充到长度：<code>total_length</code>。</p>
<p><strong>说明</strong></p>
<p>如果在喂给网络数据的时候，用了 pack_sequence 进行打包，pytorch 的 RNN
也会把输出 out 打包成一个 PackedSequence 对象。</p>
<p>这个函数实际上是 pack_padded_sequence
函数的逆向操作。就是把压紧的序列再填充回来。</p>
<p>为啥要填充回来呢？我的理解是，在 collate_fn 函数里面通常也会调用
pad_sequence 对 label 进行填充，RNN 的输出结果为了和 label
对齐，需要将压紧的序列再填充回来，方便后续的计算。</p>
<p><strong>示例</strong></p>
<p>需要说明的是，下面的程序中，为了产生符合 LSTM 输入格式 [batch_size,
seq_len, feature] 的数据，使用了函数 unsqueeze 进行升维处理。其中，
batch_size 是<strong>样本数</strong>，seq_len
是<strong>序列长度</strong>，feature 是<strong>特征数</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>        self.data = data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.data[idx]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>    seq_len = [s.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data]<br>    data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()    <br>    data = data.unsqueeze(-<span class="hljs-number">1</span>)<br>    data = pack_padded_sequence(data, seq_len, batch_first=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> data<br><br>a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br>b = torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>])<br>c = torch.tensor([<span class="hljs-number">7</span>,<span class="hljs-number">8</span>])<br>d = torch.tensor([<span class="hljs-number">9</span>])<br>train_x = [a, b, c, d]<br><br>data = MyData(train_x)<br>data_loader = DataLoader(data, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=collate_fn)<br>batch_x = <span class="hljs-built_in">iter</span>(data_loader).<span class="hljs-built_in">next</span>()<br><br>rnn = nn.LSTM(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, batch_first=<span class="hljs-literal">True</span>)<br>h0 = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>()<br>c0 = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>()<br>out, (h1, c1) = rnn(batch_x, (h0, c0))<br></code></pre></div></td></tr></table></figure>
<p>得到 out 的结果如下，是一个 PackedSequence 类型的对象，与前面调用
pack_padded_sequence 得到的结果类型相同。</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># out</span><br>PackedSequence(data=tensor([[-<span class="hljs-number">1.3302e-04</span>,  <span class="hljs-number">5.7754e-02</span>,  <span class="hljs-number">4.3181e-02</span>,  <span class="hljs-number">6.4226e-02</span>],<br>        [-<span class="hljs-number">2.8673e-02</span>,  <span class="hljs-number">3.9089e-02</span>, -<span class="hljs-number">2.6875e-03</span>,  <span class="hljs-number">4.2686e-03</span>],<br>        [-<span class="hljs-number">1.0216e-01</span>,  <span class="hljs-number">2.5236e-02</span>, -<span class="hljs-number">1.2230e-01</span>,  <span class="hljs-number">5.1524e-02</span>],<br>        [-<span class="hljs-number">1.6211e-01</span>,  <span class="hljs-number">2.1079e-02</span>, -<span class="hljs-number">1.5849e-01</span>,  <span class="hljs-number">5.2800e-02</span>],<br>        [-<span class="hljs-number">1.5774e-01</span>,  <span class="hljs-number">2.6749e-02</span>, -<span class="hljs-number">1.3333e-01</span>,  <span class="hljs-number">4.7894e-02</span>]],<br>       grad_fn=&lt;CatBackward&gt;), <br>       batch_sizes=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>       sorted_indices=<span class="hljs-literal">None</span>, unsorted_indices=<span class="hljs-literal">None</span>)<br></code></pre></div></td></tr></table></figure>
<p>对 out 调用 pad_packed_sequence 进行填充：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">out_pad, out_len = pad_packed_sequence(out, batch_first=<span class="hljs-literal">True</span>)<br></code></pre></div></td></tr></table></figure>
<p>out_pad 和 out_len 的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># out_pad </span><br>tensor([[[-<span class="hljs-number">1.3302e-04</span>,  <span class="hljs-number">5.7754e-02</span>,  <span class="hljs-number">4.3181e-02</span>,  <span class="hljs-number">6.4226e-02</span>],<br>         [-<span class="hljs-number">1.0216e-01</span>,  <span class="hljs-number">2.5236e-02</span>, -<span class="hljs-number">1.2230e-01</span>,  <span class="hljs-number">5.1524e-02</span>],<br>         [-<span class="hljs-number">1.6211e-01</span>,  <span class="hljs-number">2.1079e-02</span>, -<span class="hljs-number">1.5849e-01</span>,  <span class="hljs-number">5.2800e-02</span>],<br>         [-<span class="hljs-number">1.5774e-01</span>,  <span class="hljs-number">2.6749e-02</span>, -<span class="hljs-number">1.3333e-01</span>,  <span class="hljs-number">4.7894e-02</span>]],<br><br>        [[-<span class="hljs-number">2.8673e-02</span>,  <span class="hljs-number">3.9089e-02</span>, -<span class="hljs-number">2.6875e-03</span>,  <span class="hljs-number">4.2686e-03</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>]]],<br>       grad_fn=&lt;TransposeBackward0&gt;)<br><br><span class="hljs-comment"># out_len</span><br>tensor([<span class="hljs-number">4</span>, <span class="hljs-number">1</span>])<br></code></pre></div></td></tr></table></figure>
<p>再回想下我们调用 pad_sequence 填充之后的输入：</p>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>        [<span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br></code></pre></div></td></tr></table></figure>
<p>这个 out_pad 结果其实就和我们填充之后的输入对应起来了。</p>
<h2 id="cat">22、cat</h2>
<p>出了两个张量A和B，分别是2行3列，4行3列。即他们都是2维张量。因为只有两维，这样在用torch.cat拼接的时候就有两种拼接方式：按行拼接和按列拼接。即所谓的维数0和维数1.</p>
<p>C=torch.cat((A,B),0)就表示按维数0（行）拼接A和B，也就是竖着拼接，A上B下。此时需要注意：列数必须一致，即维数1数值要相同，这里都是3列，方能列对齐。拼接后的C的第0维是两个维数0数值和，即2+4=6</p>
<p>C=torch.cat((A,B),1)就表示按维数1（列）拼接A和B，也就是横着拼接，A左B右。此时需要注意：行数必须一致，即维数0数值要相同，这里都是2行，方能行对齐。拼接后的C的第1维是两个维数1数值和，即3+4=7</p>
<h2 id="torch.split">23、torch.split</h2>
<p><code>torch.split(tensor, split_size_or_sections, dim=0)</code></p>
<p>torch.split()作用将tensor分成块结构。</p>
<p>参数：</p>
<ul>
<li>tesnor：input，待分输入</li>
<li>split_size_or_sections：需要切分的大小(int or list )</li>
<li>dim：切分维度</li>
<li>output：切分后块结构 &lt;class 'tuple'&gt;</li>
<li>当split_size_or_sections为<strong>int</strong>时，tenor结构和split_size_or_sections，正好匹配，那么ouput就是大小相同的块结构。如果按照split_size_or_sections结构，tensor不够了，那么就把剩下的那部分做一个块处理。</li>
<li>当split_size_or_sections
为<strong>list</strong>时，那么tensor结构会一共切分成len(list)这么多的小块，每个小块中的大小按照list中的大小决定，其中list中的数字总和应等于该维度的大小，否则会报错（注意这里与split_size_or_sections为int时的情况不同）。</li>
</ul>
<h2
id="torch.squeeze和torch.unsqueeze">24、torch.squeeze和torch.unsqueeze</h2>
<p>⼀、先看torch.squeeze()
这个函数主要对数据的维度进⾏压缩，去掉维数为1的的维度，⽐如是⼀⾏或者⼀列这种，⼀个⼀⾏三列（1,3）的数去掉第⼀个维数为⼀的维度之后就变成（3）⾏。</p>
<ul>
<li>1.squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。</li>
<li>2.a.squeeze(N) 就是去掉a中指定的维数为⼀的维度。</li>
<li>3.还有⼀种形式就是b=torch.squeeze(a，N)
a中去掉指定的维数N为⼀的维度。</li>
</ul>
<p>⼆、再看torch.unsqueeze()这个函数主要是对数据维度进⾏扩充。</p>
<ul>
<li>1.给指定位置加上维数为⼀的维度，⽐如原本有个三⾏的数据（3），在0的位置加了⼀维就变成⼀⾏三列（1,3）。a.unsqueeze(N)
就是在a中指定位置N加上⼀个维数为1的维度。</li>
<li>2.还有⼀种形式就是b=torch.unsqueeze(a，N)
a就是在a中指定位置N加上⼀个维数为1的维度</li>
</ul>
<h2 id="nn.embedding">25、nn.Embedding</h2>
<p><code>torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None,max_norm=None,  norm_type=2.0,   scale_grad_by_freq=False, sparse=False,  _weight=None)</code></p>
<p>其为一个简单的存储固定大小的词典的嵌入向量的查找表，意思就是说，给一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系。</p>
<p>输入为一个编号列表，输出为对应的符号嵌入向量列表。</p>
<p><strong>参数解释</strong></p>
<ul>
<li>num_embeddings (python:int) –
词典的大小尺寸，比如总共出现5000个词，那就输入5000。此时index为（0-4999）</li>
<li>embedding_dim (python:int) –
嵌入向量的维度，即用多少维来表示一个符号。</li>
<li>padding_idx (python:int, optional) –
填充id，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。<strong>（初始化为0）</strong></li>
<li>max_norm (python:float, optional) –
最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。</li>
<li>norm_type (python:float, optional) –
指定利用什么范数计算，并用于对比max_norm，默认为2范数。</li>
<li>scale_grad_by_freq (boolean, optional) –
根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.</li>
<li>sparse (bool, optional) –
若为True,则与权重矩阵相关的梯度转变为稀疏张量。</li>
</ul>
<h2 id="torch.bmm">26、torch.bmm</h2>
<p>计算两个torch的矩阵乘法</p>
<p>函数定义：</p>
<p><code>def bmm(self: Tensor,mat2: Tensor,*,out: Optional[Tensor] = None) -&gt; Tensor</code></p>
<p>函数的传入参数很简单，两个三维<a
target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=矩阵&amp;spm=1001.2101.3001.7020">矩阵</a>而已，只是要注意这两个矩阵的shape有一些要求：</p>
<p><code>res = torch.bmm(ma, mb)</code></p>
<p><code>ma: [a, b, c]</code></p>
<p><code>mb: [a, c, d]</code></p>
<h2 id="dp训练">27、DP训练</h2>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.nn.parallel <span class="hljs-keyword">import</span> DataParallel<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets, transforms<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br>    <br>device_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>] <span class="hljs-comment"># 可用GPU</span><br>BATCH_SIZE = <span class="hljs-number">64</span><br><br>transform = transforms.Compose([transforms.ToTensor()])<br>data_train = datasets.MNIST(root = <span class="hljs-string">&quot;./data/&quot;</span>,<br>                            transform=transform,<br>                            train=<span class="hljs-literal">True</span>,<br>                            download=<span class="hljs-literal">True</span>)<br>data_test = datasets.MNIST(root=<span class="hljs-string">&quot;./data/&quot;</span>,<br>                           transform=transform,<br>                           train=<span class="hljs-literal">False</span>)<br> <br>data_loader_train = torch.utils.data.DataLoader(dataset=data_train,<br>                                                <span class="hljs-comment"># 单卡batch size * 卡数</span><br>                                                batch_size=BATCH_SIZE * <span class="hljs-built_in">len</span>(device_ids),<br>                                                shuffle=<span class="hljs-literal">True</span>,<br>                                                num_workers=<span class="hljs-number">2</span>)<br> <br>data_loader_test = torch.utils.data.DataLoader(dataset=data_test,<br>                                               batch_size=BATCH_SIZE * <span class="hljs-built_in">len</span>(device_ids),<br>                                               shuffle=<span class="hljs-literal">True</span>,<br>                                               num_workers=<span class="hljs-number">2</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.conv1 = torch.nn.Sequential(<br>        torch.nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>        torch.nn.ReLU(),<br>        torch.nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>        torch.nn.ReLU(),<br>        torch.nn.MaxPool2d(stride=<span class="hljs-number">2</span>, kernel_size=<span class="hljs-number">2</span>),<br>    )<br>        self.dense = torch.nn.Sequential(<br>            torch.nn.Linear(<span class="hljs-number">14</span> * <span class="hljs-number">14</span> * <span class="hljs-number">128</span>, <span class="hljs-number">1024</span>),<br>            torch.nn.ReLU(),<br>            torch.nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            torch.nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>)<br>    )<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        x = x.view(-<span class="hljs-number">1</span>, <span class="hljs-number">14</span> * <span class="hljs-number">14</span> * <span class="hljs-number">128</span>)<br>        x = self.dense(x)<br>        <span class="hljs-keyword">return</span> x<br> <br> <br>model = Model()<br><span class="hljs-comment"># 指定要用到的设备，会自动将模型复制到各个GPU上</span><br>model = torch.nn.DataParallel(model.cuda(), device_ids=device_ids)<br><br>cost = torch.nn.CrossEntropyLoss()<br>optimizer = torch.optim.Adam(model.parameters())<br><br>n_epochs = <span class="hljs-number">50</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_epochs):<br>    running_loss = <span class="hljs-number">0.0</span><br>    running_correct = <span class="hljs-number">0</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Epoch &#123;&#125;/&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch, n_epochs))<br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> tqdm(data_loader_train):<br>        X_train, y_train = data<br>        <span class="hljs-comment"># 数据会自动分配到各个GPU中</span><br>        X_train, y_train = X_train.cuda(), y_train.cuda()<br>        outputs = model(X_train)<br><br>        optimizer.zero_grad()<br>        loss = cost(outputs, y_train)<br>        loss.backward()<br>        optimizer.step()<br>        <br>    testing_correct = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> data_loader_test:<br>        X_test, y_test = data<br>        <span class="hljs-comment"># 数据会自动分配到各个GPU中</span><br>        X_test, y_test = X_test.cuda(), y_test.cuda()<br>        outputs = model(X_test)<br>        _, pred = torch.<span class="hljs-built_in">max</span>(outputs.data, <span class="hljs-number">1</span>)<br>        testing_correct += torch.<span class="hljs-built_in">sum</span>(pred == y_test.data)<br>torch.save(model.state_dict(), <span class="hljs-string">&quot;model_parameter.pth&quot;</span>)<br></code></pre></div></td></tr></table></figure>
<h2 id="ddp训练">28、DDP训练</h2>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><br></code></pre></div></td></tr></table></figure>
<h2 id="warm-up和余弦退火">29、warm up和余弦退火</h2>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> math<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> lr_scheduler <span class="hljs-keyword">as</span> lr_scheduler<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader, Dataset<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">warm_up_cosine_lr_scheduler</span>(<span class="hljs-params">optimizer, epochs=<span class="hljs-number">100</span>, warm_up_epochs=<span class="hljs-number">10</span>, eta_min=<span class="hljs-number">1e-9</span></span>):<br><br>    <span class="hljs-keyword">if</span> warm_up_epochs &lt;= <span class="hljs-number">0</span>:<br>        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=eta_min)<br><br>    <span class="hljs-keyword">else</span>:<br>        warm_up_with_cosine_lr = <span class="hljs-keyword">lambda</span> epoch: eta_min + (<br>                    epoch / warm_up_epochs) <span class="hljs-keyword">if</span> epoch &lt;= warm_up_epochs <span class="hljs-keyword">else</span> <span class="hljs-number">0.5</span> * (<br>                np.cos((epoch - warm_up_epochs) / (epochs - warm_up_epochs) * np.pi) + <span class="hljs-number">1</span>)<br>        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warm_up_with_cosine_lr)<br><br>    <span class="hljs-keyword">return</span> scheduler<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WarmupCosineLR</span>(lr_scheduler._LRScheduler):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, optimizer, lr_min, lr_max, warm_up=<span class="hljs-number">0</span>, T_max=<span class="hljs-number">10</span>, start_ratio=<span class="hljs-number">0.1</span></span>):<br>        self.lr_min = lr_min<br>        self.lr_max = lr_max<br>        self.warm_up = warm_up<br>        self.T_max = T_max<br>        self.start_ratio = start_ratio<br>        self.cur = <span class="hljs-number">0</span>  <span class="hljs-comment"># current epoch or iteration</span><br><br>        <span class="hljs-built_in">super</span>().__init__(optimizer, -<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_lr</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> (self.warm_up == <span class="hljs-number">0</span>) &amp; (self.cur == <span class="hljs-number">0</span>):<br>            lr = self.lr_max<br>        <span class="hljs-keyword">elif</span> (self.warm_up != <span class="hljs-number">0</span>) &amp; (self.cur &lt;= self.warm_up):<br>            <span class="hljs-keyword">if</span> self.cur == <span class="hljs-number">0</span>:<br>                lr = self.lr_min + (self.lr_max - self.lr_min) * (self.cur + self.start_ratio) / self.warm_up<br>            <span class="hljs-keyword">else</span>:<br>                lr = self.lr_min + (self.lr_max - self.lr_min) * (self.cur) / self.warm_up<br>                <span class="hljs-comment"># print(f&#x27;&#123;self.cur&#125; -&gt; &#123;lr&#125;&#x27;)</span><br>        <span class="hljs-keyword">else</span>:<br>            lr = self.lr_min + (self.lr_max - self.lr_min) * <span class="hljs-number">0.5</span> * \<br>                 (np.cos((self.cur - self.warm_up) / (self.T_max - self.warm_up) * np.pi) + <span class="hljs-number">1</span>)<br><br>        self.cur += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">return</span> [lr <span class="hljs-keyword">for</span> base_lr <span class="hljs-keyword">in</span> self.base_lrs]<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TestDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.images = []<br>        self.labels = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num):<br>            tmp = torch.randn(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>            tmp2 = torch.randn(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>            self.images.append(tmp)<br>            self.labels.append(tmp2)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.images)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, item</span>):<br>        image = self.images[item]<br>        label = self.labels[item]<br>        <span class="hljs-keyword">return</span> image, label<br><br><br>test_dataset = TestDataset(<span class="hljs-number">10</span>)<br>train_loader = DataLoader(test_dataset, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.conv = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-keyword">return</span> self.conv(x)<br><br>net = Net().cuda()<br><br>T_0 = <span class="hljs-number">100</span><br>T_mult = <span class="hljs-number">2</span><br>eta_max = <span class="hljs-number">0.1</span><br>warmup = <span class="hljs-number">10</span><br>epochs = <span class="hljs-number">100</span><br><br>optimizer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">1e-3</span>, momentum=<span class="hljs-number">5e-4</span>, nesterov=<span class="hljs-literal">True</span>)<br>scheduler = WarmupCosineLR(optimizer, <span class="hljs-number">1e-9</span>, <span class="hljs-number">1e-3</span>, warmup, epochs, <span class="hljs-number">0.1</span>)<br><span class="hljs-comment"># scheduler = warm_up_cosine_lr_scheduler(optimizer, epochs, warmup)</span><br>criterion = nn.CrossEntropyLoss()<br><br>lrs = []<br><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(epochs)):<br>    net.train()<br>    <span class="hljs-keyword">for</span> i, (inputs, targets) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_loader):<br>        inputs, targets = inputs.cuda(), targets.cuda()<br><br>        optimizer.zero_grad()<br>        outputs = net(inputs)<br>        loss = criterion(outputs, targets)<br><br>        loss.backward()<br>        optimizer.step()<br>        lrs.append(optimizer.state_dict()[<span class="hljs-string">&#x27;param_groups&#x27;</span>][<span class="hljs-number">0</span>][<span class="hljs-string">&#x27;lr&#x27;</span>])<br>    scheduler.step()<br><br>plt.figure(figsize=(<span class="hljs-number">10</span>, <span class="hljs-number">6</span>))<br>plt.plot(lrs, color=<span class="hljs-string">&#x27;r&#x27;</span>)<br>plt.text(<span class="hljs-number">0</span>, lrs[<span class="hljs-number">0</span>], <span class="hljs-built_in">str</span>(lrs[<span class="hljs-number">0</span>]))<br>plt.text(<span class="hljs-number">10</span>, lrs[<span class="hljs-number">10</span>], <span class="hljs-built_in">str</span>(lrs[<span class="hljs-number">10</span>]))<br>plt.text(epochs, lrs[-<span class="hljs-number">1</span>], <span class="hljs-built_in">str</span>(lrs[-<span class="hljs-number">1</span>]))<br>plt.show()<br></code></pre></div></td></tr></table></figure>
<h2 id="混合精度训练">30、混合精度训练</h2>
<figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, optim<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> models, datasets, transforms<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> autocast, GradScaler<br><br><span class="hljs-comment"># 定义模型</span><br>model = models.resnet18(pretrained=<span class="hljs-literal">True</span>)<br>num_classes = <span class="hljs-number">10</span>  <span class="hljs-comment"># 根据你的任务修改输出类别数</span><br>model.fc = nn.Linear(model.fc.in_features, num_classes)<br><br><span class="hljs-comment"># 定义数据转换和加载器</span><br>transform = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>    transforms.RandomHorizontalFlip(),<br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>]),<br>])<br><br>dataset = datasets.CIFAR10(root=<span class="hljs-string">&#x27;path/to/cifar10&#x27;</span>, train=<span class="hljs-literal">True</span>, download=<span class="hljs-literal">True</span>, transform=transform)<br>dataloader = DataLoader(dataset, batch_size=<span class="hljs-number">32</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">4</span>)<br><br><span class="hljs-comment"># 定义优化器和学习率调度器</span><br>optimizer = optim.SGD(model.parameters(), lr=<span class="hljs-number">0.001</span>, momentum=<span class="hljs-number">0.9</span>)<br>scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=<span class="hljs-number">5</span>, gamma=<span class="hljs-number">0.1</span>)  <span class="hljs-comment"># 5 个 epoch 降低学习率</span><br><br><span class="hljs-comment"># 定义损失函数</span><br>criterion = nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># 定义混合精度训练的 GradScaler</span><br>scaler = GradScaler()<br><br><span class="hljs-comment"># 训练循环</span><br>num_epochs = <span class="hljs-number">10</span><br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):<br>    model.train()<br><br>    <span class="hljs-keyword">for</span> inputs, labels <span class="hljs-keyword">in</span> dataloader:<br>        inputs, labels = inputs.cuda(), labels.cuda()<br><br>        optimizer.zero_grad()<br><br>        <span class="hljs-keyword">with</span> autocast():<br>            outputs = model(inputs)<br>            loss = criterion(outputs, labels)<br><br>        <span class="hljs-comment"># 反向传播和优化</span><br>        scaler.scale(loss).backward()<br>        scaler.step(optimizer)<br>        scaler.update()<br><br>    <span class="hljs-comment"># 学习率调度器步进</span><br>    scheduler.step()<br></code></pre></div></td></tr></table></figure>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Deep-Learning/">Deep Learning</a>
                    
                  </div>
                
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/04/28/%E6%95%B0%E7%BB%84%E5%8F%8A%E6%95%B0%E5%AD%A6/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">数组及数学</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/04/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">
                        <span class="hidden-mobile">线性模型</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                

              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  






  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
