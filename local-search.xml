<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>FPN</title>
    <link href="/2022/09/30/FPN/"/>
    <url>/2022/09/30/FPN/</url>
    
    <content type="html"><![CDATA[<h1 id="模型介绍">模型介绍</h1><p><img src="/img/cv/image-20220930111912244.png" /></p><p><img src="/img/cv/image-20220930111926838.png" /></p><p><img src="/img/cv/image-20220930111940668.png" /></p><p><img src="/img/cv/image-20220930112000674.png" /></p><p><img src="/img/cv/image-20220930112024501.png" /></p><p><img src="/img/cv/image-20220930112036794.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>cv</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Faster_R-CNN</title>
    <link href="/2022/09/30/Faster-R-CNN/"/>
    <url>/2022/09/30/Faster-R-CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="模型介绍">模型介绍</h1><p><img src="/img/cv/image-20220930104009662.png" /></p><p><img src="/img/cv/image-20220930104029226.png" /></p><p><img src="/img/cv/image-20220930104047080.png" /></p><p><img src="/img/cv/image-20220930104101860.png" /></p><p><img src="/img/cv/image-20220930104253294.png" /></p><p><img src="/img/cv/image-20220930104309535.png" /></p><p><img src="/img/cv/image-20220930104322156.png" /></p><p><img src="/img/cv/image-20220930104332006.png" /></p><p><img src="/img/cv/image-20220930104345120.png" /></p><p><img src="/img/cv/image-20220930104356421.png" /></p><p><img src="/img/cv/image-20220930104409007.png" /></p><p><img src="/img/cv/image-20220930104420565.png" /></p><p><img src="/img/cv/image-20220930104433453.png" /></p><p><img src="/img/cv/image-20220930104446017.png" /></p><p><img src="/img/cv/image-20220930104500570.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>cv</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>Fast_R-CNN</title>
    <link href="/2022/09/30/Fast-R-CNN/"/>
    <url>/2022/09/30/Fast-R-CNN/</url>
    
    <content type="html"><![CDATA[<h1 id="模型介绍">模型介绍</h1><p><img src="/img/cv/image-20220930103138705.png" /></p><p><img src="/img/cv/image-20220930103151991.png" /></p><p><img src="/img/cv/image-20220930103204019.png" /></p><p><img src="/img/cv/image-20220930103243703.png" /></p><p><img src="/img/cv/image-20220930103316912.png" /></p><p><img src="/img/cv/image-20220930103331647.png" /></p><p><img src="/img/cv/image-20220930103413400.png" /></p><p><img src="/img/cv/image-20220930103431178.png" /></p><p><img src="/img/cv/image-20220930103445275.png" /></p><p><img src="/img/cv/image-20220930103507638.png" /></p><p><img src="/img/cv/image-20220930103520918.png" /></p><p><img src="/img/cv/image-20220930103548664.png" /></p><p><img src="/img/cv/image-20220930103600909.png" /></p><p><img src="/img/cv/image-20220930103638505.png" /></p><p><img src="/img/cv/image-20220930103656760.png" /></p><p><img src="/img/cv/image-20220930103721592.png" /></p><p><img src="/img/cv/image-20220930103736379.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>cv</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RCNN</title>
    <link href="/2022/09/29/RCNN/"/>
    <url>/2022/09/29/RCNN/</url>
    
    <content type="html"><![CDATA[<h1 id="模型介绍">模型介绍</h1><p><img src="/img/cv/image-20220929174947399.png" /></p><p><img src="/img/cv/image-20220929174959830.png" /></p><p><img src="/img/cv/image-20220929175012228.png" /></p><p><img src="/img/cv/image-20220929175024811.png" /></p><p><img src="/img/cv/image-20220929175035980.png" /></p><p><img src="/img/cv/image-20220929175055640.png" /></p><p><img src="/img/cv/image-20220929175116843.png" /></p><p><img src="/img/cv/image-20220929175137830.png" /></p><p><img src="/img/cv/image-20220929175155293.png" /></p><p><img src="/img/cv/image-20220929175207362.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>cv</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>GoogleNet</title>
    <link href="/2022/09/29/GoogleNet/"/>
    <url>/2022/09/29/GoogleNet/</url>
    
    <content type="html"><![CDATA[<h1 id="模型介绍">模型介绍</h1><p><img src="/img/cv/image-20220929165102079.png" /></p><p><img src="/img/cv/image-20220929165115906.png" /></p><p><img src="/img/cv/image-20220929165128922.png" /></p><p><img src="/img/cv/image-20220929165145364.png" /></p><p><img src="/img/cv/image-20220929165157614.png" /></p><p><img src="/img/cv/image-20220929165210033.png" /></p><p><img src="/img/cv/image-20220929165223735.png" /></p><h1 id="模型代码">模型代码</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">GoogLeNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">1000</span>, aux_logits=<span class="hljs-literal">True</span>, init_weights=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(GoogLeNet, self).__init__()<br>        self.aux_logits = aux_logits<br><br>        self.conv1 = BasicConv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>)<br>        self.maxpool1 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.conv2 = BasicConv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>)<br>        self.conv3 = BasicConv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.maxpool2 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.inception3a = Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, <span class="hljs-number">96</span>, <span class="hljs-number">128</span>, <span class="hljs-number">16</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>)<br>        self.inception3b = Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">192</span>, <span class="hljs-number">32</span>, <span class="hljs-number">96</span>, <span class="hljs-number">64</span>)<br>        self.maxpool3 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.inception4a = Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, <span class="hljs-number">96</span>, <span class="hljs-number">208</span>, <span class="hljs-number">16</span>, <span class="hljs-number">48</span>, <span class="hljs-number">64</span>)<br>        self.inception4b = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, <span class="hljs-number">112</span>, <span class="hljs-number">224</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.inception4c = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">24</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.inception4d = Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, <span class="hljs-number">144</span>, <span class="hljs-number">288</span>, <span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>)<br>        self.inception4e = Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br>        self.maxpool4 = nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>        self.inception5a = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, <span class="hljs-number">160</span>, <span class="hljs-number">320</span>, <span class="hljs-number">32</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br>        self.inception5b = Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, <span class="hljs-number">192</span>, <span class="hljs-number">384</span>, <span class="hljs-number">48</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)<br><br>        <span class="hljs-keyword">if</span> self.aux_logits:<br>            self.aux1 = InceptionAux(<span class="hljs-number">512</span>, num_classes)<br>            self.aux2 = InceptionAux(<span class="hljs-number">528</span>, num_classes)<br><br>        self.avgpool = nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.dropout = nn.Dropout(<span class="hljs-number">0.4</span>)<br>        self.fc = nn.Linear(<span class="hljs-number">1024</span>, num_classes)<br>        <span class="hljs-keyword">if</span> init_weights:<br>            self._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># N x 3 x 224 x 224</span><br>        x = self.conv1(x)<br>        <span class="hljs-comment"># N x 64 x 112 x 112</span><br>        x = self.maxpool1(x)<br>        <span class="hljs-comment"># N x 64 x 56 x 56</span><br>        x = self.conv2(x)<br>        <span class="hljs-comment"># N x 64 x 56 x 56</span><br>        x = self.conv3(x)<br>        <span class="hljs-comment"># N x 192 x 56 x 56</span><br>        x = self.maxpool2(x)<br><br>        <span class="hljs-comment"># N x 192 x 28 x 28</span><br>        x = self.inception3a(x)<br>        <span class="hljs-comment"># N x 256 x 28 x 28</span><br>        x = self.inception3b(x)<br>        <span class="hljs-comment"># N x 480 x 28 x 28</span><br>        x = self.maxpool3(x)<br>        <span class="hljs-comment"># N x 480 x 14 x 14</span><br>        x = self.inception4a(x)<br>        <span class="hljs-comment"># N x 512 x 14 x 14</span><br>        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.aux_logits:    <span class="hljs-comment"># eval model lose this layer</span><br>            aux1 = self.aux1(x)<br><br>        x = self.inception4b(x)<br>        <span class="hljs-comment"># N x 512 x 14 x 14</span><br>        x = self.inception4c(x)<br>        <span class="hljs-comment"># N x 512 x 14 x 14</span><br>        x = self.inception4d(x)<br>        <span class="hljs-comment"># N x 528 x 14 x 14</span><br>        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.aux_logits:    <span class="hljs-comment"># eval model lose this layer</span><br>            aux2 = self.aux2(x)<br><br>        x = self.inception4e(x)<br>        <span class="hljs-comment"># N x 832 x 14 x 14</span><br>        x = self.maxpool4(x)<br>        <span class="hljs-comment"># N x 832 x 7 x 7</span><br>        x = self.inception5a(x)<br>        <span class="hljs-comment"># N x 832 x 7 x 7</span><br>        x = self.inception5b(x)<br>        <span class="hljs-comment"># N x 1024 x 7 x 7</span><br><br>        x = self.avgpool(x)<br>        <span class="hljs-comment"># N x 1024 x 1 x 1</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># N x 1024</span><br>        x = self.dropout(x)<br>        x = self.fc(x)<br>        <span class="hljs-comment"># N x 1000 (num_classes)</span><br>        <span class="hljs-keyword">if</span> self.training <span class="hljs-keyword">and</span> self.aux_logits:   <span class="hljs-comment"># eval model lose this layer</span><br>            <span class="hljs-keyword">return</span> x, aux2, aux1<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                nn.init.normal_(m.weight, <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj</span>):<br>        <span class="hljs-built_in">super</span>(Inception, self).__init__()<br><br>        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=<span class="hljs-number">1</span>)<br><br>        self.branch2 = nn.Sequential(<br>            BasicConv2d(in_channels, ch3x3red, kernel_size=<span class="hljs-number">1</span>),<br>            BasicConv2d(ch3x3red, ch3x3, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)   <span class="hljs-comment"># 保证输出大小等于输入大小</span><br>        )<br><br>        self.branch3 = nn.Sequential(<br>            BasicConv2d(in_channels, ch5x5red, kernel_size=<span class="hljs-number">1</span>),<br>            <span class="hljs-comment"># 在官方的实现中，其实是3x3的kernel并不是5x5，这里我也懒得改了，具体可以参考下面的issue</span><br>            <span class="hljs-comment"># Please see https://github.com/pytorch/vision/issues/906 for details.</span><br>            BasicConv2d(ch5x5red, ch5x5, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)   <span class="hljs-comment"># 保证输出大小等于输入大小</span><br>        )<br><br>        self.branch4 = nn.Sequential(<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>            BasicConv2d(in_channels, pool_proj, kernel_size=<span class="hljs-number">1</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        branch1 = self.branch1(x)<br>        branch2 = self.branch2(x)<br>        branch3 = self.branch3(x)<br>        branch4 = self.branch4(x)<br><br>        outputs = [branch1, branch2, branch3, branch4]<br>        <span class="hljs-keyword">return</span> torch.cat(outputs, <span class="hljs-number">1</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">InceptionAux</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, num_classes</span>):<br>        <span class="hljs-built_in">super</span>(InceptionAux, self).__init__()<br>        self.averagePool = nn.AvgPool2d(kernel_size=<span class="hljs-number">5</span>, stride=<span class="hljs-number">3</span>)<br>        self.conv = BasicConv2d(in_channels, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">1</span>)  <span class="hljs-comment"># output[batch, 128, 4, 4]</span><br><br>        self.fc1 = nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">1024</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">1024</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        <span class="hljs-comment"># aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14</span><br>        x = self.averagePool(x)<br>        <span class="hljs-comment"># aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4</span><br>        x = self.conv(x)<br>        <span class="hljs-comment"># N x 128 x 4 x 4</span><br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        x = F.dropout(x, <span class="hljs-number">0.5</span>, training=self.training)<br>        <span class="hljs-comment"># N x 2048</span><br>        x = F.relu(self.fc1(x), inplace=<span class="hljs-literal">True</span>)<br>        x = F.dropout(x, <span class="hljs-number">0.5</span>, training=self.training)<br>        <span class="hljs-comment"># N x 1024</span><br>        x = self.fc2(x)<br>        <span class="hljs-comment"># N x num_classes</span><br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicConv2d</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, out_channels, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(BasicConv2d, self).__init__()<br>        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)<br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv(x)<br>        x = self.relu(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></div></td></tr></table></figure><h1 id="训练代码">训练代码</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms, datasets<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> GoogLeNet<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;using &#123;&#125; device.&quot;</span>.<span class="hljs-built_in">format</span>(device))<br><br>    data_transform = &#123;<br>        <span class="hljs-string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>                                     transforms.RandomHorizontalFlip(),<br>                                     transforms.ToTensor(),<br>                                     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))]),<br>        <span class="hljs-string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>                                   transforms.ToTensor(),<br>                                   transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])&#125;<br><br>    data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="hljs-string">&quot;../..&quot;</span>))  <span class="hljs-comment"># get data root path</span><br>    image_path = os.path.join(data_root, <span class="hljs-string">&quot;data_set&quot;</span>, <span class="hljs-string">&quot;flower_data&quot;</span>)  <span class="hljs-comment"># flower data set path</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(image_path), <span class="hljs-string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="hljs-built_in">format</span>(image_path)<br>    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="hljs-string">&quot;train&quot;</span>),<br>                                         transform=data_transform[<span class="hljs-string">&quot;train&quot;</span>])<br>    train_num = <span class="hljs-built_in">len</span>(train_dataset)<br><br>    <span class="hljs-comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span><br>    flower_list = train_dataset.class_to_idx<br>    cla_dict = <span class="hljs-built_in">dict</span>((val, key) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> flower_list.items())<br>    <span class="hljs-comment"># write dict into json file</span><br>    json_str = json.dumps(cla_dict, indent=<span class="hljs-number">4</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;class_indices.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> json_file:<br>        json_file.write(json_str)<br><br>    batch_size = <span class="hljs-number">32</span><br>    nw = <span class="hljs-built_in">min</span>([os.cpu_count(), batch_size <span class="hljs-keyword">if</span> batch_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>, <span class="hljs-number">8</span>])  <span class="hljs-comment"># number of workers</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="hljs-built_in">format</span>(nw))<br><br>    train_loader = torch.utils.data.DataLoader(train_dataset,<br>                                               batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>,<br>                                               num_workers=nw)<br><br>    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="hljs-string">&quot;val&quot;</span>),<br>                                            transform=data_transform[<span class="hljs-string">&quot;val&quot;</span>])<br>    val_num = <span class="hljs-built_in">len</span>(validate_dataset)<br>    validate_loader = torch.utils.data.DataLoader(validate_dataset,<br>                                                  batch_size=batch_size, shuffle=<span class="hljs-literal">False</span>,<br>                                                  num_workers=nw)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="hljs-built_in">format</span>(train_num,<br>                                                                           val_num))<br><br>    <span class="hljs-comment"># test_data_iter = iter(validate_loader)</span><br>    <span class="hljs-comment"># test_image, test_label = test_data_iter.next()</span><br><br>    net = GoogLeNet(num_classes=<span class="hljs-number">5</span>, aux_logits=<span class="hljs-literal">True</span>, init_weights=<span class="hljs-literal">True</span>)<br>    <span class="hljs-comment"># 如果要使用官方的预训练权重，注意是将权重载入官方的模型，不是我们自己实现的模型</span><br>    <span class="hljs-comment"># 官方的模型中使用了bn层以及改了一些参数，不能混用</span><br>    <span class="hljs-comment"># import torchvision</span><br>    <span class="hljs-comment"># net = torchvision.models.googlenet(num_classes=5)</span><br>    <span class="hljs-comment"># model_dict = net.state_dict()</span><br>    <span class="hljs-comment"># # 预训练权重下载地址: https://download.pytorch.org/models/googlenet-1378be20.pth</span><br>    <span class="hljs-comment"># pretrain_model = torch.load(&quot;googlenet.pth&quot;)</span><br>    <span class="hljs-comment"># del_list = [&quot;aux1.fc2.weight&quot;, &quot;aux1.fc2.bias&quot;,</span><br>    <span class="hljs-comment">#             &quot;aux2.fc2.weight&quot;, &quot;aux2.fc2.bias&quot;,</span><br>    <span class="hljs-comment">#             &quot;fc.weight&quot;, &quot;fc.bias&quot;]</span><br>    <span class="hljs-comment"># pretrain_dict = &#123;k: v for k, v in pretrain_model.items() if k not in del_list&#125;</span><br>    <span class="hljs-comment"># model_dict.update(pretrain_dict)</span><br>    <span class="hljs-comment"># net.load_state_dict(model_dict)</span><br>    net.to(device)<br>    loss_function = nn.CrossEntropyLoss()<br>    optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.0003</span>)<br><br>    epochs = <span class="hljs-number">30</span><br>    best_acc = <span class="hljs-number">0.0</span><br>    save_path = <span class="hljs-string">&#x27;./googleNet.pth&#x27;</span><br>    train_steps = <span class="hljs-built_in">len</span>(train_loader)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-comment"># train</span><br>        net.train()<br>        running_loss = <span class="hljs-number">0.0</span><br>        train_bar = tqdm(train_loader, file=sys.stdout)<br>        <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_bar):<br>            images, labels = data<br>            optimizer.zero_grad()<br>            logits, aux_logits2, aux_logits1 = net(images.to(device))<br>            loss0 = loss_function(logits, labels.to(device))<br>            loss1 = loss_function(aux_logits1, labels.to(device))<br>            loss2 = loss_function(aux_logits2, labels.to(device))<br>            loss = loss0 + loss1 * <span class="hljs-number">0.3</span> + loss2 * <span class="hljs-number">0.3</span><br>            loss.backward()<br>            optimizer.step()<br><br>            <span class="hljs-comment"># print statistics</span><br>            running_loss += loss.item()<br><br>            train_bar.desc = <span class="hljs-string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>,<br>                                                                     epochs,<br>                                                                     loss)<br><br>        <span class="hljs-comment"># validate</span><br>        net.<span class="hljs-built_in">eval</span>()<br>        acc = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># accumulate accurate number / epoch</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            val_bar = tqdm(validate_loader, file=sys.stdout)<br>            <span class="hljs-keyword">for</span> val_data <span class="hljs-keyword">in</span> val_bar:<br>                val_images, val_labels = val_data<br>                outputs = net(val_images.to(device))  <span class="hljs-comment"># eval model only have last output layer</span><br>                predict_y = torch.<span class="hljs-built_in">max</span>(outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]<br>                acc += torch.eq(predict_y, val_labels.to(device)).<span class="hljs-built_in">sum</span>().item()<br><br>        val_accurate = acc / val_num<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %<br>              (epoch + <span class="hljs-number">1</span>, running_loss / train_steps, val_accurate))<br><br>        <span class="hljs-keyword">if</span> val_accurate &gt; best_acc:<br>            best_acc = val_accurate<br>            torch.save(net.state_dict(), save_path)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()\<br></code></pre></div></td></tr></table></figure><h1 id="预测代码">预测代码</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> GoogLeNet<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>    data_transform = transforms.Compose(<br>        [transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>         transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br>    <span class="hljs-comment"># load image</span><br>    img_path = <span class="hljs-string">&quot;../tulip.jpg&quot;</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(img_path), <span class="hljs-string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="hljs-built_in">format</span>(img_path)<br>    img = Image.<span class="hljs-built_in">open</span>(img_path)<br>    plt.imshow(img)<br>    <span class="hljs-comment"># [N, C, H, W]</span><br>    img = data_transform(img)<br>    <span class="hljs-comment"># expand batch dimension</span><br>    img = torch.unsqueeze(img, dim=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># read class_indict</span><br>    json_path = <span class="hljs-string">&#x27;./class_indices.json&#x27;</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(json_path), <span class="hljs-string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="hljs-built_in">format</span>(json_path)<br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(json_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        class_indict = json.load(f)<br><br>    <span class="hljs-comment"># create model</span><br>    model = GoogLeNet(num_classes=<span class="hljs-number">5</span>, aux_logits=<span class="hljs-literal">False</span>).to(device)<br><br>    <span class="hljs-comment"># load model weights</span><br>    weights_path = <span class="hljs-string">&quot;./googleNet.pth&quot;</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(weights_path), <span class="hljs-string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="hljs-built_in">format</span>(weights_path)<br>    missing_keys, unexpected_keys = model.load_state_dict(torch.load(weights_path, map_location=device),<br>                                                          strict=<span class="hljs-literal">False</span>)<br><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># predict class</span><br>        output = torch.squeeze(model(img.to(device))).cpu()<br>        predict = torch.softmax(output, dim=<span class="hljs-number">0</span>)<br>        predict_cla = torch.argmax(predict).numpy()<br><br>    print_res = <span class="hljs-string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="hljs-built_in">format</span>(class_indict[<span class="hljs-built_in">str</span>(predict_cla)],<br>                                                 predict[predict_cla].numpy())<br>    plt.title(print_res)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(predict)):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="hljs-built_in">format</span>(class_indict[<span class="hljs-built_in">str</span>(i)],<br>                                                  predict[i].numpy()))<br>    plt.show()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>cv</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>AlexNet</title>
    <link href="/2022/09/29/AlexNet/"/>
    <url>/2022/09/29/AlexNet/</url>
    
    <content type="html"><![CDATA[<h1 id="网络介绍">网络介绍</h1><p><img src="/img/cv/image-20220929160500941.png" /></p><p><img src="/img/cv/image-20220929160537393.png" /></p><p><img src="/img/cv/image-20220929160556682.png" /></p><p><img src="/img/cv/image-20220929160608567.png" /></p><ul><li>使用了上下两层的结构，上下两层结构一样</li></ul><p><img src="/img/cv/image-20220929160655961.png" /></p><p><img src="/img/cv/image-20220929160708562.png" /></p><h1 id="模型代码">模型代码</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AlexNet</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_classes=<span class="hljs-number">1000</span>, init_weights=<span class="hljs-literal">False</span></span>):<br>        <span class="hljs-built_in">super</span>(AlexNet, self).__init__()<br>        self.features = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">48</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">2</span>),  <span class="hljs-comment"># input[3, 224, 224]  output[48, 55, 55]</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),                  <span class="hljs-comment"># output[48, 27, 27]</span><br>            nn.Conv2d(<span class="hljs-number">48</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),           <span class="hljs-comment"># output[128, 27, 27]</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),                  <span class="hljs-comment"># output[128, 13, 13]</span><br>            nn.Conv2d(<span class="hljs-number">128</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),          <span class="hljs-comment"># output[192, 13, 13]</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),          <span class="hljs-comment"># output[192, 13, 13]</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Conv2d(<span class="hljs-number">192</span>, <span class="hljs-number">128</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),          <span class="hljs-comment"># output[128, 13, 13]</span><br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),                  <span class="hljs-comment"># output[128, 6, 6]</span><br>        )<br>        self.classifier = nn.Sequential(<br>            nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">128</span> * <span class="hljs-number">6</span> * <span class="hljs-number">6</span>, <span class="hljs-number">2048</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>, <span class="hljs-number">2048</span>),<br>            nn.ReLU(inplace=<span class="hljs-literal">True</span>),<br>            nn.Linear(<span class="hljs-number">2048</span>, num_classes),<br>        )<br>        <span class="hljs-keyword">if</span> init_weights:<br>            self._initialize_weights()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.features(x)<br>        x = torch.flatten(x, start_dim=<span class="hljs-number">1</span>)<br>        x = self.classifier(x)<br>        <span class="hljs-keyword">return</span> x<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_initialize_weights</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> m <span class="hljs-keyword">in</span> self.modules():<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(m, nn.Conv2d):<br>                nn.init.kaiming_normal_(m.weight, mode=<span class="hljs-string">&#x27;fan_out&#x27;</span>, nonlinearity=<span class="hljs-string">&#x27;relu&#x27;</span>)<br>                <span class="hljs-keyword">if</span> m.bias <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                    nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br>            <span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(m, nn.Linear):<br>                nn.init.normal_(m.weight, <span class="hljs-number">0</span>, <span class="hljs-number">0.01</span>)<br>                nn.init.constant_(m.bias, <span class="hljs-number">0</span>)<br></code></pre></div></td></tr></table></figure><h1 id="训练代码">训练代码</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms, datasets, utils<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> AlexNet<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;using &#123;&#125; device.&quot;</span>.<span class="hljs-built_in">format</span>(device))<br><br>    data_transform = &#123;<br>        <span class="hljs-string">&quot;train&quot;</span>: transforms.Compose([transforms.RandomResizedCrop(<span class="hljs-number">224</span>),<br>                                     transforms.RandomHorizontalFlip(),<br>                                     transforms.ToTensor(),<br>                                     transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))]),<br>        <span class="hljs-string">&quot;val&quot;</span>: transforms.Compose([transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),  <span class="hljs-comment"># cannot 224, must (224, 224)</span><br>                                   transforms.ToTensor(),<br>                                   transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])&#125;<br><br>    data_root = os.path.abspath(os.path.join(os.getcwd(), <span class="hljs-string">&quot;../..&quot;</span>))  <span class="hljs-comment"># get data root path</span><br>    image_path = os.path.join(data_root, <span class="hljs-string">&quot;data_set&quot;</span>, <span class="hljs-string">&quot;flower_data&quot;</span>)  <span class="hljs-comment"># flower data set path</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(image_path), <span class="hljs-string">&quot;&#123;&#125; path does not exist.&quot;</span>.<span class="hljs-built_in">format</span>(image_path)<br>    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="hljs-string">&quot;train&quot;</span>),<br>                                         transform=data_transform[<span class="hljs-string">&quot;train&quot;</span>])<br>    train_num = <span class="hljs-built_in">len</span>(train_dataset)<br><br>    <span class="hljs-comment"># &#123;&#x27;daisy&#x27;:0, &#x27;dandelion&#x27;:1, &#x27;roses&#x27;:2, &#x27;sunflower&#x27;:3, &#x27;tulips&#x27;:4&#125;</span><br>    flower_list = train_dataset.class_to_idx<br>    cla_dict = <span class="hljs-built_in">dict</span>((val, key) <span class="hljs-keyword">for</span> key, val <span class="hljs-keyword">in</span> flower_list.items())<br>    <span class="hljs-comment"># write dict into json file</span><br>    json_str = json.dumps(cla_dict, indent=<span class="hljs-number">4</span>)<br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;class_indices.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> json_file:<br>        json_file.write(json_str)<br><br>    batch_size = <span class="hljs-number">32</span><br>    nw = <span class="hljs-built_in">min</span>([os.cpu_count(), batch_size <span class="hljs-keyword">if</span> batch_size &gt; <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>, <span class="hljs-number">8</span>])  <span class="hljs-comment"># number of workers</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Using &#123;&#125; dataloader workers every process&#x27;</span>.<span class="hljs-built_in">format</span>(nw))<br><br>    train_loader = torch.utils.data.DataLoader(train_dataset,<br>                                               batch_size=batch_size, shuffle=<span class="hljs-literal">True</span>,<br>                                               num_workers=nw)<br><br>    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, <span class="hljs-string">&quot;val&quot;</span>),<br>                                            transform=data_transform[<span class="hljs-string">&quot;val&quot;</span>])<br>    val_num = <span class="hljs-built_in">len</span>(validate_dataset)<br>    validate_loader = torch.utils.data.DataLoader(validate_dataset,<br>                                                  batch_size=<span class="hljs-number">4</span>, shuffle=<span class="hljs-literal">False</span>,<br>                                                  num_workers=nw)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;using &#123;&#125; images for training, &#123;&#125; images for validation.&quot;</span>.<span class="hljs-built_in">format</span>(train_num,<br>                                                                           val_num))<br>    <span class="hljs-comment"># test_data_iter = iter(validate_loader)</span><br>    <span class="hljs-comment"># test_image, test_label = test_data_iter.next()</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># def imshow(img):</span><br>    <span class="hljs-comment">#     img = img / 2 + 0.5  # unnormalize</span><br>    <span class="hljs-comment">#     npimg = img.numpy()</span><br>    <span class="hljs-comment">#     plt.imshow(np.transpose(npimg, (1, 2, 0)))</span><br>    <span class="hljs-comment">#     plt.show()</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># print(&#x27; &#x27;.join(&#x27;%5s&#x27; % cla_dict[test_label[j].item()] for j in range(4)))</span><br>    <span class="hljs-comment"># imshow(utils.make_grid(test_image))</span><br><br>    net = AlexNet(num_classes=<span class="hljs-number">5</span>, init_weights=<span class="hljs-literal">True</span>)<br><br>    net.to(device)<br>    loss_function = nn.CrossEntropyLoss()<br>    <span class="hljs-comment"># pata = list(net.parameters())</span><br>    optimizer = optim.Adam(net.parameters(), lr=<span class="hljs-number">0.0002</span>)<br><br>    epochs = <span class="hljs-number">10</span><br>    save_path = <span class="hljs-string">&#x27;./AlexNet.pth&#x27;</span><br>    best_acc = <span class="hljs-number">0.0</span><br>    train_steps = <span class="hljs-built_in">len</span>(train_loader)<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>        <span class="hljs-comment"># train</span><br>        net.train()<br>        running_loss = <span class="hljs-number">0.0</span><br>        train_bar = tqdm(train_loader, file=sys.stdout)<br>        <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(train_bar):<br>            images, labels = data<br>            optimizer.zero_grad()<br>            outputs = net(images.to(device))<br>            loss = loss_function(outputs, labels.to(device))<br>            loss.backward()<br>            optimizer.step()<br><br>            <span class="hljs-comment"># print statistics</span><br>            running_loss += loss.item()<br><br>            train_bar.desc = <span class="hljs-string">&quot;train epoch[&#123;&#125;/&#123;&#125;] loss:&#123;:.3f&#125;&quot;</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>,<br>                                                                     epochs,<br>                                                                     loss)<br><br>        <span class="hljs-comment"># validate</span><br>        net.<span class="hljs-built_in">eval</span>()<br>        acc = <span class="hljs-number">0.0</span>  <span class="hljs-comment"># accumulate accurate number / epoch</span><br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            val_bar = tqdm(validate_loader, file=sys.stdout)<br>            <span class="hljs-keyword">for</span> val_data <span class="hljs-keyword">in</span> val_bar:<br>                val_images, val_labels = val_data<br>                outputs = net(val_images.to(device))<br>                predict_y = torch.<span class="hljs-built_in">max</span>(outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]<br>                acc += torch.eq(predict_y, val_labels.to(device)).<span class="hljs-built_in">sum</span>().item()<br><br>        val_accurate = acc / val_num<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;[epoch %d] train_loss: %.3f  val_accuracy: %.3f&#x27;</span> %<br>              (epoch + <span class="hljs-number">1</span>, running_loss / train_steps, val_accurate))<br><br>        <span class="hljs-keyword">if</span> val_accurate &gt; best_acc:<br>            best_acc = val_accurate<br>            torch.save(net.state_dict(), save_path)<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Finished Training&#x27;</span>)<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></div></td></tr></table></figure><h1 id="预测代码">预测代码</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> json<br><br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> transforms<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> AlexNet<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">main</span>():<br>    device = torch.device(<span class="hljs-string">&quot;cuda:0&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)<br><br>    data_transform = transforms.Compose(<br>        [transforms.Resize((<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)),<br>         transforms.ToTensor(),<br>         transforms.Normalize((<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>), (<span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>, <span class="hljs-number">0.5</span>))])<br><br>    <span class="hljs-comment"># load image</span><br>    img_path = <span class="hljs-string">&quot;../tulip.jpg&quot;</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(img_path), <span class="hljs-string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="hljs-built_in">format</span>(img_path)<br>    img = Image.<span class="hljs-built_in">open</span>(img_path)<br><br>    plt.imshow(img)<br>    <span class="hljs-comment"># [N, C, H, W]</span><br>    img = data_transform(img)<br>    <span class="hljs-comment"># expand batch dimension</span><br>    img = torch.unsqueeze(img, dim=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># read class_indict</span><br>    json_path = <span class="hljs-string">&#x27;./class_indices.json&#x27;</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(json_path), <span class="hljs-string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="hljs-built_in">format</span>(json_path)<br><br>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(json_path, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>        class_indict = json.load(f)<br><br>    <span class="hljs-comment"># create model</span><br>    model = AlexNet(num_classes=<span class="hljs-number">5</span>).to(device)<br><br>    <span class="hljs-comment"># load model weights</span><br>    weights_path = <span class="hljs-string">&quot;./AlexNet.pth&quot;</span><br>    <span class="hljs-keyword">assert</span> os.path.exists(weights_path), <span class="hljs-string">&quot;file: &#x27;&#123;&#125;&#x27; dose not exist.&quot;</span>.<span class="hljs-built_in">format</span>(weights_path)<br>    model.load_state_dict(torch.load(weights_path))<br><br>    model.<span class="hljs-built_in">eval</span>()<br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-comment"># predict class</span><br>        output = torch.squeeze(model(img.to(device))).cpu()<br>        predict = torch.softmax(output, dim=<span class="hljs-number">0</span>)<br>        predict_cla = torch.argmax(predict).numpy()<br><br>    print_res = <span class="hljs-string">&quot;class: &#123;&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="hljs-built_in">format</span>(class_indict[<span class="hljs-built_in">str</span>(predict_cla)],<br>                                                 predict[predict_cla].numpy())<br>    plt.title(print_res)<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(predict)):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;class: &#123;:10&#125;   prob: &#123;:.3&#125;&quot;</span>.<span class="hljs-built_in">format</span>(class_indict[<span class="hljs-built_in">str</span>(i)],<br>                                                  predict[i].numpy()))<br>    plt.show()<br><br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    main()<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>cv</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>redis</title>
    <link href="/2022/09/19/redis/"/>
    <url>/2022/09/19/redis/</url>
    
    <content type="html"><![CDATA[<p>什么鬼啊！快点部署成功！！！！</p>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>递归</title>
    <link href="/2022/08/18/%E9%80%92%E5%BD%92/"/>
    <url>/2022/08/18/%E9%80%92%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h2 id="powx-n">50、Pow(x, n)</h2><p><img src="/img/LeetCode/数组及数学/50.png" /></p><p>使用递归的思想，直接循环会导致运行时间过长</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">myPow</span>(<span class="hljs-params">self, x: <span class="hljs-built_in">float</span>, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">quickPow</span>(<span class="hljs-params">N</span>):<br>            <span class="hljs-keyword">if</span> N == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span><br>            y = quickPow(N // <span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">return</span> y * y <span class="hljs-keyword">if</span> N % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> y * y * x<br>        <span class="hljs-keyword">return</span> quickPow(n) <span class="hljs-keyword">if</span> n &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1.0</span> / quickPow(-n)<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>SpringBoot</title>
    <link href="/2022/07/26/SpringBoot/"/>
    <url>/2022/07/26/SpringBoot/</url>
    
    <content type="html"><![CDATA[<h1 id="员工管理系统">员工管理系统</h1><h2 id="国际化">国际化</h2><p>新建properties配置文件，创建第二个zh_CN文件是会自动生成ResourceBundle文件夹，可通过右键文件夹快速生成配置文件</p><p><img src="/img/开发/SpringBoot/1.png" /></p><p>安装了Resource Bundle Editor插件后，可以点击左下角的ResourceBundle进行可视化配置</p><p><img src="/img/开发/SpringBoot/2.png" /></p><p>就会出现这个页面</p><p><img src="/img/开发/SpringBoot/3.png" /></p><p>右键添加配置</p><p><img src="/img/开发/SpringBoot/4.png" /></p><p>可通过此方法进行可视化配置国际化</p><p><img src="/img/开发/SpringBoot/5.png" /></p><p>随后在application.properties中将配置文件导入</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"># 国际化配置文件在这<br>spring.messages.basename=i18n.login<br></code></pre></div></td></tr></table></figure><p>在html中将需要国际化的文字进行修改，通过#{xxxx}取得，如下图所示</p><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">th:placeholder</span>=<span class="hljs-string">&quot;#&#123;login.username&#125;&quot;</span> <span class="hljs-attr">required</span>=<span class="hljs-string">&quot;&quot;</span> <span class="hljs-attr">autofocus</span>=<span class="hljs-string">&quot;&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;password&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">th:placeholder</span>=<span class="hljs-string">&quot;#&#123;login.password&#125;&quot;</span> <span class="hljs-attr">required</span>=<span class="hljs-string">&quot;&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;checkbox&quot;</span> <span class="hljs-attr">value</span>=<span class="hljs-string">&quot;remember-me&quot;</span>&gt;</span>[[#&#123;login.remember&#125;]]<br><span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-lg btn-primary btn-block&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;submit&quot;</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;#&#123;login.btn&#125;&quot;</span>&gt;</span>Sign in<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>为了能够在页面中点击中文或者英文按钮进行页面的中英文切换，我们在html中发送请求，并且在config文件夹新建MyLocaleResolver类</p><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm&quot;</span> <span class="hljs-attr">th:href</span>=<span class="hljs-string">&quot;@&#123;/index.html(l=&#x27;zh_CN&#x27;)&#125;&quot;</span>&gt;</span>中文<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm&quot;</span> <span class="hljs-attr">th:href</span>=<span class="hljs-string">&quot;@&#123;/index.html(l=&#x27;en_US&#x27;)&#125;&quot;</span>&gt;</span>English<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></div></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyLocaleResolver</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">LocaleResolver</span> &#123;<br><br>    <span class="hljs-comment">//解析请求</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> Locale <span class="hljs-title function_">resolveLocale</span><span class="hljs-params">(HttpServletRequest httpServletRequest)</span> &#123;<br>        <span class="hljs-comment">//获取请求中的语言参数</span><br>        <span class="hljs-type">String</span> <span class="hljs-variable">language</span> <span class="hljs-operator">=</span> httpServletRequest.getParameter(<span class="hljs-string">&quot;l&quot;</span>);<br>        <br>        <span class="hljs-comment">//如果没有就使用默认的</span><br>        <span class="hljs-type">Locale</span> <span class="hljs-variable">locale</span> <span class="hljs-operator">=</span> Locale.getDefault();<br>        <br>        <span class="hljs-comment">//如果请求的参数链接携带了国际化的参数</span><br>        <span class="hljs-keyword">if</span> (!StringUtils.isEmpty(language)) &#123;<br>            <span class="hljs-comment">//zh_CN</span><br>            String[] split = language.split(<span class="hljs-string">&quot;_&quot;</span>);<br>            <span class="hljs-comment">//国家，地区</span><br>            locale = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Locale</span>(split[<span class="hljs-number">0</span>], split[<span class="hljs-number">1</span>]);<br>        &#125;<br>        <span class="hljs-keyword">return</span> locale;<br>    &#125;<br><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">setLocale</span><span class="hljs-params">(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Locale locale)</span> &#123;<br><br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>最后在视图解析器中注册Bean</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Configuration</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">MyMVCConfig</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">WebMvcConfigurer</span> &#123;<br><br>    <span class="hljs-comment">//页面跳转</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">addViewControllers</span><span class="hljs-params">(ViewControllerRegistry registry)</span> &#123;<br>        registry.addViewController(<span class="hljs-string">&quot;/&quot;</span>).setViewName(<span class="hljs-string">&quot;index&quot;</span>);<br>        registry.addViewController(<span class="hljs-string">&quot;/index.html&quot;</span>).setViewName(<span class="hljs-string">&quot;index&quot;</span>);<br>    &#125;<br><br>    <span class="hljs-comment">//自定义的国际化主键就生效了</span><br>    <span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> LocaleResolver <span class="hljs-title function_">localeResolver</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">MyLocaleResolver</span>();<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><h2 id="登录功能实现">登录功能实现</h2><p>在html中进行表单请求</p><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">form</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-signin&quot;</span> <span class="hljs-attr">th:action</span>=<span class="hljs-string">&quot;@&#123;/user/login&#125;&quot;</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>后端新建LoginController类进行处理</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Controller</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">LoginController</span> &#123;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/user/login&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">login</span><span class="hljs-params">(<span class="hljs-meta">@RequestParam(&quot;username&quot;)</span> String username, <span class="hljs-meta">@RequestParam(&quot;username&quot;)</span> String password, Model model)</span> &#123;<br>        <span class="hljs-comment">//具体的业务</span><br>        <span class="hljs-keyword">if</span> (!StringUtils.isEmpty(username) &amp;&amp; <span class="hljs-string">&quot;123123&quot;</span>.equals(password)) &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;dashboard&quot;</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">//告诉用户，你登录错误了</span><br>            model.addAttribute(<span class="hljs-string">&quot;msg&quot;</span>, <span class="hljs-string">&quot;用户名或密码错误&quot;</span>);<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;index&quot;</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>在前端页面中显示登录错误的信息</p><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-comment">&lt;!--如果msg消息为空，就不显示消息--&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">p</span> <span class="hljs-attr">style</span>=<span class="hljs-string">&quot;color: red&quot;</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;msg&#125;&quot;</span> <span class="hljs-attr">th:if</span>=<span class="hljs-string">&quot;$&#123;not #strings.isEmpty(msg)&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">p</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>为了使得更加规范，所以我们要使用重定向，使得网址页面不会显示过多信息</p><p>先在视图解析器中添加main.html页面跳转</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//页面跳转</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">addViewControllers</span><span class="hljs-params">(ViewControllerRegistry registry)</span> &#123;<br>        registry.addViewController(<span class="hljs-string">&quot;/&quot;</span>).setViewName(<span class="hljs-string">&quot;index&quot;</span>);<br>        registry.addViewController(<span class="hljs-string">&quot;/index.html&quot;</span>).setViewName(<span class="hljs-string">&quot;index&quot;</span>);<br>        registry.addViewController(<span class="hljs-string">&quot;/main.html&quot;</span>).setViewName(<span class="hljs-string">&quot;dashboard&quot;</span>);<br>    &#125;<br></code></pre></div></td></tr></table></figure><p>修改controller中的重定向</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">if</span> (!StringUtils.isEmpty(username) &amp;&amp; <span class="hljs-string">&quot;123123&quot;</span>.equals(password)) &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;redirect:/main.html&quot;</span>;<br>        &#125;<br></code></pre></div></td></tr></table></figure><p>使用了重定向后，我们不需要登录都可以通过输入网址进行访问，所以我们需要拦截器</p><h2 id="登录拦截器">登录拦截器</h2><p>拦截器的思想是通过查看session中是否有登录的信息从而判断是否已经登录，所以我们先在LoginController中输入参数先加入HttpSession，在成功登录的时候添加session</p><p><code>addPathPatterns：该方法用于指定拦截路径，例如拦截路径为“/**”，表示拦截所有请求，包括对静态资源的请求。</code><code>excludePathPatterns：该方法用于排除拦截路径，即指定不需要被拦截器拦截的请求。</code></p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Controller</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">LoginController</span> &#123;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/user/login&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">login</span><span class="hljs-params">(<span class="hljs-meta">@RequestParam(&quot;username&quot;)</span> String username, <span class="hljs-meta">@RequestParam(&quot;password&quot;)</span> String password, Model model, HttpSession session)</span> &#123;<br>        <span class="hljs-comment">//具体</span><br>        <span class="hljs-keyword">if</span> (!StringUtils.isEmpty(username) &amp;&amp; <span class="hljs-string">&quot;123123&quot;</span>.equals(password)) &#123;<br>            session.setAttribute(<span class="hljs-string">&quot;loginUser&quot;</span>, username);<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;redirect:/main.html&quot;</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-comment">//告诉用户，你登录错误了</span><br>            model.addAttribute(<span class="hljs-string">&quot;msg&quot;</span>, <span class="hljs-string">&quot;用户名或密码错误&quot;</span>);<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;index&quot;</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>随后在Config文件夹中添加LoginHandlerInterceptor拦截器类，返回true代表可以访问，返回false代表不能访问</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">LoginHandlerInterceptor</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">HandlerInterceptor</span> &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">preHandle</span><span class="hljs-params">(HttpServletRequest request, HttpServletResponse response, Object handler)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br><br>        <span class="hljs-comment">//登录成功之后，应该有用户的session</span><br>        <span class="hljs-type">Object</span> <span class="hljs-variable">loginUser</span> <span class="hljs-operator">=</span> request.getSession().getAttribute(<span class="hljs-string">&quot;loginUser&quot;</span>);<br><br>        <span class="hljs-keyword">if</span> (loginUser == <span class="hljs-literal">null</span>) &#123;<br>            request.setAttribute(<span class="hljs-string">&quot;msg&quot;</span>, <span class="hljs-string">&quot;没有权限，请先登录&quot;</span>);<br>            request.getRequestDispatcher(<span class="hljs-string">&quot;/index.html&quot;</span>).forward(request, response);<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>        &#125;<br>        <span class="hljs-keyword">else</span> &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        &#125;<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>最后在视图解析器中添加拦截器，其中excludePathPatterns中的参数代表不需要过滤的资源，例如登录页面和静态资源</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">addInterceptors</span><span class="hljs-params">(InterceptorRegistry registry)</span> &#123;<br>        registry.addInterceptor(<span class="hljs-keyword">new</span> <span class="hljs-title class_">LoginHandlerInterceptor</span>()).addPathPatterns(<span class="hljs-string">&quot;/**&quot;</span>).excludePathPatterns(<span class="hljs-string">&quot;/index.html&quot;</span>, <span class="hljs-string">&quot;/&quot;</span>, <span class="hljs-string">&quot;/user/login&quot;</span>, <span class="hljs-string">&quot;/static/**&quot;</span>);<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><h2 id="展示员工列表">展示员工列表</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Controller</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">EmploeeController</span> &#123;<br><br>    <span class="hljs-meta">@Autowired</span><br>    EmployeeDao employeeDao;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/emps&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">list</span><span class="hljs-params">(Model model)</span> &#123;<br>        Collection&lt;Employee&gt; employees = employeeDao.getAll();<br>        model.addAttribute(<span class="hljs-string">&quot;emps&quot;</span>, employees);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;emp/list&quot;</span>;<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">main</span> <span class="hljs-attr">role</span>=<span class="hljs-string">&quot;main&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;col-md-9 ml-sm-auto col-lg-10 pt-3 px-4&quot;</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">h2</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm btn-success&quot;</span> <span class="hljs-attr">th:href</span>=<span class="hljs-string">&quot;@&#123;/emp&#125;&quot;</span>&gt;</span>添加员工<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">h2</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;table-responsive&quot;</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">table</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;table table-striped table-sm&quot;</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">thead</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">tr</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>id<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>lastName<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>email<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>gender<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>department<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>birth<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">th</span>&gt;</span>操作<span class="hljs-tag">&lt;/<span class="hljs-name">th</span>&gt;</span><br>                    <span class="hljs-tag">&lt;/<span class="hljs-name">tr</span>&gt;</span><br>                    <span class="hljs-tag">&lt;/<span class="hljs-name">thead</span>&gt;</span><br>                    <span class="hljs-tag">&lt;<span class="hljs-name">tbody</span>&gt;</span><br>                        <span class="hljs-tag">&lt;<span class="hljs-name">tr</span> <span class="hljs-attr">th:each</span>=<span class="hljs-string">&quot;emp:$&#123;emps&#125;&quot;</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;emp.getId()&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;emp.getLastName()&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;emp.getEmail()&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;emp.getGender()==0?&#x27;女&#x27;:&#x27;男&#x27;&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;emp.getDepartment().getDepartmentName()&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;#dates.format(emp.getBirth(), &#x27;yyyy-MM-dd HH:mm:ss&#x27;)&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                            <span class="hljs-tag">&lt;<span class="hljs-name">td</span>&gt;</span><br>                                <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm btn-primary&quot;</span>&gt;</span>编辑<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br>                                <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm btn-danger&quot;</span>&gt;</span>删除<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br>                            <span class="hljs-tag">&lt;/<span class="hljs-name">td</span>&gt;</span><br>                        <span class="hljs-tag">&lt;/<span class="hljs-name">tr</span>&gt;</span><br>                    <span class="hljs-tag">&lt;/<span class="hljs-name">tbody</span>&gt;</span><br>                <span class="hljs-tag">&lt;/<span class="hljs-name">table</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">main</span>&gt;</span><br></code></pre></div></td></tr></table></figure><h2 id="增加员工">增加员工</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@PostMapping(&quot;/emp&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">addEmp</span><span class="hljs-params">(Employee employee)</span> &#123;<br><br>        employeeDao.save(employee);<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;redirect:/emps&quot;</span>;<br>    &#125;<br></code></pre></div></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">form</span> <span class="hljs-attr">th:action</span>=<span class="hljs-string">&quot;@&#123;/emp&#125;&quot;</span> <span class="hljs-attr">method</span>=<span class="hljs-string">&quot;post&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>LastName<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;lastName&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;海绵宝宝&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>Email<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;email&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;email&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;1176244270@qq.com&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>Gender<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check form-check-inline&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-input&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;radio&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;gender&quot;</span> <span class="hljs-attr">value</span>=<span class="hljs-string">&quot;1&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-label&quot;</span>&gt;</span>男<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check form-check-inline&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-input&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;radio&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;gender&quot;</span> <span class="hljs-attr">value</span>=<span class="hljs-string">&quot;0&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-label&quot;</span>&gt;</span>女<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>department<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">select</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;department.id&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">option</span> <span class="hljs-attr">th:each</span>=<span class="hljs-string">&quot;dept:$&#123;departments&#125;&quot;</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;dept.getDepartmentName()&#125;&quot;</span> <span class="hljs-attr">th:value</span>=<span class="hljs-string">&quot;$&#123;dept.getId()&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">option</span>&gt;</span><br>                 <span class="hljs-tag">&lt;/<span class="hljs-name">select</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>Birth<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;birth&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;嘤嘤嘤&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;submit&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-primary&quot;</span>&gt;</span>添加<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br>         <span class="hljs-tag">&lt;/<span class="hljs-name">form</span>&gt;</span><br></code></pre></div></td></tr></table></figure><h2 id="修改员工">修改员工</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//去员工的修改页面</span><br>    <span class="hljs-meta">@GetMapping(&quot;/emp/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">toUpdateEmp</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span>Integer id, Model model)</span> &#123;<br>        <span class="hljs-comment">//查出来原来的数据</span><br>        <span class="hljs-type">Employee</span> <span class="hljs-variable">employee</span> <span class="hljs-operator">=</span> employeeDao.getEmployById(id);<br>        model.addAttribute(<span class="hljs-string">&quot;emp&quot;</span>, employee);<br><br>        Collection&lt;Department&gt; departments = departmentDao.getDepartments();<br>        model.addAttribute(<span class="hljs-string">&quot;departments&quot;</span>, departments);<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;emp/update&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@PostMapping(&quot;/updateEmp&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">updateEmp</span><span class="hljs-params">(Employee employee)</span> &#123;<br>        employeeDao.save(employee);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;redirect:/emps&quot;</span>;<br>    &#125;<br></code></pre></div></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm btn-primary&quot;</span> <span class="hljs-attr">th:href</span>=<span class="hljs-string">&quot;@&#123;/emp/&#123;id&#125;(id=$&#123;emp.getId()&#125;)&#125;&quot;</span>&gt;</span>编辑<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></div></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">form</span> <span class="hljs-attr">th:action</span>=<span class="hljs-string">&quot;@&#123;/updateEmp&#125;&quot;</span> <span class="hljs-attr">method</span>=<span class="hljs-string">&quot;post&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;hidden&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;id&quot;</span> <span class="hljs-attr">th:value</span>=<span class="hljs-string">&quot;$&#123;emp.getId()&#125;&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>LastName<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">th:value</span>=<span class="hljs-string">&quot;$&#123;emp.getLastName()&#125;&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;lastName&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;海绵宝宝&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>Email<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">th:value</span>=<span class="hljs-string">&quot;$&#123;emp.getEmail()&#125;&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;email&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;email&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;1176244270@qq.com&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>Gender<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><span class="hljs-tag">&lt;<span class="hljs-name">br</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check form-check-inline&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">th:checked</span>=<span class="hljs-string">&quot;$&#123;emp.getGender()==1&#125;&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-input&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;radio&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;gender&quot;</span> <span class="hljs-attr">value</span>=<span class="hljs-string">&quot;1&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-label&quot;</span>&gt;</span>男<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check form-check-inline&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">th:checked</span>=<span class="hljs-string">&quot;$&#123;emp.getGender()==0&#125;&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-input&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;radio&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;gender&quot;</span> <span class="hljs-attr">value</span>=<span class="hljs-string">&quot;0&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">label</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-check-label&quot;</span>&gt;</span>女<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>department<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">select</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;department.id&quot;</span>&gt;</span><br>                     <span class="hljs-tag">&lt;<span class="hljs-name">option</span> <span class="hljs-attr">th:selected</span>=<span class="hljs-string">&quot;$&#123;dept.getId() == emp.getDepartment().getId()&#125;&quot;</span> <span class="hljs-attr">th:each</span>=<span class="hljs-string">&quot;dept:$&#123;departments&#125;&quot;</span> <span class="hljs-attr">th:text</span>=<span class="hljs-string">&quot;$&#123;dept.getDepartmentName()&#125;&quot;</span> <span class="hljs-attr">th:value</span>=<span class="hljs-string">&quot;$&#123;dept.getId()&#125;&quot;</span>&gt;</span><span class="hljs-tag">&lt;/<span class="hljs-name">option</span>&gt;</span><br>                 <span class="hljs-tag">&lt;/<span class="hljs-name">select</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">div</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-group&quot;</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">label</span>&gt;</span>Birth<span class="hljs-tag">&lt;/<span class="hljs-name">label</span>&gt;</span><br>                 <span class="hljs-tag">&lt;<span class="hljs-name">input</span> <span class="hljs-attr">th:value</span>=<span class="hljs-string">&quot;$&#123;#dates.format(emp.getBirth(), &#x27;yyyy-MM-dd HH:mm&#x27;)&#125;&quot;</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;text&quot;</span> <span class="hljs-attr">name</span>=<span class="hljs-string">&quot;birth&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;form-control&quot;</span> <span class="hljs-attr">placeholder</span>=<span class="hljs-string">&quot;嘤嘤嘤&quot;</span>&gt;</span><br>             <span class="hljs-tag">&lt;/<span class="hljs-name">div</span>&gt;</span><br>             <span class="hljs-tag">&lt;<span class="hljs-name">button</span> <span class="hljs-attr">type</span>=<span class="hljs-string">&quot;submit&quot;</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-primary&quot;</span>&gt;</span>修改<span class="hljs-tag">&lt;/<span class="hljs-name">button</span>&gt;</span><br>         <span class="hljs-tag">&lt;/<span class="hljs-name">form</span>&gt;</span><br></code></pre></div></td></tr></table></figure><h2 id="删除员工">删除员工</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//删除员工</span><br>    <span class="hljs-meta">@GetMapping(&quot;/deleteEmp/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">deleteEmp</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span> <span class="hljs-type">int</span> id)</span> &#123;<br>        employeeDao.delete(id);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;redirect:/emps&quot;</span>;<br>    &#125;<br></code></pre></div></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;btn btn-sm btn-danger&quot;</span> <span class="hljs-attr">th:href</span>=<span class="hljs-string">&quot;@&#123;/deleteEmp/&#123;id&#125;(id=$&#123;emp.getId()&#125;)&#125;&quot;</span>&gt;</span>删除<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></div></td></tr></table></figure><h2 id="section">404</h2><p>SpringBoot只用在templates中新建一个error文件夹，在error中放置我们自定义的404.html即可</p><h2 id="注销">注销</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@RequestMapping(&quot;/user/logout&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">logout</span><span class="hljs-params">(HttpSession session)</span> &#123;<br>        session.invalidate();<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;redirect:/index.html&quot;</span>;<br>    &#125;<br></code></pre></div></td></tr></table></figure><figure class="highlight html"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">a</span> <span class="hljs-attr">class</span>=<span class="hljs-string">&quot;nav-link&quot;</span> <span class="hljs-attr">th:href</span>=<span class="hljs-string">&quot;@&#123;/user/logout&#125;&quot;</span>&gt;</span>退出<span class="hljs-tag">&lt;/<span class="hljs-name">a</span>&gt;</span><br></code></pre></div></td></tr></table></figure><h1 id="spring-data">Spring Data</h1><h2 id="整合jdbc">整合JDBC</h2><p>新建项目时需要勾选上关系型数据库中的<code>JDBC API</code>以及<code>MYSQL Driver</code></p><p>首先新建一个application.yml文件，在文件中输入</p><figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">datasource:</span><br>    <span class="hljs-attr">username:</span> <span class="hljs-string">root</span><br>    <span class="hljs-attr">password:</span> <span class="hljs-number">123123</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">jdbc:mysql://localhost:3306/mybatis?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8</span><br>    <span class="hljs-attr">driver-class-name:</span> <span class="hljs-string">com.mysql.cj.jdbc.Driver</span><br></code></pre></div></td></tr></table></figure><p>编写一个Controller来测试一下jdbc的使用</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@RestController</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">JDBCController</span> &#123;<br>    <span class="hljs-meta">@Autowired</span><br>    JdbcTemplate jdbcTemplate;<br><br>    <span class="hljs-comment">//查询数据库的所有信息</span><br>    <span class="hljs-meta">@GetMapping(&quot;/userList&quot;)</span><br>    <span class="hljs-keyword">public</span> List&lt;Map&lt;String, Object&gt;&gt; <span class="hljs-title function_">userList</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">sql</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;select * from user&quot;</span>;<br>        List&lt;Map&lt;String, Object&gt;&gt; list_maps = jdbcTemplate.queryForList(sql);<br>        <span class="hljs-keyword">return</span> list_maps;<br>    &#125;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/addList&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">addUser</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">sql</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;insert into mybatis.user values (4, &#x27;小明&#x27;, &#x27;123123&#x27;)&quot;</span>;<br>        jdbcTemplate.update(sql);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;add-ok&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/updateList/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">updateUser</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span> <span class="hljs-type">int</span> id)</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">sql</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;update mybatis.user set name=?, pwd=? where id =&quot;</span> + id;<br>        Object[] objects = <span class="hljs-keyword">new</span> <span class="hljs-title class_">Object</span>[<span class="hljs-number">2</span>];<br>        objects[<span class="hljs-number">0</span>] = <span class="hljs-string">&quot;小明2&quot;</span>;<br>        objects[<span class="hljs-number">1</span>] = <span class="hljs-string">&quot;asdfasdf&quot;</span>;<br>        jdbcTemplate.update(sql, objects);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;update-ok&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/deleteList/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">deleteUser</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span> <span class="hljs-type">int</span> id)</span> &#123;<br>        <span class="hljs-type">String</span> <span class="hljs-variable">sql</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;delete from mybatis.user where id=?&quot;</span> ;<br>        jdbcTemplate.update(sql, id);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;delete-ok&quot;</span>;<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><h2 id="整合druid数据源">整合Druid数据源</h2><p>添加依赖</p><figure class="highlight xml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>com.alibaba<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>druid<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>1.2.11<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>在application.yml中进行Druid配置</p><figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">spring:</span><br>  <span class="hljs-attr">datasource:</span><br>    <span class="hljs-attr">username:</span> <span class="hljs-string">root</span><br>    <span class="hljs-attr">password:</span> <span class="hljs-number">123123</span><br>    <span class="hljs-attr">url:</span> <span class="hljs-string">jdbc:mysql://localhost:3306/mybatis?serverTimezone=UTC&amp;useUnicode=true&amp;characterEncoding=utf-8</span><br>    <span class="hljs-attr">driver-class-name:</span> <span class="hljs-string">com.mysql.cj.jdbc.Driver</span><br>    <span class="hljs-attr">type:</span> <span class="hljs-string">com.alibaba.druid.pool.DruidDataSource</span><br><br>    <span class="hljs-comment">#SpringBoot默认是不注入这些的，需要自己绑定</span><br>    <span class="hljs-comment">#druid数据源专有配置</span><br>    <span class="hljs-attr">initialSize:</span> <span class="hljs-number">5</span><br>    <span class="hljs-attr">minIdle:</span> <span class="hljs-number">5</span><br>    <span class="hljs-attr">maxActive:</span> <span class="hljs-number">20</span><br>    <span class="hljs-attr">maxWait:</span> <span class="hljs-number">60000</span><br>    <span class="hljs-attr">timeBetweenEvictionRunsMillis:</span> <span class="hljs-number">60000</span><br>    <span class="hljs-attr">minEvictableIdleTimeMillis:</span> <span class="hljs-number">300000</span><br>    <span class="hljs-attr">validationQuery:</span> <span class="hljs-string">SELECT</span> <span class="hljs-number">1</span> <span class="hljs-string">FROM</span> <span class="hljs-string">DUAL</span><br>    <span class="hljs-attr">testWhileIdle:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">testOnBorrow:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">testOnReturn:</span> <span class="hljs-literal">false</span><br>    <span class="hljs-attr">poolPreparedStatements:</span> <span class="hljs-literal">true</span><br><br>    <span class="hljs-comment">#配置监控统计拦截的filters，stat：监控统计、log4j：日志记录、wall：防御sql注入</span><br>    <span class="hljs-comment">#如果允许报错，java.lang.ClassNotFoundException: org.apache.Log4j.Properity</span><br>    <span class="hljs-comment">#则导入log4j 依赖就行</span><br>    <span class="hljs-attr">filters:</span> <span class="hljs-string">stat,wall,log4j</span><br>    <span class="hljs-attr">maxPoolPreparedStatementPerConnectionSize:</span> <span class="hljs-number">20</span><br>    <span class="hljs-attr">useGlobalDataSourceStat:</span> <span class="hljs-literal">true</span><br>    <span class="hljs-attr">connectionoProperties:</span> <span class="hljs-string">druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500</span><br></code></pre></div></td></tr></table></figure><p>我们新建config文件夹，新建一个DruidConfig类进行自定义配置</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Configuration</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">DruidConfig</span> &#123;<br><br>    <span class="hljs-meta">@ConfigurationProperties(prefix = &quot;spring.datasource&quot;)</span><br>    <span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> DataSource <span class="hljs-title function_">druidDataSource</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">DruidDataSource</span>();<br>    &#125;<br><br>    <span class="hljs-comment">//后台监控</span><br>    <span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> ServletRegistrationBean <span class="hljs-title function_">statViewServlet</span><span class="hljs-params">()</span> &#123;<br>        ServletRegistrationBean&lt;StatViewServlet&gt; bean= <span class="hljs-keyword">new</span> <span class="hljs-title class_">ServletRegistrationBean</span>&lt;&gt;(<span class="hljs-keyword">new</span> <span class="hljs-title class_">StatViewServlet</span>(), <span class="hljs-string">&quot;/druid/*&quot;</span>);<br>        <span class="hljs-comment">//后台需要有人登录</span><br>        HashMap&lt;String, String&gt; initParameters = <span class="hljs-keyword">new</span> <span class="hljs-title class_">HashMap</span>&lt;&gt;();<br>        <span class="hljs-comment">//增加配置</span><br>        initParameters.put(<span class="hljs-string">&quot;loinUsername&quot;</span>, <span class="hljs-string">&quot;admin&quot;</span>); <span class="hljs-comment">//登录key是固定的 loinUsername和loginPassword</span><br>        initParameters.put(<span class="hljs-string">&quot;loginPassword&quot;</span>, <span class="hljs-string">&quot;123123&quot;</span>);<br><br>        <span class="hljs-comment">//允许谁访问</span><br>        initParameters.put(<span class="hljs-string">&quot;allow&quot;</span>, <span class="hljs-string">&quot;&quot;</span>);<br><br>        bean.setInitParameters(initParameters);<span class="hljs-comment">//初始化参数</span><br>        <span class="hljs-keyword">return</span> bean;<br>    &#125;<br></code></pre></div></td></tr></table></figure><p>在网页中输入<code>localhost:8080/druid</code>即可进入后台监控，可以在监控中查看各种sql，Session等。</p><h2 id="整合mybatis">整合MyBatis</h2><p>导入与SpringBoot整合的依赖</p><figure class="highlight xml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.mybatis.spring.boot<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>mybatis-spring-boot-starter<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.2.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>新建一个pojo</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Data</span><br><span class="hljs-meta">@NoArgsConstructor</span><br><span class="hljs-meta">@AllArgsConstructor</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">User</span> &#123;<br><br>    <span class="hljs-keyword">private</span> <span class="hljs-type">int</span> id;<br>    <span class="hljs-keyword">private</span> String name;<br>    <span class="hljs-keyword">private</span> String pwd;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>新建一个Mapper接口</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//这个注解表示了这是一个mybatis的mapper类</span><br><span class="hljs-meta">@Mapper</span><br><span class="hljs-meta">@Repository</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">interface</span> <span class="hljs-title class_">UserMapper</span> &#123;<br>    List&lt;User&gt; <span class="hljs-title function_">queryUserList</span><span class="hljs-params">()</span>;<br><br>    User <span class="hljs-title function_">queryUserById</span><span class="hljs-params">(<span class="hljs-type">int</span> id)</span>;<br><br>    <span class="hljs-type">int</span> <span class="hljs-title function_">addUser</span><span class="hljs-params">(User user)</span>;<br><br>    <span class="hljs-type">int</span> <span class="hljs-title function_">updateUser</span><span class="hljs-params">(User user)</span>;<br><br>    <span class="hljs-type">int</span> <span class="hljs-title function_">deleteUser</span><span class="hljs-params">(<span class="hljs-type">int</span> id)</span>;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>对于Mapper接口的实现需要在resources文件夹中进行实现，新建文件目录为</p><ul><li>resources<ul><li>mybatis<ul><li>mapper<ul><li>UserMapper.xml</li></ul></li></ul></li></ul></li></ul><figure class="highlight xml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span> ?&gt;</span><br><span class="hljs-meta">&lt;!DOCTYPE <span class="hljs-keyword">mapper</span></span><br><span class="hljs-meta">        <span class="hljs-keyword">PUBLIC</span> <span class="hljs-string">&quot;-//mybatis.org//dtd Mapper 3.0//EN&quot;</span></span><br><span class="hljs-meta">        <span class="hljs-string">&quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot;</span>&gt;</span><br><span class="hljs-tag">&lt;<span class="hljs-name">mapper</span> <span class="hljs-attr">namespace</span>=<span class="hljs-string">&quot;com.zhou.mapper.UserMapper&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">select</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;queryUserList&quot;</span> <span class="hljs-attr">resultType</span>=<span class="hljs-string">&quot;User&quot;</span>&gt;</span><br>        select * from user;<br>    <span class="hljs-tag">&lt;/<span class="hljs-name">select</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">select</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;queryUserById&quot;</span> <span class="hljs-attr">resultType</span>=<span class="hljs-string">&quot;User&quot;</span>&gt;</span><br>        select * from user where id = #&#123;id&#125;;<br>    <span class="hljs-tag">&lt;/<span class="hljs-name">select</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">insert</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;addUser&quot;</span> <span class="hljs-attr">parameterType</span>=<span class="hljs-string">&quot;User&quot;</span>&gt;</span><br>        insert into mybatis.user(id, name, pwd) VALUES (#&#123;id&#125;,#&#123;name&#125;,#&#123;pwd&#125;);<br>    <span class="hljs-tag">&lt;/<span class="hljs-name">insert</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">update</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;updateUser&quot;</span> <span class="hljs-attr">parameterType</span>=<span class="hljs-string">&quot;User&quot;</span>&gt;</span><br>        update user set name = #&#123;name&#125;,pwd=#&#123;pwd&#125; where id = #&#123;id&#125;;<br>    <span class="hljs-tag">&lt;/<span class="hljs-name">update</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">delete</span> <span class="hljs-attr">id</span>=<span class="hljs-string">&quot;deleteUserById&quot;</span> <span class="hljs-attr">parameterType</span>=<span class="hljs-string">&quot;int&quot;</span>&gt;</span><br>        delete from user where id = #&#123;id&#125;;<br>    <span class="hljs-tag">&lt;/<span class="hljs-name">delete</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">mapper</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>为了使得包能够被扫描到，需要在application中进行配置</p><figure class="highlight properties"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs properties"><span class="hljs-comment"># 整合mybatis</span><br><span class="hljs-attr">mybatis.type-aliases-package</span>=<span class="hljs-string">com.zhou.pojo</span><br><span class="hljs-attr">mybatis.mapper-locations</span>=<span class="hljs-string">classpath:mybatis/mapper/*.xml</span><br></code></pre></div></td></tr></table></figure><p>为了学习，省略了service层，直接从Controller层调用Mapper</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@RestController</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">UserController</span> &#123;<br><br>    <span class="hljs-meta">@Autowired</span><br>    <span class="hljs-keyword">private</span> UserMapper userMapper;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/queryUserList&quot;)</span><br>    <span class="hljs-keyword">public</span> List&lt;User&gt; <span class="hljs-title function_">queryUserList</span><span class="hljs-params">()</span> &#123;<br>        List&lt;User&gt; users = userMapper.queryUserList();<br>        <span class="hljs-keyword">return</span> users;<br>    &#125;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/addUser&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">addUser</span><span class="hljs-params">()</span> &#123;<br>        userMapper.addUser(<span class="hljs-keyword">new</span> <span class="hljs-title class_">User</span>(<span class="hljs-number">4</span>, <span class="hljs-string">&quot;肘子开&quot;</span>, <span class="hljs-string">&quot;123123&quot;</span>));<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;add_finished&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/updateUser&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">updateUser</span><span class="hljs-params">()</span> &#123;<br>        userMapper.updateUser(<span class="hljs-keyword">new</span> <span class="hljs-title class_">User</span>(<span class="hljs-number">4</span>, <span class="hljs-string">&quot;肘子开2&quot;</span>, <span class="hljs-string">&quot;12351234&quot;</span>));<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;update_finished&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@GetMapping(&quot;/deleteUser&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">deleteUser</span><span class="hljs-params">()</span> &#123;<br>        userMapper.deleteUser(<span class="hljs-number">4</span>);<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;delete_finished&quot;</span>;<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><h1 id="springsecurity">SpringSecurity</h1><p>shiro和SpringSecurity很像，除了类不一样，名字不一样</p><ul><li>功能权限</li><li>访问权限</li><li>菜单权限</li><li>拦截器、过滤器</li></ul><h2 id="用户认证和授权">用户认证和授权</h2><p>导入模板</p><p><img src="/img/开发/SpringBoot/8.png" /></p><p>先对页面访问进行编写controller进行跳转</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Controller</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">RouterController</span> &#123;<br>    <span class="hljs-meta">@RequestMapping(&#123;&quot;/&quot;,&quot;/index&quot;&#125;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">index</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;index&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/toLogin&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">toLogin</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;views/login&quot;</span>;<br>    &#125;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/level1/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">level1</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span> <span class="hljs-type">int</span> id)</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;views/level1/&quot;</span> + id;<br>    &#125;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/level2/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">level2</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span> <span class="hljs-type">int</span> id)</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;views/level2/&quot;</span> + id;<br>    &#125;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/level3/&#123;id&#125;&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">level3</span><span class="hljs-params">(<span class="hljs-meta">@PathVariable(&quot;id&quot;)</span> <span class="hljs-type">int</span> id)</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;views/level3/&quot;</span> + id;<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><p>访问主页为</p><p><img src="/img/开发/SpringBoot/9.png" /></p><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.springframework.boot<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spring-boot-starter-security<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>现在我们要实现不同权限的人访问不同的功能页面，例如只有vip1权限的用户才能访问level1下的页面，这时候需要新建一个配置类进行配置</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@EnableWebSecurity</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SecurityConfig</span> <span class="hljs-keyword">extends</span> <span class="hljs-title class_">WebSecurityConfigurerAdapter</span> &#123;<br><br>    <span class="hljs-comment">//授权</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(HttpSecurity http)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-comment">//首页所有人都可以访问，功能页只有对应有权限的人才能访问</span><br>        <span class="hljs-comment">//第一行代表首页所有人都可以访问 第二行代表需要vip1才能进行访问</span><br>        http.authorizeRequests()<br>                .antMatchers(<span class="hljs-string">&quot;/&quot;</span>).permitAll()<br>                .antMatchers(<span class="hljs-string">&quot;/level1/**&quot;</span>).hasRole(<span class="hljs-string">&quot;vip1&quot;</span>)<br>                .antMatchers(<span class="hljs-string">&quot;/level2/**&quot;</span>).hasRole(<span class="hljs-string">&quot;vip2&quot;</span>)<br>                .antMatchers(<span class="hljs-string">&quot;/level3/**&quot;</span>).hasRole(<span class="hljs-string">&quot;vip3&quot;</span>);<br><br>        <span class="hljs-comment">//没有权限默认会回到登陆页面</span><br>        http.formLogin();<br>    &#125;<br><br>    <span class="hljs-comment">//认证</span><br>    <span class="hljs-comment">//密码编码：PasswordEncoder</span><br>    <span class="hljs-comment">//在Spring Security 5.0+ 新增了很多的加密方法 需要对密码进行加密，否则会报错</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(AuthenticationManagerBuilder auth)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br><br>        <span class="hljs-comment">//这些数据正常应该从数据库中读</span><br>        auth.inMemoryAuthentication().passwordEncoder(<span class="hljs-keyword">new</span> <span class="hljs-title class_">BCryptPasswordEncoder</span>())<br>                .withUser(<span class="hljs-string">&quot;zhouzikai&quot;</span>).password(<span class="hljs-keyword">new</span> <span class="hljs-title class_">BCryptPasswordEncoder</span>().encode(<span class="hljs-string">&quot;123123&quot;</span>)).roles(<span class="hljs-string">&quot;vip2&quot;</span>, <span class="hljs-string">&quot;vip3&quot;</span>)<br>                .and()<br>                .withUser(<span class="hljs-string">&quot;root&quot;</span>).password(<span class="hljs-keyword">new</span> <span class="hljs-title class_">BCryptPasswordEncoder</span>().encode(<span class="hljs-string">&quot;123123&quot;</span>)).roles(<span class="hljs-string">&quot;vip1&quot;</span>, <span class="hljs-string">&quot;vip2&quot;</span>, <span class="hljs-string">&quot;vip3&quot;</span>)<br>                .and()<br>                .withUser(<span class="hljs-string">&quot;guest&quot;</span>).password(<span class="hljs-keyword">new</span> <span class="hljs-title class_">BCryptPasswordEncoder</span>().encode(<span class="hljs-string">&quot;123123&quot;</span>)).roles(<span class="hljs-string">&quot;vip1&quot;</span>);<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><h2 id="注销及权限控制">注销及权限控制</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//授权</span><br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">protected</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">configure</span><span class="hljs-params">(HttpSecurity http)</span> <span class="hljs-keyword">throws</span> Exception &#123;<br>        <span class="hljs-comment">//首页所有人都可以访问，功能页只有对应有权限的人才能访问</span><br>        <span class="hljs-comment">//第一行代表首页所有人都可以访问 第二行代表需要vip1才能进行访问</span><br>        http.authorizeRequests()<br>                .antMatchers(<span class="hljs-string">&quot;/&quot;</span>).permitAll()<br>                .antMatchers(<span class="hljs-string">&quot;/level1/**&quot;</span>).hasRole(<span class="hljs-string">&quot;vip1&quot;</span>)<br>                .antMatchers(<span class="hljs-string">&quot;/level2/**&quot;</span>).hasRole(<span class="hljs-string">&quot;vip2&quot;</span>)<br>                .antMatchers(<span class="hljs-string">&quot;/level3/**&quot;</span>).hasRole(<span class="hljs-string">&quot;vip3&quot;</span>);<br><br>        <span class="hljs-comment">//没有权限默认会回到登陆页面</span><br>        http.formLogin();<br><br>        <span class="hljs-comment">//注销，开启了注销功能，再跳到首页</span><br>        http.logout().logoutSuccessUrl(<span class="hljs-string">&quot;/&quot;</span>);<br>    &#125;<br></code></pre></div></td></tr></table></figure><h2 id="记住我及首页定制">记住我及首页定制</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//开启记住我功能:cookie，默认保存两周</span><br>        http.rememberMe();<br></code></pre></div></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//没有权限默认会回到登陆页面</span><br>        <span class="hljs-comment">//定制登录页到toLogin</span><br>        http.formLogin().loginPage(<span class="hljs-string">&quot;/toLogin&quot;</span>);<br></code></pre></div></td></tr></table></figure><h1 id="swagger">Swagger</h1><h2 id="集成swagger">集成swagger</h2><p>导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger2 --&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>io.springfox<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>springfox-swagger2<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.9.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-comment">&lt;!-- https://mvnrepository.com/artifact/io.springfox/springfox-swagger-ui --&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>io.springfox<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>springfox-swagger-ui<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>                <span class="hljs-tag">&lt;<span class="hljs-name">version</span>&gt;</span>2.9.2<span class="hljs-tag">&lt;/<span class="hljs-name">version</span>&gt;</span><br>            <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></div></td></tr></table></figure><p>编写hello的Controller</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@RestController</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">HelloController</span> &#123;<br><br>    <span class="hljs-meta">@RequestMapping(value = &quot;/hello&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">hello</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;hello&quot;</span>;<br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><p>配置Swagger</p><ul><li>新建config.SwaggerConfig.java</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Configuration</span><br><span class="hljs-meta">@EnableSwagger2</span>   <span class="hljs-comment">//开启Swagger2</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">SwaggerConfig</span> &#123;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><p>测试运行</p><ul><li>输入<code>localhost:8080/swagger-ui.html</code></li></ul><p><img src="/img/开发/SpringBoot/10.png" /></p><h2 id="配置swagger信息">配置Swagger信息</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//配置了Swagger的bean实例</span><br>    <span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2)<br>                .apiInfo(apiInfo());<br>    &#125;<br><br>    <span class="hljs-comment">//配置Swagger信息</span><br>    <span class="hljs-keyword">private</span> ApiInfo <span class="hljs-title function_">apiInfo</span><span class="hljs-params">()</span> &#123;<br><br>        <span class="hljs-comment">//作者信息</span><br>        <span class="hljs-type">Contact</span> <span class="hljs-variable">contact</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Contact</span>(<span class="hljs-string">&quot;肘子开&quot;</span>, <span class="hljs-string">&quot;https://www.baidu.com&quot;</span>, <span class="hljs-string">&quot;13600004906&quot;</span>);<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ApiInfo</span>(<span class="hljs-string">&quot;肘子开的SwaggerAPI文档&quot;</span>,<br>                <span class="hljs-string">&quot;Api Documentation&quot;</span>,<br>                <span class="hljs-string">&quot;1.0&quot;</span>,<br>                <span class="hljs-string">&quot;urn:tos&quot;</span>,<br>                contact,<br>                <span class="hljs-string">&quot;Apache 2.0&quot;</span>,<br>                <span class="hljs-string">&quot;http://www.apache.org/licenses/LICENSE-2.0&quot;</span>,<br>                <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>());<br>    &#125;<br></code></pre></div></td></tr></table></figure><p><img src="/img/开发/SpringBoot/11.png" /></p><h2 id="配置扫描接口及开关">配置扫描接口及开关</h2><ul><li>通过select()、apis()以及build()方法进行配置需要扫描的接口</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//配置了Swagger的bean实例</span><br>    <span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2)<br>                .apiInfo(apiInfo())<br>                .select()<br>                <span class="hljs-comment">//RequestHandlerSelectors配置要扫描结果的方式</span><br>                <span class="hljs-comment">//basePackage()指定要扫描的包</span><br>            <span class="hljs-comment">//any()扫描全部</span><br>                <span class="hljs-comment">//none()不扫描</span><br>                <span class="hljs-comment">//withClassAnnotation扫描类上的注解</span><br>                <span class="hljs-comment">//withMethodAnnotation扫描方法上的注解</span><br>                .apis(RequestHandlerSelectors.basePackage(<span class="hljs-string">&quot;com.example.swagger.controller&quot;</span>))<br>            <span class="hljs-comment">//path()过滤路径</span><br>                <span class="hljs-comment">//.paths(PathSelectors.ant(&quot;/zhou/**&quot;))</span><br>                .build();<br>    &#125;<br></code></pre></div></td></tr></table></figure><ul><li>通过enable()方法对swagger进行开关</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//配置了Swagger的bean实例</span><br>    <span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2)<br>                .apiInfo(apiInfo())<br>                <span class="hljs-comment">//如果为false则不启动</span><br>                .enable(<span class="hljs-literal">false</span>)<br>                .select()<br>                .apis(RequestHandlerSelectors.basePackage(<span class="hljs-string">&quot;com.example.swagger.controller&quot;</span>))<br>                .build();<br>    &#125;<br></code></pre></div></td></tr></table></figure><ul><li>如果我们想要在开发环境中使用，但是在上线环境中不使用</li><li>首先会新建两个配置文件，分别为application-dev.properties和application-prod.properties，分别为开发环境和上线环境中的配置</li><li>之后对环境进行读取，判断是否为开发环境，再传给enable()</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Bean</span><br>    <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket</span><span class="hljs-params">(Environment environment)</span> &#123;<br><br>        <span class="hljs-comment">//设置要显示的Swagger环境</span><br>        <span class="hljs-type">Profiles</span> <span class="hljs-variable">profiles</span> <span class="hljs-operator">=</span> Profiles.of(<span class="hljs-string">&quot;dev&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>);<br><br>        <span class="hljs-comment">//通过environment.acceptsProfiles判断是否在自己设定的环境当中</span><br>        <span class="hljs-type">boolean</span> <span class="hljs-variable">flag</span> <span class="hljs-operator">=</span> environment.acceptsProfiles(profiles);<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2)<br>                .apiInfo(apiInfo())<br>                .enable(flag)<br>                .select()<br>                .apis(RequestHandlerSelectors.basePackage(<span class="hljs-string">&quot;com.example.swagger.controller&quot;</span>))<br>                .build();<br>    &#125;<br></code></pre></div></td></tr></table></figure><ul><li>在application.properties中配置</li></ul><figure class="highlight properties"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs properties"><span class="hljs-attr">spring.profiles.active</span>=<span class="hljs-string">dev</span><br></code></pre></div></td></tr></table></figure><ul><li>在application-dev.properties中配置</li></ul><figure class="highlight properties"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs properties"><span class="hljs-attr">server.port</span>=<span class="hljs-string">8081</span><br></code></pre></div></td></tr></table></figure><ul><li>在application-prod.properties中配置</li></ul><figure class="highlight properties"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs properties"><span class="hljs-attr">server.port</span>=<span class="hljs-string">8082</span><br></code></pre></div></td></tr></table></figure><h2 id="配置api分组">配置api分组</h2><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Bean</span><br>   <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket1</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2).groupName(<span class="hljs-string">&quot;A&quot;</span>);<br>   &#125;<br><br>   <span class="hljs-meta">@Bean</span><br>   <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket2</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2).groupName(<span class="hljs-string">&quot;B&quot;</span>);<br>   &#125;<br><br>   <span class="hljs-meta">@Bean</span><br>   <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket3</span><span class="hljs-params">()</span> &#123;<br>       <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2).groupName(<span class="hljs-string">&quot;C&quot;</span>);<br>   &#125;<br><br><span class="hljs-meta">@Bean</span><br>   <span class="hljs-keyword">public</span> Docket <span class="hljs-title function_">docket</span><span class="hljs-params">(Environment environment)</span> &#123;<br><br>       <span class="hljs-type">Profiles</span> <span class="hljs-variable">profiles</span> <span class="hljs-operator">=</span> Profiles.of(<span class="hljs-string">&quot;dev&quot;</span>, <span class="hljs-string">&quot;test&quot;</span>);<br><br>       <span class="hljs-type">boolean</span> <span class="hljs-variable">flag</span> <span class="hljs-operator">=</span> environment.acceptsProfiles(profiles);<br><br>       <span class="hljs-keyword">return</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Docket</span>(DocumentationType.SWAGGER_2)<br>               .apiInfo(apiInfo())<br>               .enable(flag)<br>               .groupName(<span class="hljs-string">&quot;肘子开&quot;</span>)<br>               .select()<br>               .apis(RequestHandlerSelectors.basePackage(<span class="hljs-string">&quot;com.example.swagger.controller&quot;</span>))<br>               .build();<br>   &#125;<br></code></pre></div></td></tr></table></figure><ul><li>新建实体类，其中api注解相当于是注释，能够在swagger中看到</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@ApiModel(&quot;用户实体类&quot;)</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">User</span> &#123;<br>    <br>    <span class="hljs-meta">@ApiModelProperty(&quot;用户名&quot;)</span><br>    <span class="hljs-keyword">public</span> String username;<br>    <br>    <span class="hljs-meta">@ApiModelProperty(&quot;密码&quot;)</span><br>    <span class="hljs-keyword">public</span> String password;<br>&#125;<br></code></pre></div></td></tr></table></figure><ul><li>在控制类进行注释</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@RestController</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">HelloController</span> &#123;<br><br>    <span class="hljs-meta">@ApiOperation(&quot;Hello控制方法&quot;)</span><br>    <span class="hljs-meta">@GetMapping(&quot;/hello2&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">hello2</span><span class="hljs-params">(<span class="hljs-meta">@ApiParam(&quot;用户名&quot;)</span> String username)</span> &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;hello&quot;</span> + username;<br>    &#125;<br>    <br>&#125;<br></code></pre></div></td></tr></table></figure><h1 id="异步任务">异步任务</h1><p>首先编写一个等待3秒钟回复请求的需求</p><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Service</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">AsyncService</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">hello</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">try</span>&#123;<br>            Thread.sleep(<span class="hljs-number">3000</span>);<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            e.printStackTrace();<br>        &#125;<br><br>        System.out.println(<span class="hljs-string">&quot;数据正在处理。。。&quot;</span>);<br><br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@RestController</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">AsyncController</span> &#123;<br><br>    <span class="hljs-meta">@Autowired</span><br>    AsyncService asyncService;<br><br>    <span class="hljs-meta">@RequestMapping(&quot;/hello&quot;)</span><br>    <span class="hljs-keyword">public</span> String <span class="hljs-title function_">hello</span><span class="hljs-params">()</span> &#123;<br>        asyncService.hello();<br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;ok&quot;</span>;<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><p>这时候用户必须要等待3秒之后才可以收到回复的请求，这时候需要异步请求</p><ul><li>先在Service中添加<code>@Async</code>注解</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Service</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">AsyncService</span> &#123;<br><br>    <span class="hljs-comment">//告诉Spring这是一个异步的方法</span><br>    <span class="hljs-meta">@Async</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">hello</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-keyword">try</span>&#123;<br>            Thread.sleep(<span class="hljs-number">3000</span>);<br>        &#125; <span class="hljs-keyword">catch</span> (InterruptedException e) &#123;<br>            e.printStackTrace();<br>        &#125;<br><br>        System.out.println(<span class="hljs-string">&quot;数据正在处理。。。&quot;</span>);<br><br>    &#125;<br>&#125;<br></code></pre></div></td></tr></table></figure><ul><li>之后再main方法中开启方法</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//开启异步注解功能</span><br><span class="hljs-meta">@EnableAsync</span><br><span class="hljs-meta">@SpringBootApplication</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TestApplication</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        SpringApplication.run(TestApplication.class, args);<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><h1 id="邮件任务">邮件任务</h1><ul><li>添加依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">dependency</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">groupId</span>&gt;</span>org.springframework.boot<span class="hljs-tag">&lt;/<span class="hljs-name">groupId</span>&gt;</span><br>            <span class="hljs-tag">&lt;<span class="hljs-name">artifactId</span>&gt;</span>spring-boot-starter-mail<span class="hljs-tag">&lt;/<span class="hljs-name">artifactId</span>&gt;</span><br>        <span class="hljs-tag">&lt;/<span class="hljs-name">dependency</span>&gt;</span><br></code></pre></div></td></tr></table></figure><ul><li>需要再配置文件中添加配置</li></ul><figure class="highlight properties"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs properties"><span class="hljs-attr">spring.mail.username</span>=<span class="hljs-string">1735257086@qq.com</span><br><span class="hljs-attr">spring.mail.password</span>=<span class="hljs-string">dzzmxmfwltrpcbid</span><br><span class="hljs-attr">spring.mail.host</span>=<span class="hljs-string">smtp.qq.com</span><br><span class="hljs-comment"># 如果是qq邮箱，开启加密验证</span><br><span class="hljs-attr">spring.mail.properties.mail.smtp.ssl.enable</span>=<span class="hljs-string">true</span><br></code></pre></div></td></tr></table></figure><ul><li>编写请求</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@SpringBootTest</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">TestApplicationTests</span> &#123;<br><br>    <span class="hljs-meta">@Autowired</span><br>    JavaMailSenderImpl mailSender;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">void</span> <span class="hljs-title function_">contextLoads</span><span class="hljs-params">()</span> &#123;<br>        <span class="hljs-comment">//一个简单的邮件</span><br>        <span class="hljs-type">SimpleMailMessage</span> <span class="hljs-variable">mailMessage</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">SimpleMailMessage</span>();<br><br>        mailMessage.setSubject(<span class="hljs-string">&quot;肘子开你好&quot;</span>);<br>        mailMessage.setText(<span class="hljs-string">&quot;谢谢你肘子开,太喜欢你啦&quot;</span>);<br><br>        mailMessage.setTo(<span class="hljs-string">&quot;1735257086@qq.com&quot;</span>);<br>        mailMessage.setFrom(<span class="hljs-string">&quot;1735257086@qq.com&quot;</span>);<br><br>        mailSender.send(mailMessage);<br>    &#125;<br><br>    <span class="hljs-meta">@Test</span><br>    <span class="hljs-keyword">void</span> <span class="hljs-title function_">contextLoads2</span><span class="hljs-params">()</span> <span class="hljs-keyword">throws</span> MessagingException &#123;<br>        <span class="hljs-comment">//一个复杂的邮件</span><br>        <span class="hljs-type">MimeMessage</span> <span class="hljs-variable">mimeMessage</span> <span class="hljs-operator">=</span> mailSender.createMimeMessage();<br><br>        <span class="hljs-type">MimeMessageHelper</span> <span class="hljs-variable">helper</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">MimeMessageHelper</span>(mimeMessage, <span class="hljs-literal">true</span>);<br><br>        helper.setSubject(<span class="hljs-string">&quot;肘子开你好&quot;</span>);<br>        helper.setText(<span class="hljs-string">&quot;&lt;p style=&#x27;color:red&gt;邮件测试&lt;/p&gt;&quot;</span>, <span class="hljs-literal">true</span>);   <span class="hljs-comment">//true代表为html</span><br><br>        <span class="hljs-comment">//附件</span><br>        helper.addAttachment(<span class="hljs-string">&quot;1.jpg&quot;</span>, <span class="hljs-keyword">new</span> <span class="hljs-title class_">File</span>(<span class="hljs-string">&quot;D:\\1.jpg&quot;</span>));<br><br>        helper.setTo(<span class="hljs-string">&quot;1735257086@qq.com&quot;</span>);<br>        helper.setFrom(<span class="hljs-string">&quot;1735257086@qq.com&quot;</span>);<br><br>        mailSender.send(mimeMessage);<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><h1 id="定时任务">定时任务</h1><ul><li>在main方法中添加注释</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-comment">//开启异步注解功能</span><br><span class="hljs-meta">@EnableAsync</span><br><span class="hljs-comment">//开始定时功能注解</span><br><span class="hljs-meta">@EnableScheduling</span><br><span class="hljs-meta">@SpringBootApplication</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">TestApplication</span> &#123;<br><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>        SpringApplication.run(TestApplication.class, args);<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><ul><li>编写需求</li></ul><figure class="highlight java"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs java"><span class="hljs-meta">@Service</span><br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">ScheduledService</span> &#123;<br><br>    <span class="hljs-comment">//在特定的时间点执行这个方法 Timer</span><br>    <span class="hljs-comment">//Cron表达式</span><br>    <span class="hljs-comment">//秒 分 时 日 月 周几</span><br>    <span class="hljs-comment">/*</span><br><span class="hljs-comment">    * 30 15 10 * * ？    每天的10点15分30秒</span><br><span class="hljs-comment">    * 30 0/5 10,18 * * ?    每天10点和18点，每隔5分钟执行一次</span><br><span class="hljs-comment">    * */</span><br>    <span class="hljs-meta">@Scheduled(cron = &quot;0 * * * * 0-7&quot;)</span>  <span class="hljs-comment">//每天每分钟的第0秒运行</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">hello</span><span class="hljs-params">()</span> &#123;<br>        System.out.println(<span class="hljs-string">&quot;你被执行了&quot;</span>);<br>    &#125;<br><br>&#125;<br></code></pre></div></td></tr></table></figure><ul><li>cron表达式可以在百度查找生成器</li></ul><h1 id="整合redis">整合Redis</h1><ul><li>新建项目时需要在非关系型数据库(NoSQL)中勾选上Spring Data Redis(Access+Driver)</li></ul><h1 id="分布式理论">分布式理论</h1>]]></content>
    
    
    <categories>
      
      <category>开发</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第十三讲-基于上下文的表征与NLP预训练模型</title>
    <link href="/2022/06/15/%E7%AC%AC%E5%8D%81%E4%B8%89%E8%AE%B2-%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9A%84%E8%A1%A8%E5%BE%81%E4%B8%8ENLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/06/15/%E7%AC%AC%E5%8D%81%E4%B8%89%E8%AE%B2-%E5%9F%BA%E4%BA%8E%E4%B8%8A%E4%B8%8B%E6%96%87%E7%9A%84%E8%A1%A8%E5%BE%81%E4%B8%8ENLP%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="词向量知识回顾">1 词向量知识回顾</h1><h2 id="词向量表征">1.1 词向量表征</h2><ul><li>现在我们可以获得一个单词的表示<ul><li>我们开始时学过的单词向量<ul><li>Word2vec，GloVe，fastText</li></ul></li></ul></li></ul><h2 id="预训练的词向量">1.2 预训练的词向量</h2><ul><li>我们可以随机初始化词向量，并根据我们自己的下游任务训练它们</li><li>但在绝大多数情况下，使用预训练词向量是有帮助的，因为它们本身是自带信息的(我们可以在更大体量的预训练语料上训练得到它们)</li></ul><h2 id="未知词的词向量应用建议">1.3 未知词的词向量应用建议</h2><ul><li><p>简单且常见的解决方案：</p></li><li><p>训练时：词汇表<span class="math inline">\(\{\text { wordsoccurring, say }, \geq 5 \text { times }\}\cup\{&lt;\mathrm{UNK}&gt;\}\)</span></p><ul><li>将<strong>所有</strong>罕见的词 (数据集中出现次数小于 5)都映射为<spanclass="math inline">\(&lt;UNK&gt;\)</span>，为其训练一个词向量</li></ul></li><li><p><strong>运行时</strong>：使用<spanclass="math inline">\(&lt;UNK&gt;\)</span>代替词汇表之外的词OOV</p></li><li><p><strong>问题</strong>：</p><ul><li>没有办法区分不同 UNK words，无论是身份还是意义</li></ul></li></ul><p><strong>解决方案</strong></p><p>​ <strong>使用字符级模型学习词向量</strong></p><ul><li>特别是在 QA 中，match on word identity是很重要的，即使词向量词汇表以外的单词</li></ul><p>​ <strong>尝试这些建议</strong> (from Dhingra, Liu, Salakhutdinov,Cohen 2017)</p><ul><li><p>如果测试时的<spanclass="math inline">\(&lt;UNK&gt;\)</span>单词不在你的词汇表中，但是出现在你使用的无监督词嵌入中，测试时直接使用这个向量</p></li><li><p>此外，你可以将其视为新的单词，并为其分配一个随机向量，将它们添加到你的词汇表</p></li><li><p>帮助很大或者也许能帮点忙</p></li><li><p>你可以试试另一件事</p><ul><li>将它们分解为词类 (如未知号码，大写等等)，每种都对应一个 <spanclass="math inline">\(&lt;UNK-class&gt;\)</span></li></ul></li></ul><h2 id="单词的表示">1.4 单词的表示</h2><p><strong>存在两个大问题</strong></p><ul><li>对于一个 word type 总是是用相同的表示，不考虑这个 word token出现的上下文<ul><li>我们可以进行非常细粒度的词义消歧</li></ul></li><li>我们对一个词只有一种表示，但是单词有不同的方面，包括语义，句法行为，以及表达/ 含义</li></ul><h2 id="我们一直都有解决这个问题的办法吗">1.5我们一直都有解决这个问题的办法吗？</h2><ul><li>在NLM中，我们直接将单词向量 (可能只在语料库上训练) 插入LSTM层</li><li>那些LSTM层被训练来预测下一个单词</li><li>但这些语言模型在每一个位置生成特定于上下文的词表示</li></ul><h2 id="论文解读-peters-et-al.-2017-taglm-pre-elmo">1.6 论文解读 Peterset al. (2017): TagLM – “Pre-ELMo”</h2><ul><li><strong>想法</strong>：想要获得单词在上下文的意思，但标准的 RNN学习任务只在 task-labeled 的小数据上 (如 NER )</li><li>为什么不通过半监督学习的方式在大型无标签数据集上训练NLM，而不只是词向量</li></ul><h2 id="标签语言模型-tag-lm">1.7 标签语言模型 (Tag LM )</h2><ul><li><strong>步骤3</strong>：在序列标记模型中同时使用单词嵌入和 LM嵌入</li><li><strong>步骤2</strong>：为输入序列中的每个标记准备单词嵌入和 LM嵌入</li><li><strong>步骤1</strong>：预训练词嵌入和语言模型</li><li>与上文无关的单词嵌入 + RNN model 得到的 hidden states作为特征输入</li></ul><p><img src="/img/nlp/第十三讲/1.png" /></p><ul><li>Char CNN / RNN + Token Embedding 作为 bi-LSTM 的输入</li><li>得到的 hidden states 与 Pre-trained bi-LM (冻结的) 的 hidden states连接起来输入到第二层的 bi-LSTM 中</li></ul><p><img src="/img/nlp/第十三讲/2.png" /></p><h2 id="命名实体识别-ner">1.8 命名实体识别 (NER)</h2><ul><li>一个非常重要的NLP子任务：<strong>查找</strong>和<strong>分类</strong>文本中的实体</li></ul><h2 id="论文解读-peters-et-al.2017-taglm-pre-elmo">1.9 论文解读 Peterset al.(2017): TagLM-"Pre-ELMo"</h2><ul><li>语言模型在 <code>Billion word benchmark</code>的8亿个训练单词上训练</li></ul><p><strong>语言模型观察结果</strong></p><ul><li>在监督数据集上训练的语言模型并不会受益</li><li>双向语言模型仅有助于 forward 过程，提升约 0.2</li><li>具有巨大的语言模型设计 (困惑度 30) 比较小的模型 (困惑度 48) 提升约0.3</li></ul><p><strong>任务特定的BiLSTM观察结果</strong></p><ul><li>仅使用LM嵌入来预测并不是很好：88.17 F1<ul><li>远低于仅在标记数据上使用 BiLSTM 标记器</li></ul></li></ul><h2 id="论文解读-also-in-the-air-mccann-et-al.2017cove">1.10 论文解读Also in the air: McCann et al.2017:CoVe</h2><ul><li><p>也有一种思路：使用训练好的序列模型，为其他NLP模型提供上下文</p></li><li><p><strong>思路</strong>：机器翻译是为了保存意思，所以这也许是个好目标？</p></li><li><p>使用 seq2seq + attention NMT system 中的 Encoder，即 2 层bi-LSTM，作为上下文提供者</p></li><li><p>所得到的 CoVe 向量在各种任务上都优于 GloVe 向量</p></li><li><p>但是，结果并不像其他幻灯片中描述的更简单的 NLM训练那么好，所以似乎被放弃了</p><ul><li>也许NMT只是比语言建模更难？</li><li>或许有一天这个想法会回来？</li></ul></li></ul><h1 id="elmo模型">2.ELMo模型</h1><h2id="论文解读-peters-et-al.-2018-elmo-embeddings-from-language-models">2.1论文解读 Peters et al. (2018): ELMo: Embeddings from LanguageModels</h2><ul><li><p>单词标记向量或上下文词向量的突破</p></li><li><p>使用长上下文而不是上下文窗口学习词标记向量(这里，整个句子可能更长)</p></li><li><p>学习深度 Bi-NLM，并在预测中使用它的所有层</p></li><li><p>训练一个双向语言模型 (LM)</p></li><li><p>目标是效果 OK 但不要太大的语言模型 (LM)</p><ul><li>使用 2 个 biLSTM 层</li><li>(仅) 使用字符CNN构建初始单词表示<ul><li>2048 个 char n-gram filters 和 2 个 highway layers，512 维的projection</li></ul></li><li>4096 dim hidden/cell LSTM状态，使用 512 dim的对下一个输入的投影</li><li>使用残差连接</li><li>绑定 token 的输入和输出的参数(softmax)，并将这些参数绑定到正向和反向语言模型 (LM) 之间</li></ul></li><li><p>ELMo 学习 biLM 表示的特定任务组合</p></li><li><p>这是一个创新，TagLM 中仅仅使用堆叠 LSTM 的顶层，ELMo 认为 BiLSTM所有层都是有用的</p></li></ul><p><span class="math display">\[\begin{aligned}R_{k} &amp;=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k,j}^{L M}, \overleftarrow{\mathbf{h}}_{k, j}^{L M} \mid j=1, \ldots,L\right\} \\&amp;=\left\{\mathbf{h}_{k, j}^{L M} \mid j=0, \ldots, L\right\}\end{aligned}\]</span></p><p><span class="math display">\[\mathbf{E L M o}_{k}^{\text {task }}=E\left(R_{k} ; \Theta^{\text {task}}\right)=\gamma^{\text {task }} \sum_{j=0}^{L} s_{j}^{\text {task }}\mathbf{h}_{k, j}^{L M}\]</span></p><ul><li><p><span class="math inline">\(\gamma^{t a s k}\)</span>衡量 ELMo对任务的总体有用性，是为特定任务学习的全局比例因子</p></li><li><p><span class="math inline">\(s^{task}\)</span>是 softmax归一化的混合模型权重，是 BiLSTM的加权平均值的权重，对不同的任务是不同的，因为不同的任务对不同层的BiLSTM 的</p></li><li><p>首先运行 biLM 获取每个单词的表示</p></li><li><p>然后，让 (无论什么) 最终任务模型使用它们</p><ul><li>冻结 ELMo 的权重，用于监督模型</li><li>将 ELMo 权重连接到特定于任务的模型中<ul><li>细节取决于任务<ul><li>像 TagLM 一样连接到中间层是典型的</li><li>可以在生产输出时提供更多的表示，例如在问答系统中</li></ul></li></ul></li></ul></li></ul><h2 id="elmo在序列标记器中的使用">2.2 ELMo在序列标记器中的使用</h2><p><img src="/img/nlp/第十三讲/3.png" /></p><h2 id="elmo-层权重">2.3 ELMo ：层权重</h2><ul><li>这两个 biLSTM NLM 层有不同的用途 / 含义<ul><li>低层更适合低级语法，例如<ul><li>词性标注(part-of-speech tagging)、句法依赖(syntacticdependency)、NER</li></ul></li><li>高层更适合更高级别的语义<ul><li>情绪、语义角色标记、问答系统、SNLI</li></ul></li></ul></li><li>这似乎很有趣，但它是如何通过两层以上的网络来实现的看起来更有趣</li></ul><h1 id="ulmfit模型">3.ULMfit模型</h1><h2 id="ulmfit">3.1 ULMfit</h2><ul><li><p>转移 NLM 知识的一般思路是一样的</p></li><li><p>这里应用于文本分类</p><p><img src="/img/nlp/第十三讲/4.png" /></p></li><li><p>在大型通用领域的无监督语料库上使用 biLM 训练</p><ul><li><p>在目标任务数据上调整 LM</p></li><li><p>对特定任务将分类器进行微调</p><p><img src="/img/nlp/第十三讲/5.png" /></p></li></ul></li><li><p>使用合理大小的 <code>1 GPU</code>语言模型，并不是真的很大</p></li><li><p>在LM调优中要注意很多</p><ul><li>不同的每层学习速度</li><li>倾斜三角形学习率 (STLR) 计划</li></ul></li><li><p>学习分类器时逐步分层解冻和STLR</p></li><li><p>使用<span class="math inline">\(\left[h_{T},\operatorname{maxpool}(\mathbf{h}), \text { meanpool}(\mathbf{h})\right]\)</span>进行分类</p></li><li><p>使用大型的预训练语言模型，是一种提高性能的非常有效的方法</p></li></ul><h1 id="transformer结构">4.Transformer结构</h1><h2 id="transformer介绍">4.1 Transformer介绍</h2><p><img src="/img/nlp/第十三讲/6.png" /></p><ul><li>所有这些模型都是以Transformer为主结构的，我们应该学习一下Transformer吧</li></ul><p><strong>补充说明</strong></p><ul><li>Transformer 不仅很强大，而且允许扩展到更大的尺寸</li></ul><h2 id="transformers-动机">4.2 Transformers 动机</h2><ul><li><p>我们想要并行化，但是RNNs本质上是顺序的</p></li><li><p>尽管有 GRUs 和 LSTMs，RNNs仍然需要注意机制来处理长期依赖关系——否则状态之间的 path length<strong>路径长度</strong> 会随着序列增长</p></li><li><p>但如果注意力让我们进入任何一个状态……也许我们可以只用注意力而不需要RNN?</p></li></ul><h2 id="transformer-概览">4.3 Transformer 概览</h2><ul><li>序列到序列编码解码模型，但它是非循环非串行结构</li><li><strong>任务</strong>：平行语料库的机器翻译</li><li>预测每个翻译单词</li><li>最终成本/误差函数是 softmax 分类器基础上的标准交叉熵误差</li></ul><p><img src="/img/nlp/第十三讲/7.png" /></p><h2 id="transformer-基础">4.4 Transformer 基础</h2><ul><li>自学 transformer<ul><li>主要推荐资源<ul><li>http://nlp.seas.harvard.edu/2018/04/03/attention.html</li><li>The Annotated Transformer by Sasha Rush</li></ul></li><li>一个使用PyTorch的Jupyter笔记本，解释了一切！</li></ul></li><li>现在：我们定义 Transformer 网络的基本构建块：第一，新的注意力层</li></ul><h2 id="点乘注意力-dot-product-attention">4.5 点乘注意力 Dot-ProductAttention</h2><ul><li><p><strong>输入</strong>：对于一个输出而言的查询<spanclass="math inline">\(q\)</span>和一组键-值对<spanclass="math inline">\((k-v)\)</span></p></li><li><p>Query，keys，values，and output 都是向量</p></li><li><p>输出值的加权和</p></li><li><p>权重的每个值是由查询和相关键的内积计算结果</p></li><li><p>Query 和 keys 有相同维数<spanclass="math inline">\(d_k\)</span>，value 的维数为<spanclass="math inline">\(d_v\)</span></p></li></ul><p><span class="math display">\[A(q, K, V)=\sum_{i} \frac{e^{q \cdot k_{i}}}{\sum_{j} e^{q \cdot k_{j}}}v_{i}\]</span></p><h2 id="点乘注意力矩阵表示法">4.6 点乘注意力矩阵表示法</h2><ul><li>当我们有多个查询<spanclass="math inline">\(q\)</span>时，我们将它们叠加在一个矩阵<spanclass="math inline">\(Q\)</span>中</li><li>变成<span class="math inline">\(A(Q, K,V)=\operatorname{softmax}\left(Q K^{T}\right) V\)</span></li></ul><h2 id="缩放点乘注意力">4.7 缩放点乘注意力</h2><ul><li><p><strong>问题</strong>：<spanclass="math inline">\(d_k\)</span>变大时，<spanclass="math inline">\(q^Tk\)</span>的方差增大 → 一些 softmax中的值的方差将会变大 → softmax 得到的是峰值 → 因此梯度变小了</p></li><li><p><strong>解决方案</strong>：通过 query / key向量的长度进行缩放</p></li></ul><p><span class="math display">\[A(Q, K, V)=\operatorname{softmax}\left(\frac{QK^{T}}{\sqrt{d_{k}}}\right) V\]</span></p><p><img src="/img/nlp/第十三讲/11.png" /></p><p><img src="/img/nlp/第十三讲/9.png" /></p><p><img src="/img/nlp/第十三讲/8.png" /></p><p><img src="/img/nlp/第十三讲/10.png" /></p><h2 id="编码器中的自注意力">4.8 编码器中的自注意力</h2><ul><li><p>输入单词向量是 queries，keys and values</p></li><li><p>换句话说：<strong>这个词向量自己选择彼此</strong></p></li><li><p>词向量堆栈= Q = K = V</p></li><li><p>我们会通过解码器明白为什么我们在定义中将他们分开</p></li></ul><h2 id="多头注意力">4.9 多头注意力</h2><ul><li>简单 self-attention 的问题<ul><li>单词只有一种相互交互的方式</li></ul></li><li><strong>解决方案</strong>：<strong>多头注意力</strong></li><li>首先，通过矩阵<span class="math inline">\(W\)</span>将<spanclass="math inline">\(Q\)</span>，<spanclass="math inline">\(K\)</span>，<spanclass="math inline">\(V\)</span>映射到<spanclass="math inline">\(h=8\)</span>的许多低维空间</li><li>然后，应用注意力，然后连接输出，通过线性层</li></ul><p><span class="math display">\[\operatorname{MultiHead}(\boldsymbol{Q}, \boldsymbol{K},\boldsymbol{V})=\operatorname{Concat}\left(\right.  head  _{1}, \ldots ,head  \left._{h}\right)  where \ head\  hention  _{i}=  Attention (\left.Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}\right)\]</span></p><p><img src="/img/nlp/第十三讲/12.png" /></p><p><img src="/img/nlp/第十三讲/13.png" /></p><h2 id="完整的transformer模块">4.10 完整的transformer模块</h2><ul><li><p>每个 Block 都有两个子层</p><ul><li><p>多头 attention</p></li><li><p>两层的前馈神经网络，使用 ReLU</p></li></ul></li><li><p>这两个子层都</p><ul><li>残差连接以及层归一化</li><li>LayerNorm(x+Sublayer(x))</li><li>层归一化将输入转化为均值是0，方差是1，每一层和每一个训练点(并且添加了两个参数)</li></ul></li></ul><p><span class="math display">\[\mu^{l}=\frac{1}{H} \sum_{i=1}^{H} a_{i}^{l} \quad\sigma^{l}=\sqrt{\frac{1}{H}\sum_{i=1}^{H}\left(a_{i}^{l}-\mu^{l}\right)^{2}} \quadh_{i}=f\left(\frac{g_{i}}{\sigma_{i}}\left(a_{i}-\mu_{i}\right)+b_{i}\right)\]</span></p><h2 id="编码器输入">4.11 编码器输入</h2><ul><li><p>实际的词表示是 byte-pair 编码</p></li><li><p>还添加了一个 positional encoding位置编码，相同的词语在不同的位置有不同的整体表征</p></li></ul><p><span class="math display">\[\left\{\begin{array}{l}P E(p o s, 2 i)=\sin \left(p o s / 10000^{2 i / d_{\text {model}}}\right) \\P E(\operatorname{pos}, 2 i+1)=\cos \left(p o s / 10000^{2 i / d_{\text{model }}}\right)\end{array}\right.\]</span></p><p><img src="/img/nlp/第十三讲/14.png" /></p><h2 id="完整编码器encoder">4.12 完整编码器Encoder</h2><ul><li><p>encoder 中，每个 Block 都是来自前一层的<spanclass="math inline">\(Q,K,V\)</span></p></li><li><p>Blocks 被重复 6 次 (垂直方向)</p></li><li><p>在每个阶段，你可以通过多头注意力看到句子中的各个地方，累积信息并将其推送到下一层。在任一方向上的序列逐步推送信息来计算感兴趣的值</p></li><li><p>非常善于学习语言结构</p></li></ul><p><img src="/img/nlp/第十三讲/15.png" /></p><h2 id="注意力可视化">4.13 注意力可视化</h2><p><img src="/img/nlp/第十三讲/16.png" /></p><h2 id="transformer解码器">4.14 Transformer解码器</h2><ul><li><p>decoder 中有两个稍加改变的子层</p></li><li><p>对之前生成的输出进行 Masked decoder self-attention</p></li><li><p>Encoder-Decoder Attention，queries 来自于前一个 decoder 层，keys和 values 来自于 encoder 的输出</p></li><li><p>Blocks 同样重复 6 次</p></li></ul><p><img src="/img/nlp/第十三讲/17.png" /></p><h2 id="transformer的技巧与建议">4.15 Transformer的技巧与建议</h2><p><strong>细节</strong>(论文/之后的讲座)</p><ul><li><p>Byte-pair encodings</p></li><li><p>Checkpoint averaging</p></li><li><p>Adam 优化器控制学习速率变化</p></li><li><p>训练时，在每一层添加残差之前进行 Dropout</p></li><li><p>标签平滑</p></li><li><p>带有束搜索和长度惩罚的自回归解码</p></li><li><p>因为 transformer正在蔓延，但他们很难优化并且不像LSTMs那样开箱即用，他们还不能很好与其他任务的构件共同工作</p></li></ul><h1 id="bert模型">5.BERT模型</h1><h2 id="论文解读-bert-devlin-chang-lee-toutanova-2018">5.1 论文解读BERT: Devlin, Chang, Lee, Toutanova (2018)</h2><ul><li><p>BERT：用于语言理解的预训练深度双向 transformers</p></li><li><p>问题：语言模型只使用左上下文或右上下文，但语言理解是双向的</p></li><li><p>为什么LMs是单向的？</p><ul><li><strong>原因1</strong>：方向性对于生成格式良好的概率分布是有必要的[我们不在乎这个]</li><li><strong>原因2</strong>：双向编码器中单词可以<code>看到自己</code></li></ul></li></ul><p><img src="/img/nlp/第十三讲/18.png" /></p><ul><li><p>解决方案：掩盖k%的输入单词，然后预测 masked words</p></li><li><p>不再是传统的计算生成句子的概率的语言模型，目标是填空</p><ul><li>总是使用k=15%</li></ul></li><li><p>Masking 太少：训练太昂贵</p></li><li><p>Masking 太多：没有足够的上下文</p></li><li><p><strong>GPT</strong> 是经典的单项的语言模型</p></li><li><p><strong>ELMo</strong>是双向的，但是两个模型是完全独立训练的，只是将输出连接在一起，并没有使用双向的context</p></li><li><p><strong>BERT</strong> 使用 mask的方式进行整个上下文的预测，使用了双向的上下文信息</p></li></ul><p><img src="/img/nlp/第十三讲/19.png" /></p><h2 id="bert-模型微调">5.2 BERT 模型微调</h2><ul><li>只学习一个建立在顶层的分类器，微调的每个任务</li></ul><p><img src="/img/nlp/第十三讲/120.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第十一讲-NLP中的卷积神经网络</title>
    <link href="/2022/06/15/%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AE%B2-NLP%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2022/06/15/%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AE%B2-NLP%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>第十讲-NLP中的问答系统</title>
    <link href="/2022/06/06/%E7%AC%AC%E5%8D%81%E8%AE%B2-NLP%E4%B8%AD%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/"/>
    <url>/2022/06/06/%E7%AC%AC%E5%8D%81%E8%AE%B2-NLP%E4%B8%AD%E7%9A%84%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h1 id="squad问答数据集">1、SQuAD问答数据集</h1><h2 id="斯坦福问答数据集-squad">1.1 斯坦福问答数据集 (SQuAD)</h2><ul><li><p>Passage是来自维基百科的一段文本，系统需要回答问题，在文章中找出答案</p></li><li><p><span class="math inline">\(1000k\)</span>个样本</p></li><li><p>答案必须是文章中的一系列单词序列</p></li><li><p>也就是提取式问答</p></li></ul><h2 id="squad-评估v1.1">1.2 SQuAD 评估，v1.1</h2><p>作者收集了3个参考答案</p><p>系统在两个指标上计算得分 -<strong>精确匹配</strong>：1/0的准确度，你是否匹配三个答案中的一个 -F1：将系统和每个答案都视为词袋，并评估</p><p><span class="math display">\[\text { Precision }=\frac{T P}{T P+F P}\]</span></p><p><span class="math display">\[\text { Recall }=\frac{T P}{T P+F N}\]</span></p><p><span class="math display">\[\text { harmonic mean } \mathrm{F} 1=\frac{2 P R}{P+R}\]</span></p><ul><li>分数是 (宏观) 平均每题 F1分数</li></ul><p>F1测量被视为更可靠的指标，作为主要指标使用</p><ul><li>它不是基于选择是否和人类选择的跨度完全相同，人类选择的跨度容易受到各种影响，包括换行</li><li>在单次级别匹配不同的答案</li></ul><p>这两个指标忽视标点符号和冠词 (a, an, the only)</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>新闻文本分类</title>
    <link href="/2022/05/20/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    <url>/2022/05/20/%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
    
    <content type="html"><![CDATA[<h1 id="解题思路">解题思路</h1><p>赛题思路分析：赛题本质是一个文本分类问题，需要根据每句的字符进行分类。但赛题给出的数据是匿名化的，不能直接使用中文分词等操作，这个是赛题的难点。</p><p>因此本次赛题的难点是需要对匿名字符进行建模，进而完成文本分类的过程。由于文本数据是一种典型的非结构化数据，因此可能涉及到<code>特征提取</code>和<code>分类模型</code>两个部分。为了减低参赛难度，我们提供了一些解题思路供大家参考：</p><ul><li>思路1：TF-IDF + 机器学习分类器</li></ul><p>直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。</p><ul><li>思路2：FastText</li></ul><p>FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。</p><ul><li>思路3：WordVec + 深度学习分类器</li></ul><p>WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRNN或者BiLSTM。</p><ul><li>思路4：Bert词向量</li></ul><p>Bert是高配款的词向量，具有强大的建模学习能力。</p><h2 id="数据下载">数据下载</h2><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">! mkdir ./data<br></code></pre></div></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">train data</span><br>! wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/train_set.csv.zip<br>! unzip train_set.csv.zip -d ./data<br>! rm train_set.csv.zip<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-built_in">test</span> data</span><br>! wget https://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531810/test_a.csv.zip<br>! unzip test_a.csv.zip -d ./data<br>! rm test_a.csv.zip<br></code></pre></div></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">2.预训练下载</span><br>! wget http://tianchi-media.oss-cn-beijing.aliyuncs.com/dragonball/NLP/emb.zip<br>! unzip emb.zip<br>! rm emb.zip<br>! mv ./emb/bert-mini/bert_config.json ./emb/bert-mini/config.json <br></code></pre></div></td></tr></table></figure><h2 id="新建保存目录">新建保存目录</h2><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">! mkdir ./save<br></code></pre></div></td></tr></table></figure><h2 id="安装必要包">安装必要包</h2><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">! pip install fasttext transformers==2.9.0 gensim torch==1.3.0<br></code></pre></div></td></tr></table></figure><h2 id="数据读取">数据读取</h2><p>赛题数据虽然是文本数据，每个新闻是不定长的，但任然使用csv格式进行存储。因此可以直接用<code>Pandas</code>完成数据读取的操作。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br>train_df = pd.read_csv(<span class="hljs-string">&#x27;./data/train_set.csv&#x27;</span>, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, nrows=<span class="hljs-number">100</span>)<br></code></pre></div></td></tr></table></figure><p>这里的<code>read_csv</code>由三部分构成：</p><ul><li>读取的文件路径，这里需要根据改成你本地的路径，可以使用相对路径或绝对路径；</li><li>分隔符<code>sep</code>，为每列分割的字符，设置为<code>\t</code>即可；</li><li>读取行数<code>nrows</code>，为此次读取文件的函数，是数值类型（由于数据集比较大，建议先设置为100）；</li></ul><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">train_df.head()<br></code></pre></div></td></tr></table></figure><table><thead><tr class="header"><th style="text-align: left;">label</th><th style="text-align: left;">text</th><th></th></tr></thead><tbody><tr class="odd"><td style="text-align: left;">0</td><td style="text-align: left;">2</td><td>2967 6758 339 2021 1854 3731 4109 3792 4149 15...</td></tr><tr class="even"><td style="text-align: left;">1</td><td style="text-align: left;">11</td><td>4464 486 6352 5619 2465 4802 1452 3137 5778 54...</td></tr><tr class="odd"><td style="text-align: left;">2</td><td style="text-align: left;">3</td><td>7346 4068 5074 3747 5681 6093 1777 2226 7354 6...</td></tr><tr class="even"><td style="text-align: left;">3</td><td style="text-align: left;">2</td><td>7159 948 4866 2109 5520 2490 211 3956 5520 549...</td></tr><tr class="odd"><td style="text-align: left;">4</td><td style="text-align: left;">3</td><td>3646 3055 3055 2490 4659 6065 3370 5814 2465 5...</td></tr></tbody></table><p>上图是读取好的数据，是表格的形式。第一列为新闻的类别，第二列为新闻的字符。</p><h1 id="数据分析">数据分析</h1><p>在读取完成数据集后，我们还可以对数据集进行数据分析的操作。虽然对于非结构数据并不需要做很多的数据分析，但通过数据分析还是可以找出一些规律的。</p><p>此步骤我们读取了所有的训练集数据，在此我们通过数据分析希望得出以下结论：</p><ul><li>赛题数据中，新闻文本的长度是多少？</li><li>赛题数据的类别分布是怎么样的，哪些类别比较多？</li><li>赛题数据中，字符分布是怎么样的？</li></ul><h2 id="句子长度分析">句子长度分析</h2><p>在赛题数据中每行句子的字符使用空格进行隔开，所以可以直接统计单词的个数来得到每个句子的长度。统计并如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">%pylab inline<br>train_df[<span class="hljs-string">&#x27;text_len&#x27;</span>] = train_df[<span class="hljs-string">&#x27;text&#x27;</span>].apply(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x.split(<span class="hljs-string">&#x27; &#x27;</span>)))<br><span class="hljs-built_in">print</span>(train_df[<span class="hljs-string">&#x27;text_len&#x27;</span>].describe())<br></code></pre></div></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">opulating the interactive namespace from numpy and matplotlib<br>count     100.000000<br>mean      872.320000<br>std       923.138191<br>min        64.000000<br><span class="hljs-meta prompt_">25% </span><span class="language-bash">      359.500000</span><br><span class="hljs-meta prompt_">50% </span><span class="language-bash">      598.000000</span><br><span class="hljs-meta prompt_">75% </span><span class="language-bash">     1058.000000</span><br>max      7125.000000<br>Name: text_len, dtype: float64<br></code></pre></div></td></tr></table></figure><p>对新闻句子的统计可以得出，本次赛题给定的文本比较长，每个句子平均由907个字符构成，最短的句子长度为2，最长的句子长度为57921。</p><h2 id="新闻类别分布">新闻类别分布</h2><p>接下来可以对数据集的类别进行分布统计，具体统计每类新闻的样本个数。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">train_df[<span class="hljs-string">&#x27;label&#x27;</span>].value_counts().plot(kind=<span class="hljs-string">&#x27;bar&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;News class count&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&quot;category&quot;</span>)<br></code></pre></div></td></tr></table></figure><figure class="highlight stylus"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">Text</span><span class="hljs-params">(<span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;category&#x27;</span>)</span></span><br></code></pre></div></td></tr></table></figure><p><img src="/img/nlp/新闻文本分类/1.png" /></p><p>在数据集中标签的对应的关系如下：{'科技': 0, '股票': 1, '体育': 2,'娱乐': 3, '时政': 4, '社会': 5, '教育': 6, '财经': 7, '家居': 8,'游戏': 9, '房产': 10, '时尚': 11, '彩票': 12, '星座': 13}</p><p>从统计结果可以看出，赛题的数据集类别分布存在较为不均匀的情况。在训练集中科技类新闻最多，其次是股票类新闻，最少的新闻是星座新闻。</p><h2 id="字符分布统计">字符分布统计</h2><p>接下来可以统计每个字符出现的次数，首先可以将训练集中所有的句子进行拼接进而划分为字符，并统计每个字符的个数。</p><p>从统计结果中可以看出，在训练集中总共包括6869个字，其中编号3750的字出现的次数最多，编号5034的字出现的次数最少。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br>all_lines = <span class="hljs-string">&#x27; &#x27;</span>.join(<span class="hljs-built_in">list</span>(train_df[<span class="hljs-string">&#x27;text&#x27;</span>]))<br>word_count = Counter(all_lines.split(<span class="hljs-string">&quot; &quot;</span>))<br>word_count = <span class="hljs-built_in">sorted</span>(word_count.items(), key=<span class="hljs-keyword">lambda</span> d:d[<span class="hljs-number">1</span>], reverse = <span class="hljs-literal">True</span>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">len</span>(word_count))<br><br><span class="hljs-built_in">print</span>(word_count[<span class="hljs-number">0</span>])<br><br><span class="hljs-built_in">print</span>(word_count[-<span class="hljs-number">1</span>])<br></code></pre></div></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">2405<br>(&#x27;3750&#x27;, 3702)<br>(&#x27;5034&#x27;, 1)<br></code></pre></div></td></tr></table></figure><h2 id="数据分析的结论">数据分析的结论</h2><p>通过上述分析我们可以得出以下结论：</p><ol type="1"><li>赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长；</li><li>赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k；</li><li>赛题总共包括7000-8000个字符；</li></ol><p>通过数据分析，我们还可以得出以下结论：</p><ol type="1"><li>每个新闻平均字符个数较多，可能需要截断；</li><li>由于类别不均衡，会严重影响模型的精度；</li></ol><h1 id="基于机器学习的文本分类">基于机器学习的文本分类</h1><h2 id="学习目标"><strong>学习目标</strong></h2><ul><li>学会TF-IDF的原理和使用</li><li>使用sklearn的机器学习模型完成文本分类</li></ul><h2 id="文本表示方法">文本表示方法</h2><h3 id="one-hot">One-hot</h3><p>这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。</p><p>One-hot表示方法的例子如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">句子<span class="hljs-number">1</span>：我 爱 北 京 天 安 门<br>句子<span class="hljs-number">2</span>：我 喜 欢 上 海<br></code></pre></div></td></tr></table></figure><p>首先对所有句子的字进行索引，即将每个字确定一个编号：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">&#123;<br>    <span class="hljs-string">&#x27;我&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;爱&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;北&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;京&#x27;</span>: <span class="hljs-number">4</span>, <span class="hljs-string">&#x27;天&#x27;</span>: <span class="hljs-number">5</span>,<br>  <span class="hljs-string">&#x27;安&#x27;</span>: <span class="hljs-number">6</span>, <span class="hljs-string">&#x27;门&#x27;</span>: <span class="hljs-number">7</span>, <span class="hljs-string">&#x27;喜&#x27;</span>: <span class="hljs-number">8</span>, <span class="hljs-string">&#x27;欢&#x27;</span>: <span class="hljs-number">9</span>, <span class="hljs-string">&#x27;上&#x27;</span>: <span class="hljs-number">10</span>, <span class="hljs-string">&#x27;海&#x27;</span>: <span class="hljs-number">11</span><br>&#125;<br></code></pre></div></td></tr></table></figure><p>在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量：</p><figure class="highlight prolog"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs prolog">我：[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br>爱：[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br>...<br>海：[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]<br></code></pre></div></td></tr></table></figure><h3 id="bag-of-words">Bag of Words</h3><p>Bag of Words（词袋表示），也称为CountVectors，每个文档的字/词可以使用其出现次数来进行表示。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">句子<span class="hljs-number">1</span>：我 爱 北 京 天 安 门<br>句子<span class="hljs-number">2</span>：我 喜 欢 上 海<br></code></pre></div></td></tr></table></figure><p>直接统计每个字出现的次数，并进行赋值：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">句子<span class="hljs-number">1</span>：我 爱 北 京 天 安 门<br>转换为 [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]<br><br>句子<span class="hljs-number">2</span>：我 喜 欢 上 海<br>转换为 [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]<br></code></pre></div></td></tr></table></figure><p>在sklearn中可以直接<code>CountVectorizer</code>来实现这一步骤：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br>corpus = [<br>    <span class="hljs-string">&#x27;This is the first document.&#x27;</span>,<br>    <span class="hljs-string">&#x27;This document is the second document.&#x27;</span>,<br>    <span class="hljs-string">&#x27;And this is the third one.&#x27;</span>,<br>    <span class="hljs-string">&#x27;Is this the first document?&#x27;</span>,<br>]<br>vectorizer = CountVectorizer()<br>vectorizer.fit_transform(corpus).toarray()<br></code></pre></div></td></tr></table></figure><h3 id="n-gram">N-gram</h3><p>N-gram与CountVectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。</p><p>如果N取值为2，则句子1和句子2就变为：</p><figure class="highlight"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs">句子1：我爱 爱北 北京 京天 天安 安门<br>句子2：我喜 喜欢 欢上 上海<br></code></pre></div></td></tr></table></figure><h3 id="tf-idf">TF-IDF</h3><p>TF-IDF 分数由两部分组成：第一部分是<strong>词语频率</strong>（TermFrequency），第二部分是<strong>逆文档频率</strong>（Inverse DocumentFrequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。</p><figure class="highlight stylus"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">TF</span><span class="hljs-params">(t)</span></span>= 该词语在当前文档出现的次数 / 当前文档中词语的总数<br><span class="hljs-function"><span class="hljs-title">IDF</span><span class="hljs-params">(t)</span></span>= log_e（文档总数 / 出现该词语的文档总数）<br></code></pre></div></td></tr></table></figure><h2 id="基于机器学习的文本分类-1">基于机器学习的文本分类</h2><p>接下来我们将对比不同文本表示算法的精度，通过本地构建验证集计算F1得分。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Count Vectors + RidgeClassifier</span><br><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><span class="hljs-keyword">from</span> sklearn.feature_extraction.text <span class="hljs-keyword">import</span> CountVectorizer<br><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> RidgeClassifier<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score<br><br>train_df = pd.read_csv(<span class="hljs-string">&#x27;./data/train_set.csv&#x27;</span>, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, nrows=<span class="hljs-number">15000</span>)<br><br>vectorizer = CountVectorizer(max_features=<span class="hljs-number">3000</span>)<br>train_test = vectorizer.fit_transform(train_df[<span class="hljs-string">&#x27;text&#x27;</span>])<br><br>clf = RidgeClassifier()<br>clf.fit(train_test[:<span class="hljs-number">10000</span>], train_df[<span class="hljs-string">&#x27;label&#x27;</span>].values[:<span class="hljs-number">10000</span>])<br><br>val_pred = clf.predict(train_test[<span class="hljs-number">10000</span>:])<br><span class="hljs-built_in">print</span>(f1_score(train_df[<span class="hljs-string">&#x27;label&#x27;</span>].values[<span class="hljs-number">10000</span>:], val_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>))<br></code></pre></div></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">0.741494277019762<br></code></pre></div></td></tr></table></figure><h1id="基于深度学习的文本分类-word2vec">基于深度学习的文本分类-Word2Vec</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><br>logging.basicConfig(level=logging.INFO, <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)-15s %(levelname)s: %(message)s&#x27;</span>)<br><br><span class="hljs-comment"># set seed</span><br>seed = <span class="hljs-number">666</span><br>random.seed(seed)<br>np.random.seed(seed)<br>torch.cuda.manual_seed(seed)<br>torch.manual_seed(seed)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># split data to 10 fold</span><br>fold_num = <span class="hljs-number">10</span><br>data_file = <span class="hljs-string">&#x27;./data/train_set.csv&#x27;</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_data2fold</span>(<span class="hljs-params">fold_num, num=<span class="hljs-number">10000</span></span>):<br>    fold_data = []<br>    f = pd.read_csv(data_file, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>)<br>    <span class="hljs-comment"># texts: [&#x27;2967 6758 339 2021 1854 3731 4109 3792 4149 1519 ...]</span><br>    texts = f[<span class="hljs-string">&#x27;text&#x27;</span>].tolist()[:num]<br>    <span class="hljs-comment"># labels: [2, 11, 3, 2, 3, 9, 3, 10, 12, 3, 0, 7, 4, 0, 0 ...]</span><br>    labels = f[<span class="hljs-string">&#x27;label&#x27;</span>].tolist()[:num]<br><br>    total = <span class="hljs-built_in">len</span>(labels)<br><br>    <span class="hljs-comment"># 打乱index顺序，使all_texts和all_labels随机（一一对应）。</span><br>    index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(total))<br>    <span class="hljs-comment"># index: [3447, 966, 593, 5029, 4382, 2345, 974, 3786, 2249, ...]</span><br>    np.random.shuffle(index)<br><br>    all_texts = []<br>    all_labels = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index:<br>        all_texts.append(texts[i])<br>        all_labels.append(labels[i])<br>    <span class="hljs-comment"># all_texts: [&#x27;600 3373 2828 2515 5026 245 3743 26 2396 6122 3720 14 ...]</span><br>    <span class="hljs-comment"># all_labels: [2, 4, 8, 7, 0, 0, 2, 1, 12, 0, ...]</span><br><br>    <span class="hljs-comment"># 创建key:value字典，其中key是label,value是label对应的index列表。</span><br>    label2id = &#123;&#125;<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total):<br>        label = <span class="hljs-built_in">str</span>(all_labels[i])<br>        <span class="hljs-keyword">if</span> label <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> label2id:<br>            label2id[label] = [i]<br>        <span class="hljs-keyword">else</span>:<br>            label2id[label].append(i)<br>    <span class="hljs-comment"># label2id: &#123;&#x27;2&#x27;: [0, 6, 14, 21, 27, 28, 32, 39, ...], &#x27;4&#x27;: [...], ...&#125;</span><br><br>    <span class="hljs-comment"># 将同一label划分为10折。</span><br>    all_index = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num)]<br>    <span class="hljs-keyword">for</span> label, data <span class="hljs-keyword">in</span> label2id.items():<br>        <span class="hljs-comment"># print(label, len(data))</span><br>        <span class="hljs-comment"># 设data = 105</span><br>        <span class="hljs-comment"># fold_num = 10</span><br>        <span class="hljs-comment"># batch_size = 10</span><br>        <span class="hljs-comment"># other = 5</span><br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        i=0, cur_batch_size = 11, datch_data = [data[0], data[1], ... , data[10]]</span><br><span class="hljs-string">        i=1, cur_batch_size = 11, datch_data = [data[10], data[11], ... , data[20]]</span><br><span class="hljs-string">        ......</span><br><span class="hljs-string">        i=5, cur_batch_size = 10, datch_data = [data[50], data[51], ... , data[59]]</span><br><span class="hljs-string">        i=6, cur_batch_size = 10, datch_data = [data[60], data[61], ... , data[69]]</span><br><span class="hljs-string">        ......</span><br><span class="hljs-string">        i=9, cur_batch_size = 10, datch_data = [data[90], data[91], ... , data[99]]</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># print(label, len(data))</span><br>        batch_size = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) / fold_num)<br>        other = <span class="hljs-built_in">len</span>(data) - batch_size * fold_num<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num):<br>            cur_batch_size = batch_size + <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> i &lt; other <span class="hljs-keyword">else</span> batch_size<br>            <span class="hljs-comment"># print(cur_batch_size)</span><br>            batch_data = [data[i * batch_size + b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cur_batch_size)]<br>            all_index[i].extend(batch_data)<br><br>    batch_size = <span class="hljs-built_in">int</span>(total / fold_num)<br>    other_texts = []<br>    other_labels = []<br>    other_num = <span class="hljs-number">0</span><br>    start = <span class="hljs-number">0</span><br>    <span class="hljs-comment"># 将10折分类后的all_index按照index分别划分</span><br>    <span class="hljs-keyword">for</span> fold <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num):<br>        num = <span class="hljs-built_in">len</span>(all_index[fold])<br>        texts = [all_texts[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> all_index[fold]]<br>        labels = [all_labels[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> all_index[fold]]<br><br>        <span class="hljs-comment"># 单折数量&gt;10折平均数时，文本数截断至平均数batch_size,并降截断的文本分至other_texts,标签fold_labels同上</span><br>        <span class="hljs-keyword">if</span> num &gt; batch_size:<br>            fold_texts = texts[:batch_size]<br>            other_texts.extend(texts[batch_size:])<br>            fold_labels = labels[:batch_size]<br>            other_labels.extend(labels[batch_size:])<br>            other_num += num - batch_size<br>        <span class="hljs-comment"># 单折数量&lt;10折平均数时，原有的文本加other_size中batch_size-num的文本数，标签fold_labels同上</span><br>        <span class="hljs-keyword">elif</span> num &lt; batch_size:<br>            end = start + batch_size - num<br>            fold_texts = texts + other_texts[start: end]<br>            fold_labels = labels + other_labels[start: end]<br>            start = end<br>        <span class="hljs-comment"># 否则，文本数和标签数不变。</span><br>        <span class="hljs-keyword">else</span>:<br>            fold_texts = texts<br>            fold_labels = labels<br><br>        <span class="hljs-keyword">assert</span> batch_size == <span class="hljs-built_in">len</span>(fold_labels)<br><br>        <span class="hljs-comment"># shuffle，对10折文本和标签重新刷新。</span><br>        index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(batch_size))<br>        np.random.shuffle(index)<br><br>        shuffle_fold_texts = []<br>        shuffle_fold_labels = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index:<br>            shuffle_fold_texts.append(fold_texts[i])<br>            shuffle_fold_labels.append(fold_labels[i])<br><br>        data = &#123;<span class="hljs-string">&#x27;label&#x27;</span>: shuffle_fold_labels, <span class="hljs-string">&#x27;text&#x27;</span>: shuffle_fold_texts&#125;<br>        fold_data.append(data)<br><br>    logging.info(<span class="hljs-string">&quot;Fold lens %s&quot;</span>, <span class="hljs-built_in">str</span>([<span class="hljs-built_in">len</span>(data[<span class="hljs-string">&#x27;label&#x27;</span>]) <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> fold_data]))<br><br>    <span class="hljs-keyword">return</span> fold_data<br><br><br>fold_data = all_data2fold(<span class="hljs-number">10</span>)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build train data for word2vec</span><br>fold_id = <span class="hljs-number">9</span><br><br>train_texts = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, fold_id):<br>    data = fold_data[i]<br>    train_texts.extend(data[<span class="hljs-string">&#x27;text&#x27;</span>])<br>    <br>logging.info(<span class="hljs-string">&#x27;Total %d docs.&#x27;</span> % <span class="hljs-built_in">len</span>(train_texts))<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">logging.info(<span class="hljs-string">&#x27;Start training...&#x27;</span>)<br><span class="hljs-keyword">from</span> gensim.models.word2vec <span class="hljs-keyword">import</span> Word2Vec<br><br>num_features = <span class="hljs-number">100</span>     <span class="hljs-comment"># Word vector dimensionality</span><br>num_workers = <span class="hljs-number">8</span>       <span class="hljs-comment"># Number of threads to run in parallel</span><br><br>train_texts = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">list</span>(x.split()), train_texts))<br>model = Word2Vec(train_texts, workers=num_workers, size=num_features)<br>model.init_sims(replace=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># save model</span><br>model.save(<span class="hljs-string">&quot;./save/word2vec.bin&quot;</span>)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># load model</span><br>model = Word2Vec.load(<span class="hljs-string">&quot;./save/word2vec.bin&quot;</span>)<br><br><span class="hljs-comment"># convert format</span><br>model.wv.save_word2vec_format(<span class="hljs-string">&#x27;./save/word2vec.txt&#x27;</span>, binary=<span class="hljs-literal">False</span>)<br></code></pre></div></td></tr></table></figure><h1id="基于深度学习的文本分类-textrnn">基于深度学习的文本分类-TextRNN</h1><p>TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><br>logging.basicConfig(level=logging.INFO, <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)-15s %(levelname)s: %(message)s&#x27;</span>)<br><br><span class="hljs-comment"># set seed</span><br>seed = <span class="hljs-number">666</span><br>random.seed(seed)<br>np.random.seed(seed)<br>torch.cuda.manual_seed(seed)<br>torch.manual_seed(seed)<br><br><span class="hljs-comment"># set cuda</span><br>gpu = <span class="hljs-number">0</span><br>use_cuda = gpu &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> torch.cuda.is_available()<br><span class="hljs-keyword">if</span> use_cuda:<br>    torch.cuda.set_device(gpu)<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>, gpu)<br><span class="hljs-keyword">else</span>:<br>    device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>logging.info(<span class="hljs-string">&quot;Use cuda: %s, gpu id: %d.&quot;</span>, use_cuda, gpu)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># split data to 10 fold</span><br>fold_num = <span class="hljs-number">10</span><br>data_file = <span class="hljs-string">&#x27;../data/train_set.csv&#x27;</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_data2fold</span>(<span class="hljs-params">fold_num, num=<span class="hljs-number">1000</span></span>):<br>    fold_data = []<br>    f = pd.read_csv(data_file, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>)<br>    texts = f[<span class="hljs-string">&#x27;text&#x27;</span>].tolist()[:num]<br>    labels = f[<span class="hljs-string">&#x27;label&#x27;</span>].tolist()[:num]<br><br>    total = <span class="hljs-built_in">len</span>(labels)<br><br>    index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(total))<br>    np.random.shuffle(index)<br><br>    all_texts = []<br>    all_labels = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index:<br>        all_texts.append(texts[i])<br>        all_labels.append(labels[i])<br><br>    label2id = &#123;&#125;<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total):<br>        label = <span class="hljs-built_in">str</span>(all_labels[i])<br>        <span class="hljs-keyword">if</span> label <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> label2id:<br>            label2id[label] = [i]<br>        <span class="hljs-keyword">else</span>:<br>            label2id[label].append(i)<br><br>    all_index = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num)]<br>    <span class="hljs-keyword">for</span> label, data <span class="hljs-keyword">in</span> label2id.items():<br>        <span class="hljs-comment"># print(label, len(data))</span><br>        batch_size = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) / fold_num)<br>        other = <span class="hljs-built_in">len</span>(data) - batch_size * fold_num<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num):<br>            cur_batch_size = batch_size + <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> i &lt; other <span class="hljs-keyword">else</span> batch_size<br>            <span class="hljs-comment"># print(cur_batch_size)</span><br>            batch_data = [data[i * batch_size + b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cur_batch_size)]<br>            all_index[i].extend(batch_data)<br><br>    batch_size = <span class="hljs-built_in">int</span>(total / fold_num)<br>    other_texts = []<br>    other_labels = []<br>    other_num = <span class="hljs-number">0</span><br>    start = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> fold <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num):<br>        num = <span class="hljs-built_in">len</span>(all_index[fold])<br>        texts = [all_texts[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> all_index[fold]]<br>        labels = [all_labels[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> all_index[fold]]<br><br>        <span class="hljs-keyword">if</span> num &gt; batch_size:<br>            fold_texts = texts[:batch_size]<br>            other_texts.extend(texts[batch_size:])<br>            fold_labels = labels[:batch_size]<br>            other_labels.extend(labels[batch_size:])<br>            other_num += num - batch_size<br>        <span class="hljs-keyword">elif</span> num &lt; batch_size:<br>            end = start + batch_size - num<br>            fold_texts = texts + other_texts[start: end]<br>            fold_labels = labels + other_labels[start: end]<br>            start = end<br>        <span class="hljs-keyword">else</span>:<br>            fold_texts = texts<br>            fold_labels = labels<br><br>        <span class="hljs-keyword">assert</span> batch_size == <span class="hljs-built_in">len</span>(fold_labels)<br><br>        <span class="hljs-comment"># shuffle</span><br>        index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(batch_size))<br>        np.random.shuffle(index)<br><br>        shuffle_fold_texts = []<br>        shuffle_fold_labels = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index:<br>            shuffle_fold_texts.append(fold_texts[i])<br>            shuffle_fold_labels.append(fold_labels[i])<br><br>        data = &#123;<span class="hljs-string">&#x27;label&#x27;</span>: shuffle_fold_labels, <span class="hljs-string">&#x27;text&#x27;</span>: shuffle_fold_texts&#125;<br>        fold_data.append(data)<br><br>    logging.info(<span class="hljs-string">&quot;Fold lens %s&quot;</span>, <span class="hljs-built_in">str</span>([<span class="hljs-built_in">len</span>(data[<span class="hljs-string">&#x27;label&#x27;</span>]) <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> fold_data]))<br><br>    <span class="hljs-keyword">return</span> fold_data<br><br><br>fold_data = all_data2fold(<span class="hljs-number">10</span>)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build vocab</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BasicTokenizer<br><br>basic_tokenizer = BasicTokenizer()<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train_data</span>):<br>        self.min_count = <span class="hljs-number">5</span><br>        self.pad = <span class="hljs-number">0</span><br>        self.unk = <span class="hljs-number">1</span><br>        self._id2word = [<span class="hljs-string">&#x27;[PAD]&#x27;</span>, <span class="hljs-string">&#x27;[UNK]&#x27;</span>]<br>        self._id2extword = [<span class="hljs-string">&#x27;[PAD]&#x27;</span>, <span class="hljs-string">&#x27;[UNK]&#x27;</span>]<br><br>        self._id2label = []<br>        self.target_names = []<br><br>        self.build_vocab(train_data)<br><br>        reverse = <span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(x, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))))<br>        <br>        <span class="hljs-comment"># _word2id = &#123;&#x27;[PAD]&#x27;: 0, &#x27;[UNK]&#x27;: 1&#125;</span><br>        self._word2id = reverse(self._id2word)<br>        self._label2id = reverse(self._id2label)<br><br>        logging.info(<span class="hljs-string">&quot;Build vocab: words %d, labels %d.&quot;</span> % (self.word_size, self.label_size))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_vocab</span>(<span class="hljs-params">self, data</span>):<br>        self.word_counter = Counter()<br><br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> data[<span class="hljs-string">&#x27;text&#x27;</span>]:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                self.word_counter[word] += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">for</span> word, count <span class="hljs-keyword">in</span> self.word_counter.most_common():<br>            <span class="hljs-keyword">if</span> count &gt;= self.min_count:<br>                self._id2word.append(word)<br><br>        label2name = &#123;<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;科技&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;股票&#x27;</span>, <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;体育&#x27;</span>, <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;娱乐&#x27;</span>, <span class="hljs-number">4</span>: <span class="hljs-string">&#x27;时政&#x27;</span>, <span class="hljs-number">5</span>: <span class="hljs-string">&#x27;社会&#x27;</span>, <span class="hljs-number">6</span>: <span class="hljs-string">&#x27;教育&#x27;</span>, <span class="hljs-number">7</span>: <span class="hljs-string">&#x27;财经&#x27;</span>,<br>                      <span class="hljs-number">8</span>: <span class="hljs-string">&#x27;家居&#x27;</span>, <span class="hljs-number">9</span>: <span class="hljs-string">&#x27;游戏&#x27;</span>, <span class="hljs-number">10</span>: <span class="hljs-string">&#x27;房产&#x27;</span>, <span class="hljs-number">11</span>: <span class="hljs-string">&#x27;时尚&#x27;</span>, <span class="hljs-number">12</span>: <span class="hljs-string">&#x27;彩票&#x27;</span>, <span class="hljs-number">13</span>: <span class="hljs-string">&#x27;星座&#x27;</span>&#125;<br><br>        self.label_counter = Counter(data[<span class="hljs-string">&#x27;label&#x27;</span>])<br><br>        <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.label_counter)):<br>            count = self.label_counter[label]<br>            self._id2label.append(label)<br>            self.target_names.append(label2name[label])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_pretrained_embs</span>(<span class="hljs-params">self, embfile</span>):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(embfile, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            lines = f.readlines()<br>            items = lines[<span class="hljs-number">0</span>].split()<br>            word_count, embedding_dim = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">0</span>]), <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br><br>        index = <span class="hljs-built_in">len</span>(self._id2extword)<br>        embeddings = np.zeros((word_count + index, embedding_dim))<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines[<span class="hljs-number">1</span>:]:<br>            values = line.split()<br>            self._id2extword.append(values[<span class="hljs-number">0</span>])<br>            vector = np.array(values[<span class="hljs-number">1</span>:], dtype=<span class="hljs-string">&#x27;float64&#x27;</span>)<br>            embeddings[self.unk] += vector<br>            embeddings[index] = vector<br>            index += <span class="hljs-number">1</span><br><br>        embeddings[self.unk] = embeddings[self.unk] / word_count<br>        embeddings = embeddings / np.std(embeddings)<br><br>        reverse = <span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(x, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))))<br>        self._extword2id = reverse(self._id2extword)<br><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(self._id2extword)) == <span class="hljs-built_in">len</span>(self._id2extword)<br><br>        <span class="hljs-keyword">return</span> embeddings<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">word2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._word2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._word2id.get(xs, self.unk)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extword2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._extword2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._extword2id.get(xs, self.unk)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">label2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._label2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._label2id.get(xs, self.unk)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">word_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2word)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extword_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2extword)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">label_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2label)<br><br><br>vocab = Vocab(train_data)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build module</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size</span>):<br>        <span class="hljs-built_in">super</span>(Attention, self).__init__()<br>        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))<br>        self.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.05</span>)<br><br>        self.bias = nn.Parameter(torch.Tensor(hidden_size))<br>        b = np.zeros(hidden_size, dtype=np.float32)<br>        self.bias.data.copy_(torch.from_numpy(b))<br><br>        self.query = nn.Parameter(torch.Tensor(hidden_size))<br>        self.query.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.05</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, batch_hidden, batch_masks</span>):<br>        <span class="hljs-comment"># batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)</span><br>        <span class="hljs-comment"># batch_masks:  b x len</span><br><br>        <span class="hljs-comment"># linear</span><br>        key = torch.matmul(batch_hidden, self.weight) + self.bias  <span class="hljs-comment"># b x len x hidden</span><br><br>        <span class="hljs-comment"># compute attention</span><br>        outputs = torch.matmul(key, self.query)  <span class="hljs-comment"># b x len</span><br><br>        masked_outputs = outputs.masked_fill((<span class="hljs-number">1</span> - batch_masks).<span class="hljs-built_in">bool</span>(), <span class="hljs-built_in">float</span>(-<span class="hljs-number">1e32</span>))<br><br>        attn_scores = F.softmax(masked_outputs, dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># b x len</span><br><br>        <span class="hljs-comment"># 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0</span><br>        masked_attn_scores = attn_scores.masked_fill((<span class="hljs-number">1</span> - batch_masks).<span class="hljs-built_in">bool</span>(), <span class="hljs-number">0.0</span>)<br><br>        <span class="hljs-comment"># sum weighted sources</span><br>        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(<span class="hljs-number">1</span>), key).squeeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># b x hidden</span><br><br>        <span class="hljs-keyword">return</span> batch_outputs, attn_scores<br><br><br><span class="hljs-comment"># build word encoder</span><br>word2vec_path = <span class="hljs-string">&#x27;../emb/word2vec.txt&#x27;</span><br>dropout = <span class="hljs-number">0.15</span><br>word_hidden_size = <span class="hljs-number">128</span><br>word_num_layers = <span class="hljs-number">2</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordLSTMEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-built_in">super</span>(WordLSTMEncoder, self).__init__()<br>        self.dropout = nn.Dropout(dropout)<br>        self.word_dims = <span class="hljs-number">100</span><br><br>        self.word_embed = nn.Embedding(vocab.word_size, self.word_dims, padding_idx=<span class="hljs-number">0</span>)<br><br>        extword_embed = vocab.load_pretrained_embs(word2vec_path)<br>        extword_size, word_dims = extword_embed.shape<br>        logging.info(<span class="hljs-string">&quot;Load extword embed: words %d, dims %d.&quot;</span> % (extword_size, word_dims))<br><br>        self.extword_embed = nn.Embedding(extword_size, word_dims, padding_idx=<span class="hljs-number">0</span>)<br>        self.extword_embed.weight.data.copy_(torch.from_numpy(extword_embed))<br>        self.extword_embed.weight.requires_grad = <span class="hljs-literal">False</span><br><br>        input_size = self.word_dims<br><br>        self.word_lstm = nn.LSTM(<br>            input_size=input_size,<br>            hidden_size=word_hidden_size,<br>            num_layers=word_num_layers,<br>            batch_first=<span class="hljs-literal">True</span>,<br>            bidirectional=<span class="hljs-literal">True</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, word_ids, extword_ids, batch_masks</span>):<br>        <span class="hljs-comment"># word_ids: sen_num x sent_len</span><br>        <span class="hljs-comment"># extword_ids: sen_num x sent_len</span><br>        <span class="hljs-comment"># batch_masks   sen_num x sent_len</span><br><br>        word_embed = self.word_embed(word_ids)  <span class="hljs-comment"># sen_num x sent_len x 100</span><br>        extword_embed = self.extword_embed(extword_ids)<br>        batch_embed = word_embed + extword_embed<br><br>        <span class="hljs-keyword">if</span> self.training:<br>            batch_embed = self.dropout(batch_embed)<br><br>        hiddens, _ = self.word_lstm(batch_embed)  <span class="hljs-comment"># sen_num x sent_len x  hidden*2</span><br>        hiddens = hiddens * batch_masks.unsqueeze(<span class="hljs-number">2</span>)<br><br>        <span class="hljs-keyword">if</span> self.training:<br>            hiddens = self.dropout(hiddens)<br><br>        <span class="hljs-keyword">return</span> hiddens<br><br><br><span class="hljs-comment"># build sent encoder</span><br>sent_hidden_size = <span class="hljs-number">256</span><br>sent_num_layers = <span class="hljs-number">2</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SentEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, sent_rep_size</span>):<br>        <span class="hljs-built_in">super</span>(SentEncoder, self).__init__()<br>        self.dropout = nn.Dropout(dropout)<br><br>        self.sent_lstm = nn.LSTM(<br>            input_size=sent_rep_size,<br>            hidden_size=sent_hidden_size,<br>            num_layers=sent_num_layers,<br>            batch_first=<span class="hljs-literal">True</span>,<br>            bidirectional=<span class="hljs-literal">True</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sent_reps, sent_masks</span>):<br>        <span class="hljs-comment"># sent_reps:  b x doc_len x sent_rep_size</span><br>        <span class="hljs-comment"># sent_masks: b x doc_len</span><br><br>        sent_hiddens, _ = self.sent_lstm(sent_reps)  <span class="hljs-comment"># b x doc_len x hidden*2</span><br>        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(<span class="hljs-number">2</span>)<br><br>        <span class="hljs-keyword">if</span> self.training:<br>            sent_hiddens = self.dropout(sent_hiddens)<br><br>        <span class="hljs-keyword">return</span> sent_hiddens<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build model</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.sent_rep_size = word_hidden_size * <span class="hljs-number">2</span><br>        self.doc_rep_size = sent_hidden_size * <span class="hljs-number">2</span><br>        self.all_parameters = &#123;&#125;<br>        parameters = []<br>        self.word_encoder = WordLSTMEncoder(vocab)<br>        self.word_attention = Attention(self.sent_rep_size)<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.word_encoder.parameters())))<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.word_attention.parameters())))<br><br>        self.sent_encoder = SentEncoder(self.sent_rep_size)<br>        self.sent_attention = Attention(self.doc_rep_size)<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.sent_encoder.parameters())))<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.sent_attention.parameters())))<br><br>        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=<span class="hljs-literal">True</span>)<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.out.parameters())))<br><br>        <span class="hljs-keyword">if</span> use_cuda:<br>            self.to(device)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(parameters) &gt; <span class="hljs-number">0</span>:<br>            self.all_parameters[<span class="hljs-string">&quot;basic_parameters&quot;</span>] = parameters<br><br>        logging.info(<span class="hljs-string">&#x27;Build model with lstm word encoder, lstm sent encoder.&#x27;</span>)<br><br>        para_num = <span class="hljs-built_in">sum</span>([np.prod(<span class="hljs-built_in">list</span>(p.size())) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters()])<br>        logging.info(<span class="hljs-string">&#x27;Model param num: %.2f M.&#x27;</span> % (para_num / <span class="hljs-number">1e6</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, batch_inputs</span>):<br>        <span class="hljs-comment"># batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len</span><br>        <span class="hljs-comment"># batch_masks : b x doc_len x sent_len</span><br>        batch_inputs1, batch_inputs2, batch_masks = batch_inputs<br>        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[<span class="hljs-number">0</span>], batch_inputs1.shape[<span class="hljs-number">1</span>], batch_inputs1.shape[<span class="hljs-number">2</span>]<br>        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  <span class="hljs-comment"># sen_num x sent_len</span><br>        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  <span class="hljs-comment"># sen_num x sent_len</span><br>        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  <span class="hljs-comment"># sen_num x sent_len</span><br><br>        batch_hiddens = self.word_encoder(batch_inputs1, batch_inputs2,<br>                                          batch_masks)  <span class="hljs-comment"># sen_num x sent_len x sent_rep_size</span><br>        sent_reps, atten_scores = self.word_attention(batch_hiddens, batch_masks)  <span class="hljs-comment"># sen_num x sent_rep_size</span><br><br>        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  <span class="hljs-comment"># b x doc_len x sent_rep_size</span><br>        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  <span class="hljs-comment"># b x doc_len x max_sent_len</span><br>        sent_masks = batch_masks.<span class="hljs-built_in">bool</span>().<span class="hljs-built_in">any</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>()  <span class="hljs-comment"># b x doc_len</span><br><br>        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  <span class="hljs-comment"># b x doc_len x doc_rep_size</span><br>        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  <span class="hljs-comment"># b x doc_rep_size</span><br><br>        batch_outputs = self.out(doc_reps)  <span class="hljs-comment"># b x num_labels</span><br><br>        <span class="hljs-keyword">return</span> batch_outputs<br><br><br>model = Model(vocab)<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build optimizer</span><br>learning_rate = <span class="hljs-number">2e-4</span><br>decay = <span class="hljs-number">.75</span><br>decay_step = <span class="hljs-number">1000</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Optimizer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_parameters</span>):<br>        self.all_params = []<br>        self.optims = []<br>        self.schedulers = []<br><br>        <span class="hljs-keyword">for</span> name, parameters <span class="hljs-keyword">in</span> model_parameters.items():<br>            <span class="hljs-keyword">if</span> name.startswith(<span class="hljs-string">&quot;basic&quot;</span>):<br>                optim = torch.optim.Adam(parameters, lr=learning_rate)<br>                self.optims.append(optim)<br><br>                l = <span class="hljs-keyword">lambda</span> step: decay ** (step // decay_step)<br>                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)<br>                self.schedulers.append(scheduler)<br>                self.all_params.extend(parameters)<br><br>            <span class="hljs-keyword">else</span>:<br>                Exception(<span class="hljs-string">&quot;no nameed parameters.&quot;</span>)<br><br>        self.num = <span class="hljs-built_in">len</span>(self.optims)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> optim, scheduler <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.optims, self.schedulers):<br>            optim.step()<br>            scheduler.step()<br>            optim.zero_grad()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">zero_grad</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> optim <span class="hljs-keyword">in</span> self.optims:<br>            optim.zero_grad()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_lr</span>(<span class="hljs-params">self</span>):<br>        lrs = <span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.get_lr()[-<span class="hljs-number">1</span>], self.schedulers))<br>        lr = <span class="hljs-string">&#x27; %.5f&#x27;</span> * self.num<br>        res = lr % lrs<br>        <span class="hljs-keyword">return</span> res<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build dataset</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sentence_split</span>(<span class="hljs-params">text, vocab, max_sent_len=<span class="hljs-number">256</span>, max_segment=<span class="hljs-number">16</span></span>):<br>    words = text.strip().split()<br>    document_len = <span class="hljs-built_in">len</span>(words)<br><br>    index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, document_len, max_sent_len))<br>    index.append(document_len)<br><br>    segments = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(index) - <span class="hljs-number">1</span>):<br>        segment = words[index[i]: index[i + <span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(segment) &gt; <span class="hljs-number">0</span><br>        segment = [word <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> vocab._id2word <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&lt;UNK&gt;&#x27;</span> <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> segment]<br>        segments.append([<span class="hljs-built_in">len</span>(segment), segment])<br><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(segments) &gt; <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(segments) &gt; max_segment:<br>        segment_ = <span class="hljs-built_in">int</span>(max_segment / <span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> segments[:segment_] + segments[-segment_:]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> segments<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_examples</span>(<span class="hljs-params">data, vocab, max_sent_len=<span class="hljs-number">256</span>, max_segment=<span class="hljs-number">8</span></span>):<br>    label2id = vocab.label2id<br>    examples = []<br><br>    <span class="hljs-keyword">for</span> text, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data[<span class="hljs-string">&#x27;text&#x27;</span>], data[<span class="hljs-string">&#x27;label&#x27;</span>]):<br>        <span class="hljs-comment"># label</span><br>        <span class="hljs-built_in">id</span> = label2id(label)<br><br>        <span class="hljs-comment"># words</span><br>        sents_words = sentence_split(text, vocab, max_sent_len, max_segment)<br>        doc = []<br>        <span class="hljs-keyword">for</span> sent_len, sent_words <span class="hljs-keyword">in</span> sents_words:<br>            word_ids = vocab.word2id(sent_words)<br>            extword_ids = vocab.extword2id(sent_words)<br>            doc.append([sent_len, word_ids, extword_ids])<br>        examples.append([<span class="hljs-built_in">id</span>, <span class="hljs-built_in">len</span>(doc), doc])<br><br>    logging.info(<span class="hljs-string">&#x27;Total %d docs.&#x27;</span> % <span class="hljs-built_in">len</span>(examples))<br>    <span class="hljs-keyword">return</span> examples<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build loader</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_slice</span>(<span class="hljs-params">data, batch_size</span>):<br>    batch_num = <span class="hljs-built_in">int</span>(np.ceil(<span class="hljs-built_in">len</span>(data) / <span class="hljs-built_in">float</span>(batch_size)))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_num):<br>        cur_batch_size = batch_size <span class="hljs-keyword">if</span> i &lt; batch_num - <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">len</span>(data) - batch_size * i<br>        docs = [data[i * batch_size + b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cur_batch_size)]<br><br>        <span class="hljs-keyword">yield</span> docs<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_iter</span>(<span class="hljs-params">data, batch_size, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-number">1.0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    randomly permute data, then sort by source length, and partition into batches</span><br><span class="hljs-string">    ensure that the length of  sentences in each batch</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    batched_data = []<br>    <span class="hljs-keyword">if</span> shuffle:<br>        np.random.shuffle(data)<br><br>    lengths = [example[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> data]<br>    noisy_lengths = [- (l + np.random.uniform(- noise, noise)) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> lengths]<br>    sorted_indices = np.argsort(noisy_lengths).tolist()<br>    sorted_data = [data[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sorted_indices]<br><br>    batched_data.extend(<span class="hljs-built_in">list</span>(batch_slice(sorted_data, batch_size)))<br><br>    <span class="hljs-keyword">if</span> shuffle:<br>        np.random.shuffle(batched_data)<br><br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> batched_data:<br>        <span class="hljs-keyword">yield</span> batch<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># some function</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score, precision_score, recall_score<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_score</span>(<span class="hljs-params">y_ture, y_pred</span>):<br>    y_ture = np.array(y_ture)<br>    y_pred = np.array(y_pred)<br>    f1 = f1_score(y_ture, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>) * <span class="hljs-number">100</span><br>    p = precision_score(y_ture, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>) * <span class="hljs-number">100</span><br>    r = recall_score(y_ture, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>) * <span class="hljs-number">100</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">str</span>((reformat(p, <span class="hljs-number">2</span>), reformat(r, <span class="hljs-number">2</span>), reformat(f1, <span class="hljs-number">2</span>))), reformat(f1, <span class="hljs-number">2</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reformat</span>(<span class="hljs-params">num, n</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(<span class="hljs-built_in">format</span>(num, <span class="hljs-string">&#x27;0.&#x27;</span> + <span class="hljs-built_in">str</span>(n) + <span class="hljs-string">&#x27;f&#x27;</span>))<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># build trainer</span><br><br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><br>clip = <span class="hljs-number">5.0</span><br>epochs = <span class="hljs-number">1</span><br>early_stops = <span class="hljs-number">3</span><br>log_interval = <span class="hljs-number">200</span><br><br>test_batch_size = <span class="hljs-number">16</span><br>train_batch_size = <span class="hljs-number">16</span><br><br>save_model = <span class="hljs-string">&#x27;../save/rnn.bin&#x27;</span><br>save_test = <span class="hljs-string">&#x27;../save/rnn.csv&#x27;</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, vocab</span>):<br>        self.model = model<br>        self.report = <span class="hljs-literal">True</span><br><br>        self.train_data = get_examples(train_data, vocab)<br>        self.batch_num = <span class="hljs-built_in">int</span>(np.ceil(<span class="hljs-built_in">len</span>(self.train_data) / <span class="hljs-built_in">float</span>(train_batch_size)))<br>        self.dev_data = get_examples(dev_data, vocab)<br>        self.test_data = get_examples(test_data, vocab)<br><br>        <span class="hljs-comment"># criterion</span><br>        self.criterion = nn.CrossEntropyLoss()<br><br>        <span class="hljs-comment"># label name</span><br>        self.target_names = vocab.target_names<br><br>        <span class="hljs-comment"># optimizer</span><br>        self.optimizer = Optimizer(model.all_parameters)<br><br>        <span class="hljs-comment"># count</span><br>        self.step = <span class="hljs-number">0</span><br>        self.early_stop = -<span class="hljs-number">1</span><br>        self.best_train_f1, self.best_dev_f1 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        self.last_epoch = epochs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        logging.info(<span class="hljs-string">&#x27;Start training...&#x27;</span>)<br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):<br>            train_f1 = self._train(epoch)<br><br>            dev_f1 = self._<span class="hljs-built_in">eval</span>(epoch)<br><br>            <span class="hljs-keyword">if</span> self.best_dev_f1 &lt;= dev_f1:<br>                logging.info(<br>                    <span class="hljs-string">&quot;Exceed history dev = %.2f, current dev = %.2f&quot;</span> % (self.best_dev_f1, dev_f1))<br>                torch.save(self.model.state_dict(), save_model)<br><br>                self.best_train_f1 = train_f1<br>                self.best_dev_f1 = dev_f1<br>                self.early_stop = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">else</span>:<br>                self.early_stop += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> self.early_stop == early_stops:<br>                    logging.info(<br>                        <span class="hljs-string">&quot;Eearly stop in epoch %d, best train: %.2f, dev: %.2f&quot;</span> % (<br>                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))<br>                    self.last_epoch = epoch<br>                    <span class="hljs-keyword">break</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">self</span>):<br>        self.model.load_state_dict(torch.load(save_model))<br>        self._<span class="hljs-built_in">eval</span>(self.last_epoch + <span class="hljs-number">1</span>, test=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_train</span>(<span class="hljs-params">self, epoch</span>):<br>        self.optimizer.zero_grad()<br>        self.model.train()<br><br>        start_time = time.time()<br>        epoch_start_time = time.time()<br>        overall_losses = <span class="hljs-number">0</span><br>        losses = <span class="hljs-number">0</span><br>        batch_idx = <span class="hljs-number">1</span><br>        y_pred = []<br>        y_true = []<br>        <span class="hljs-keyword">for</span> batch_data <span class="hljs-keyword">in</span> data_iter(self.train_data, train_batch_size, shuffle=<span class="hljs-literal">True</span>):<br>            torch.cuda.empty_cache()<br>            batch_inputs, batch_labels = self.batch2tensor(batch_data)<br>            batch_outputs = self.model(batch_inputs)<br>            loss = self.criterion(batch_outputs, batch_labels)<br>            loss.backward()<br><br>            loss_value = loss.detach().cpu().item()<br>            losses += loss_value<br>            overall_losses += loss_value<br><br>            y_pred.extend(torch.<span class="hljs-built_in">max</span>(batch_outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].cpu().numpy().tolist())<br>            y_true.extend(batch_labels.cpu().numpy().tolist())<br><br>            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)<br>            <span class="hljs-keyword">for</span> optimizer, scheduler <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.optimizer.optims, self.optimizer.schedulers):<br>                optimizer.step()<br>                scheduler.step()<br>            self.optimizer.zero_grad()<br><br>            self.step += <span class="hljs-number">1</span><br><br>            <span class="hljs-keyword">if</span> batch_idx % log_interval == <span class="hljs-number">0</span>:<br>                elapsed = time.time() - start_time<br><br>                lrs = self.optimizer.get_lr()<br>                logging.info(<br>                    <span class="hljs-string">&#x27;| epoch &#123;:3d&#125; | step &#123;:3d&#125; | batch &#123;:3d&#125;/&#123;:3d&#125; | lr&#123;&#125; | loss &#123;:.4f&#125; | s/batch &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                        epoch, self.step, batch_idx, self.batch_num, lrs,<br>                        losses / log_interval,<br>                        elapsed / log_interval))<br><br>                losses = <span class="hljs-number">0</span><br>                start_time = time.time()<br><br>            batch_idx += <span class="hljs-number">1</span><br><br>        overall_losses /= self.batch_num<br>        during_time = time.time() - epoch_start_time<br><br>        <span class="hljs-comment"># reformat</span><br>        overall_losses = reformat(overall_losses, <span class="hljs-number">4</span>)<br>        score, f1 = get_score(y_true, y_pred)<br><br>        logging.info(<br>            <span class="hljs-string">&#x27;| epoch &#123;:3d&#125; | score &#123;&#125; | f1 &#123;&#125; | loss &#123;:.4f&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, score, f1,<br>                                                                                  overall_losses,<br>                                                                                  during_time))<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">set</span>(y_true) == <span class="hljs-built_in">set</span>(y_pred) <span class="hljs-keyword">and</span> self.report:<br>            report = classification_report(y_true, y_pred, digits=<span class="hljs-number">4</span>, target_names=self.target_names)<br>            logging.info(<span class="hljs-string">&#x27;\n&#x27;</span> + report)<br><br>        <span class="hljs-keyword">return</span> f1<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_eval</span>(<span class="hljs-params">self, epoch, test=<span class="hljs-literal">False</span></span>):<br>        self.model.<span class="hljs-built_in">eval</span>()<br>        start_time = time.time()<br><br>        y_pred = []<br>        y_true = []<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> batch_data <span class="hljs-keyword">in</span> data_iter(self.dev_data, test_batch_size, shuffle=<span class="hljs-literal">False</span>):<br>                torch.cuda.empty_cache()<br>                batch_inputs, batch_labels = self.batch2tensor(batch_data)<br>                batch_outputs = self.model(batch_inputs)<br>                y_pred.extend(torch.<span class="hljs-built_in">max</span>(batch_outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].cpu().numpy().tolist())<br>                y_true.extend(batch_labels.cpu().numpy().tolist())<br><br>            score, f1 = get_score(y_true, y_pred)<br><br>            during_time = time.time() - start_time<br>            <br>            <span class="hljs-keyword">if</span> test:<br>                df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;label&#x27;</span>: y_pred&#125;)<br>                df.to_csv(save_test, index=<span class="hljs-literal">False</span>, sep=<span class="hljs-string">&#x27;,&#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                logging.info(<br>                    <span class="hljs-string">&#x27;| epoch &#123;:3d&#125; | dev | score &#123;&#125; | f1 &#123;&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, score, f1,<br>                                                                              during_time))<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">set</span>(y_true) == <span class="hljs-built_in">set</span>(y_pred) <span class="hljs-keyword">and</span> self.report:<br>                    report = classification_report(y_true, y_pred, digits=<span class="hljs-number">4</span>, target_names=self.target_names)<br>                    logging.info(<span class="hljs-string">&#x27;\n&#x27;</span> + report)<br><br>        <span class="hljs-keyword">return</span> f1<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">batch2tensor</span>(<span class="hljs-params">self, batch_data</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        batch_size = <span class="hljs-built_in">len</span>(batch_data)<br>        doc_labels = []<br>        doc_lens = []<br>        doc_max_sent_len = []<br>        <span class="hljs-keyword">for</span> doc_data <span class="hljs-keyword">in</span> batch_data:<br>            doc_labels.append(doc_data[<span class="hljs-number">0</span>])<br>            doc_lens.append(doc_data[<span class="hljs-number">1</span>])<br>            sent_lens = [sent_data[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> sent_data <span class="hljs-keyword">in</span> doc_data[<span class="hljs-number">2</span>]]<br>            max_sent_len = <span class="hljs-built_in">max</span>(sent_lens)<br>            doc_max_sent_len.append(max_sent_len)<br><br>        max_doc_len = <span class="hljs-built_in">max</span>(doc_lens)<br>        max_sent_len = <span class="hljs-built_in">max</span>(doc_max_sent_len)<br><br>        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)<br>        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)<br>        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)<br>        batch_labels = torch.LongTensor(doc_labels)<br><br>        <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>            <span class="hljs-keyword">for</span> sent_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(doc_lens[b]):<br>                sent_data = batch_data[b][<span class="hljs-number">2</span>][sent_idx]<br>                <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(sent_data[<span class="hljs-number">0</span>]):<br>                    batch_inputs1[b, sent_idx, word_idx] = sent_data[<span class="hljs-number">1</span>][word_idx]<br>                    batch_inputs2[b, sent_idx, word_idx] = sent_data[<span class="hljs-number">2</span>][word_idx]<br>                    batch_masks[b, sent_idx, word_idx] = <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> use_cuda:<br>            batch_inputs1 = batch_inputs1.to(device)<br>            batch_inputs2 = batch_inputs2.to(device)<br>            batch_masks = batch_masks.to(device)<br>            batch_labels = batch_labels.to(device)<br><br>        <span class="hljs-keyword">return</span> (batch_inputs1, batch_inputs2, batch_masks), batch_labels<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># train</span><br>trainer = Trainer(model, vocab)<br>trainer.train()<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># test</span><br>trainer.test()<br></code></pre></div></td></tr></table></figure><h1 id="基于深度学习的文本分类-bert">基于深度学习的文本分类-BERT</h1><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> logging<br><span class="hljs-keyword">import</span> random<br><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> torch<br><br>logging.basicConfig(level=logging.INFO, <span class="hljs-built_in">format</span>=<span class="hljs-string">&#x27;%(asctime)-15s %(levelname)s: %(message)s&#x27;</span>)<br><br><span class="hljs-comment"># set seed</span><br>seed = <span class="hljs-number">666</span><br>random.seed(seed)<br>np.random.seed(seed)<br>torch.cuda.manual_seed(seed)<br>torch.manual_seed(seed)<br><br><span class="hljs-comment"># set cuda</span><br>gpu = <span class="hljs-number">0</span><br>use_cuda = gpu &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> torch.cuda.is_available()<br><span class="hljs-keyword">if</span> use_cuda:<br>    torch.cuda.set_device(gpu)<br>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>, gpu)<br><span class="hljs-keyword">else</span>:<br>    device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br>logging.info(<span class="hljs-string">&quot;Use cuda: %s, gpu id: %d.&quot;</span>, use_cuda, gpu)<br><br><span class="hljs-comment"># split data to 10 fold</span><br>fold_num = <span class="hljs-number">10</span><br>data_file = <span class="hljs-string">&#x27;../data/train_set.csv&#x27;</span><br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">all_data2fold</span>(<span class="hljs-params">fold_num, num=<span class="hljs-number">10000</span></span>):<br>    fold_data = []<br>    f = pd.read_csv(data_file, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>)<br>    texts = f[<span class="hljs-string">&#x27;text&#x27;</span>].tolist()[:num]<br>    labels = f[<span class="hljs-string">&#x27;label&#x27;</span>].tolist()[:num]<br><br>    total = <span class="hljs-built_in">len</span>(labels)<br><br>    index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(total))<br>    np.random.shuffle(index)<br><br>    all_texts = []<br>    all_labels = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index:<br>        all_texts.append(texts[i])<br>        all_labels.append(labels[i])<br><br>    label2id = &#123;&#125;<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(total):<br>        label = <span class="hljs-built_in">str</span>(all_labels[i])<br>        <span class="hljs-keyword">if</span> label <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> label2id:<br>            label2id[label] = [i]<br>        <span class="hljs-keyword">else</span>:<br>            label2id[label].append(i)<br><br>    all_index = [[] <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num)]<br>    <span class="hljs-keyword">for</span> label, data <span class="hljs-keyword">in</span> label2id.items():<br>        <span class="hljs-comment"># print(label, len(data))</span><br>        batch_size = <span class="hljs-built_in">int</span>(<span class="hljs-built_in">len</span>(data) / fold_num)<br>        other = <span class="hljs-built_in">len</span>(data) - batch_size * fold_num<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num):<br>            cur_batch_size = batch_size + <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> i &lt; other <span class="hljs-keyword">else</span> batch_size<br>            <span class="hljs-comment"># print(cur_batch_size)</span><br>            batch_data = [data[i * batch_size + b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cur_batch_size)]<br>            all_index[i].extend(batch_data)<br><br>    batch_size = <span class="hljs-built_in">int</span>(total / fold_num)<br>    other_texts = []<br>    other_labels = []<br>    other_num = <span class="hljs-number">0</span><br>    start = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> fold <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(fold_num):<br>        num = <span class="hljs-built_in">len</span>(all_index[fold])<br>        texts = [all_texts[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> all_index[fold]]<br>        labels = [all_labels[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> all_index[fold]]<br><br>        <span class="hljs-keyword">if</span> num &gt; batch_size:<br>            fold_texts = texts[:batch_size]<br>            other_texts.extend(texts[batch_size:])<br>            fold_labels = labels[:batch_size]<br>            other_labels.extend(labels[batch_size:])<br>            other_num += num - batch_size<br>        <span class="hljs-keyword">elif</span> num &lt; batch_size:<br>            end = start + batch_size - num<br>            fold_texts = texts + other_texts[start: end]<br>            fold_labels = labels + other_labels[start: end]<br>            start = end<br>        <span class="hljs-keyword">else</span>:<br>            fold_texts = texts<br>            fold_labels = labels<br><br>        <span class="hljs-keyword">assert</span> batch_size == <span class="hljs-built_in">len</span>(fold_labels)<br><br>        <span class="hljs-comment"># shuffle</span><br>        index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(batch_size))<br>        np.random.shuffle(index)<br><br>        shuffle_fold_texts = []<br>        shuffle_fold_labels = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> index:<br>            shuffle_fold_texts.append(fold_texts[i])<br>            shuffle_fold_labels.append(fold_labels[i])<br><br>        data = &#123;<span class="hljs-string">&#x27;label&#x27;</span>: shuffle_fold_labels, <span class="hljs-string">&#x27;text&#x27;</span>: shuffle_fold_texts&#125;<br>        fold_data.append(data)<br><br>    logging.info(<span class="hljs-string">&quot;Fold lens %s&quot;</span>, <span class="hljs-built_in">str</span>([<span class="hljs-built_in">len</span>(data[<span class="hljs-string">&#x27;label&#x27;</span>]) <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> fold_data]))<br><br>    <span class="hljs-keyword">return</span> fold_data<br><br><br>fold_data = all_data2fold(<span class="hljs-number">10</span>)<br><br><span class="hljs-comment"># build train, dev, test data</span><br>fold_id = <span class="hljs-number">9</span><br><br><span class="hljs-comment"># dev</span><br>dev_data = fold_data[fold_id]<br><br><span class="hljs-comment"># train</span><br>train_texts = []<br>train_labels = []<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, fold_id):<br>    data = fold_data[i]<br>    train_texts.extend(data[<span class="hljs-string">&#x27;text&#x27;</span>])<br>    train_labels.extend(data[<span class="hljs-string">&#x27;label&#x27;</span>])<br><br>train_data = &#123;<span class="hljs-string">&#x27;label&#x27;</span>: train_labels, <span class="hljs-string">&#x27;text&#x27;</span>: train_texts&#125;<br><br><span class="hljs-comment"># test</span><br>test_data_file = <span class="hljs-string">&#x27;../data/test_a.csv&#x27;</span><br>f = pd.read_csv(test_data_file, sep=<span class="hljs-string">&#x27;\t&#x27;</span>, encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>)<br>texts = f[<span class="hljs-string">&#x27;text&#x27;</span>].tolist()<br>test_data = &#123;<span class="hljs-string">&#x27;label&#x27;</span>: [<span class="hljs-number">0</span>] * <span class="hljs-built_in">len</span>(texts), <span class="hljs-string">&#x27;text&#x27;</span>: texts&#125;<br><br><span class="hljs-comment"># build vocab</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> Counter<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BasicTokenizer<br><br><span class="hljs-comment"># 分词器</span><br>basic_tokenizer = BasicTokenizer()<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Vocab</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, train_data</span>):<br>        self.min_count = <span class="hljs-number">5</span><br>        self.pad = <span class="hljs-number">0</span><br>        self.unk = <span class="hljs-number">1</span><br>        self._id2word = [<span class="hljs-string">&#x27;[PAD]&#x27;</span>, <span class="hljs-string">&#x27;[UNK]&#x27;</span>]<br>        self._id2extword = [<span class="hljs-string">&#x27;[PAD]&#x27;</span>, <span class="hljs-string">&#x27;[UNK]&#x27;</span>]<br><br>        self._id2label = []<br>        self.target_names = []<br><br>        self.build_vocab(train_data)<br><br>        reverse = <span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(x, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))))<br>        self._word2id = reverse(self._id2word)<br>        self._label2id = reverse(self._id2label)<br><br>        logging.info(<span class="hljs-string">&quot;Build vocab: words %d, labels %d.&quot;</span> % (self.word_size, self.label_size))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">build_vocab</span>(<span class="hljs-params">self, data</span>):<br>        self.word_counter = Counter()<br><br>        <span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> data[<span class="hljs-string">&#x27;text&#x27;</span>]:<br>            words = text.split()<br>            <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> words:<br>                self.word_counter[word] += <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">for</span> word, count <span class="hljs-keyword">in</span> self.word_counter.most_common():<br>            <span class="hljs-keyword">if</span> count &gt;= self.min_count:<br>                self._id2word.append(word)<br><br>        label2name = &#123;<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;科技&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;股票&#x27;</span>, <span class="hljs-number">2</span>: <span class="hljs-string">&#x27;体育&#x27;</span>, <span class="hljs-number">3</span>: <span class="hljs-string">&#x27;娱乐&#x27;</span>, <span class="hljs-number">4</span>: <span class="hljs-string">&#x27;时政&#x27;</span>, <span class="hljs-number">5</span>: <span class="hljs-string">&#x27;社会&#x27;</span>, <span class="hljs-number">6</span>: <span class="hljs-string">&#x27;教育&#x27;</span>, <span class="hljs-number">7</span>: <span class="hljs-string">&#x27;财经&#x27;</span>,<br>                      <span class="hljs-number">8</span>: <span class="hljs-string">&#x27;家居&#x27;</span>, <span class="hljs-number">9</span>: <span class="hljs-string">&#x27;游戏&#x27;</span>, <span class="hljs-number">10</span>: <span class="hljs-string">&#x27;房产&#x27;</span>, <span class="hljs-number">11</span>: <span class="hljs-string">&#x27;时尚&#x27;</span>, <span class="hljs-number">12</span>: <span class="hljs-string">&#x27;彩票&#x27;</span>, <span class="hljs-number">13</span>: <span class="hljs-string">&#x27;星座&#x27;</span>&#125;<br><br>        self.label_counter = Counter(data[<span class="hljs-string">&#x27;label&#x27;</span>])<br><br>        <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(self.label_counter)):<br>            count = self.label_counter[label]<br>            self._id2label.append(label)<br>            self.target_names.append(label2name[label])<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_pretrained_embs</span>(<span class="hljs-params">self, embfile</span>):<br>        <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(embfile, encoding=<span class="hljs-string">&#x27;utf-8&#x27;</span>) <span class="hljs-keyword">as</span> f:<br>            lines = f.readlines()<br>            items = lines[<span class="hljs-number">0</span>].split()<br>            word_count, embedding_dim = <span class="hljs-built_in">int</span>(items[<span class="hljs-number">0</span>]), <span class="hljs-built_in">int</span>(items[<span class="hljs-number">1</span>])<br><br>        index = <span class="hljs-built_in">len</span>(self._id2extword)<br>        embeddings = np.zeros((word_count + index, embedding_dim))<br>        <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> lines[<span class="hljs-number">1</span>:]:<br>            values = line.split()<br>            self._id2extword.append(values[<span class="hljs-number">0</span>])<br>            vector = np.array(values[<span class="hljs-number">1</span>:], dtype=<span class="hljs-string">&#x27;float64&#x27;</span>)<br>            embeddings[self.unk] += vector<br>            embeddings[index] = vector<br>            index += <span class="hljs-number">1</span><br><br>        embeddings[self.unk] = embeddings[self.unk] / word_count<br>        embeddings = embeddings / np.std(embeddings)<br><br>        reverse = <span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(x, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))))<br>        self._extword2id = reverse(self._id2extword)<br><br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(<span class="hljs-built_in">set</span>(self._id2extword)) == <span class="hljs-built_in">len</span>(self._id2extword)<br><br>        <span class="hljs-keyword">return</span> embeddings<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">word2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._word2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._word2id.get(xs, self.unk)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extword2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._extword2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._extword2id.get(xs, self.unk)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">label2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._label2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._label2id.get(xs, self.unk)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">word_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2word)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">extword_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2extword)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">label_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2label)<br><br><br>vocab = Vocab(train_data)<br><br><span class="hljs-comment"># build module</span><br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Attention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size</span>):<br>        <span class="hljs-built_in">super</span>(Attention, self).__init__()<br>        self.weight = nn.Parameter(torch.Tensor(hidden_size, hidden_size))<br>        self.weight.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.05</span>)<br><br>        self.bias = nn.Parameter(torch.Tensor(hidden_size))<br>        b = np.zeros(hidden_size, dtype=np.float32)<br>        self.bias.data.copy_(torch.from_numpy(b))<br><br>        self.query = nn.Parameter(torch.Tensor(hidden_size))<br>        self.query.data.normal_(mean=<span class="hljs-number">0.0</span>, std=<span class="hljs-number">0.05</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, batch_hidden, batch_masks</span>):<br>        <span class="hljs-comment"># batch_hidden: b x len x hidden_size (2 * hidden_size of lstm)</span><br>        <span class="hljs-comment"># batch_masks:  b x len</span><br><br>        <span class="hljs-comment"># linear</span><br>        key = torch.matmul(batch_hidden, self.weight) + self.bias  <span class="hljs-comment"># b x len x hidden</span><br><br>        <span class="hljs-comment"># compute attention</span><br>        outputs = torch.matmul(key, self.query)  <span class="hljs-comment"># b x len</span><br><br>        masked_outputs = outputs.masked_fill((<span class="hljs-number">1</span> - batch_masks).<span class="hljs-built_in">bool</span>(), <span class="hljs-built_in">float</span>(-<span class="hljs-number">1e32</span>))<br><br>        attn_scores = F.softmax(masked_outputs, dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># b x len</span><br><br>        <span class="hljs-comment"># 对于全零向量，-1e32的结果为 1/len, -inf为nan, 额外补0</span><br>        masked_attn_scores = attn_scores.masked_fill((<span class="hljs-number">1</span> - batch_masks).<span class="hljs-built_in">bool</span>(), <span class="hljs-number">0.0</span>)<br><br>        <span class="hljs-comment"># sum weighted sources</span><br>        batch_outputs = torch.bmm(masked_attn_scores.unsqueeze(<span class="hljs-number">1</span>), key).squeeze(<span class="hljs-number">1</span>)  <span class="hljs-comment"># b x hidden</span><br><br>        <span class="hljs-keyword">return</span> batch_outputs, attn_scores<br><br><br><span class="hljs-comment"># build word encoder</span><br>bert_path = <span class="hljs-string">&#x27;./emb/bert-mini/&#x27;</span><br>dropout = <span class="hljs-number">0.15</span><br><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WordBertEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(WordBertEncoder, self).__init__()<br>        self.dropout = nn.Dropout(dropout)<br><br>        self.tokenizer = WhitespaceTokenizer()<br>        self.bert = BertModel.from_pretrained(bert_path)<br><br>        self.pooled = <span class="hljs-literal">False</span><br>        logging.info(<span class="hljs-string">&#x27;Build Bert encoder with pooled &#123;&#125;.&#x27;</span>.<span class="hljs-built_in">format</span>(self.pooled))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, tokens</span>):<br>        tokens = self.tokenizer.tokenize(tokens)<br>        <span class="hljs-keyword">return</span> tokens<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_bert_parameters</span>(<span class="hljs-params">self</span>):<br>        no_decay = [<span class="hljs-string">&#x27;bias&#x27;</span>, <span class="hljs-string">&#x27;LayerNorm.weight&#x27;</span>]<br>        optimizer_parameters = [<br>            &#123;<span class="hljs-string">&#x27;params&#x27;</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> self.bert.named_parameters() <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay)],<br>             <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0.01</span>&#125;,<br>            &#123;<span class="hljs-string">&#x27;params&#x27;</span>: [p <span class="hljs-keyword">for</span> n, p <span class="hljs-keyword">in</span> self.bert.named_parameters() <span class="hljs-keyword">if</span> <span class="hljs-built_in">any</span>(nd <span class="hljs-keyword">in</span> n <span class="hljs-keyword">for</span> nd <span class="hljs-keyword">in</span> no_decay)],<br>             <span class="hljs-string">&#x27;weight_decay&#x27;</span>: <span class="hljs-number">0.0</span>&#125;<br>        ]<br>        <span class="hljs-keyword">return</span> optimizer_parameters<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, input_ids, token_type_ids</span>):<br>        <span class="hljs-comment"># input_ids: sen_num x bert_len</span><br>        <span class="hljs-comment"># token_type_ids: sen_num  x bert_len</span><br><br>        <span class="hljs-comment"># sen_num x bert_len x 256, sen_num x 256</span><br>        sequence_output, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)<br><br>        <span class="hljs-keyword">if</span> self.pooled:<br>            reps = pooled_output<br>        <span class="hljs-keyword">else</span>:<br>            reps = sequence_output[:, <span class="hljs-number">0</span>, :]  <span class="hljs-comment"># sen_num x 256</span><br><br>        <span class="hljs-keyword">if</span> self.training:<br>            reps = self.dropout(reps)<br><br>        <span class="hljs-keyword">return</span> reps<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">WhitespaceTokenizer</span>():<br>    <span class="hljs-string">&quot;&quot;&quot;WhitespaceTokenizer with vocab.&quot;&quot;&quot;</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        vocab_file = bert_path + <span class="hljs-string">&#x27;vocab.txt&#x27;</span><br>        self._token2id = self.load_vocab(vocab_file)<br>        self._id2token = &#123;v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self._token2id.items()&#125;<br>        self.max_len = <span class="hljs-number">256</span><br>        self.unk = <span class="hljs-number">1</span><br><br>        logging.info(<span class="hljs-string">&quot;Build Bert vocab with size %d.&quot;</span> % (self.vocab_size))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">load_vocab</span>(<span class="hljs-params">self, vocab_file</span>):<br>        f = <span class="hljs-built_in">open</span>(vocab_file, <span class="hljs-string">&#x27;r&#x27;</span>)<br>        lines = f.readlines()<br>        lines = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.strip(), lines))<br>        vocab = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(lines, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(lines))))<br>        <span class="hljs-keyword">return</span> vocab<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize</span>(<span class="hljs-params">self, tokens</span>):<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(tokens) &lt;= self.max_len - <span class="hljs-number">2</span><br>        tokens = [<span class="hljs-string">&quot;[CLS]&quot;</span>] + tokens + [<span class="hljs-string">&quot;[SEP]&quot;</span>]<br>        output_tokens = self.token2id(tokens)<br>        <span class="hljs-keyword">return</span> output_tokens<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">token2id</span>(<span class="hljs-params">self, xs</span>):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(xs, <span class="hljs-built_in">list</span>):<br>            <span class="hljs-keyword">return</span> [self._token2id.get(x, self.unk) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> xs]<br>        <span class="hljs-keyword">return</span> self._token2id.get(xs, self.unk)<br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">vocab_size</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self._id2token)<br><br><br><span class="hljs-comment"># build sent encoder</span><br>sent_hidden_size = <span class="hljs-number">256</span><br>sent_num_layers = <span class="hljs-number">2</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SentEncoder</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, sent_rep_size</span>):<br>        <span class="hljs-built_in">super</span>(SentEncoder, self).__init__()<br>        self.dropout = nn.Dropout(dropout)<br><br>        self.sent_lstm = nn.LSTM(<br>            input_size=sent_rep_size,<br>            hidden_size=sent_hidden_size,<br>            num_layers=sent_num_layers,<br>            batch_first=<span class="hljs-literal">True</span>,<br>            bidirectional=<span class="hljs-literal">True</span><br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, sent_reps, sent_masks</span>):<br>        <span class="hljs-comment"># sent_reps:  b x doc_len x sent_rep_size</span><br>        <span class="hljs-comment"># sent_masks: b x doc_len</span><br><br>        sent_hiddens, _ = self.sent_lstm(sent_reps)  <span class="hljs-comment"># b x doc_len x hidden*2</span><br>        sent_hiddens = sent_hiddens * sent_masks.unsqueeze(<span class="hljs-number">2</span>)<br><br>        <span class="hljs-keyword">if</span> self.training:<br>            sent_hiddens = self.dropout(sent_hiddens)<br><br>        <span class="hljs-keyword">return</span> sent_hiddens<br><br><br><span class="hljs-comment"># build model</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Model</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, vocab</span>):<br>        <span class="hljs-built_in">super</span>(Model, self).__init__()<br>        self.sent_rep_size = <span class="hljs-number">256</span><br>        self.doc_rep_size = sent_hidden_size * <span class="hljs-number">2</span><br>        self.all_parameters = &#123;&#125;<br>        parameters = []<br>        self.word_encoder = WordBertEncoder()<br>        bert_parameters = self.word_encoder.get_bert_parameters()<br><br>        self.sent_encoder = SentEncoder(self.sent_rep_size)<br>        self.sent_attention = Attention(self.doc_rep_size)<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.sent_encoder.parameters())))<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.sent_attention.parameters())))<br><br>        self.out = nn.Linear(self.doc_rep_size, vocab.label_size, bias=<span class="hljs-literal">True</span>)<br>        parameters.extend(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> p: p.requires_grad, self.out.parameters())))<br><br>        <span class="hljs-keyword">if</span> use_cuda:<br>            self.to(device)<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(parameters) &gt; <span class="hljs-number">0</span>:<br>            self.all_parameters[<span class="hljs-string">&quot;basic_parameters&quot;</span>] = parameters<br>        self.all_parameters[<span class="hljs-string">&quot;bert_parameters&quot;</span>] = bert_parameters<br><br>        logging.info(<span class="hljs-string">&#x27;Build model with bert word encoder, lstm sent encoder.&#x27;</span>)<br><br>        para_num = <span class="hljs-built_in">sum</span>([np.prod(<span class="hljs-built_in">list</span>(p.size())) <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> self.parameters()])<br>        logging.info(<span class="hljs-string">&#x27;Model param num: %.2f M.&#x27;</span> % (para_num / <span class="hljs-number">1e6</span>))<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, batch_inputs</span>):<br>        <span class="hljs-comment"># batch_inputs(batch_inputs1, batch_inputs2): b x doc_len x sent_len</span><br>        <span class="hljs-comment"># batch_masks : b x doc_len x sent_len</span><br>        batch_inputs1, batch_inputs2, batch_masks = batch_inputs<br>        batch_size, max_doc_len, max_sent_len = batch_inputs1.shape[<span class="hljs-number">0</span>], batch_inputs1.shape[<span class="hljs-number">1</span>], batch_inputs1.shape[<span class="hljs-number">2</span>]<br>        batch_inputs1 = batch_inputs1.view(batch_size * max_doc_len, max_sent_len)  <span class="hljs-comment"># sen_num x sent_len</span><br>        batch_inputs2 = batch_inputs2.view(batch_size * max_doc_len, max_sent_len)  <span class="hljs-comment"># sen_num x sent_len</span><br>        batch_masks = batch_masks.view(batch_size * max_doc_len, max_sent_len)  <span class="hljs-comment"># sen_num x sent_len</span><br><br>        sent_reps = self.word_encoder(batch_inputs1, batch_inputs2)  <span class="hljs-comment"># sen_num x sent_rep_size</span><br><br>        sent_reps = sent_reps.view(batch_size, max_doc_len, self.sent_rep_size)  <span class="hljs-comment"># b x doc_len x sent_rep_size</span><br>        batch_masks = batch_masks.view(batch_size, max_doc_len, max_sent_len)  <span class="hljs-comment"># b x doc_len x max_sent_len</span><br>        sent_masks = batch_masks.<span class="hljs-built_in">bool</span>().<span class="hljs-built_in">any</span>(<span class="hljs-number">2</span>).<span class="hljs-built_in">float</span>()  <span class="hljs-comment"># b x doc_len</span><br><br>        sent_hiddens = self.sent_encoder(sent_reps, sent_masks)  <span class="hljs-comment"># b x doc_len x doc_rep_size</span><br>        doc_reps, atten_scores = self.sent_attention(sent_hiddens, sent_masks)  <span class="hljs-comment"># b x doc_rep_size</span><br><br>        batch_outputs = self.out(doc_reps)  <span class="hljs-comment"># b x num_labels</span><br><br>        <span class="hljs-keyword">return</span> batch_outputs<br><br><br>model = Model(vocab)<br><br><span class="hljs-comment"># build optimizer</span><br>learning_rate = <span class="hljs-number">2e-4</span><br>bert_lr = <span class="hljs-number">5e-5</span><br>decay = <span class="hljs-number">.75</span><br>decay_step = <span class="hljs-number">1000</span><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamW, get_linear_schedule_with_warmup<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Optimizer</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model_parameters, steps</span>):<br>        self.all_params = []<br>        self.optims = []<br>        self.schedulers = []<br><br>        <span class="hljs-keyword">for</span> name, parameters <span class="hljs-keyword">in</span> model_parameters.items():<br>            <span class="hljs-keyword">if</span> name.startswith(<span class="hljs-string">&quot;basic&quot;</span>):<br>                optim = torch.optim.Adam(parameters, lr=learning_rate)<br>                self.optims.append(optim)<br><br>                l = <span class="hljs-keyword">lambda</span> step: decay ** (step // decay_step)<br>                scheduler = torch.optim.lr_scheduler.LambdaLR(optim, lr_lambda=l)<br>                self.schedulers.append(scheduler)<br>                self.all_params.extend(parameters)<br>            <span class="hljs-keyword">elif</span> name.startswith(<span class="hljs-string">&quot;bert&quot;</span>):<br>                optim_bert = AdamW(parameters, bert_lr, eps=<span class="hljs-number">1e-8</span>)<br>                self.optims.append(optim_bert)<br><br>                scheduler_bert = get_linear_schedule_with_warmup(optim_bert, <span class="hljs-number">0</span>, steps)<br>                self.schedulers.append(scheduler_bert)<br><br>                <span class="hljs-keyword">for</span> group <span class="hljs-keyword">in</span> parameters:<br>                    <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> group[<span class="hljs-string">&#x27;params&#x27;</span>]:<br>                        self.all_params.append(p)<br>            <span class="hljs-keyword">else</span>:<br>                Exception(<span class="hljs-string">&quot;no nameed parameters.&quot;</span>)<br><br>        self.num = <span class="hljs-built_in">len</span>(self.optims)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> optim, scheduler <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.optims, self.schedulers):<br>            optim.step()<br>            scheduler.step()<br>            optim.zero_grad()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">zero_grad</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">for</span> optim <span class="hljs-keyword">in</span> self.optims:<br>            optim.zero_grad()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_lr</span>(<span class="hljs-params">self</span>):<br>        lrs = <span class="hljs-built_in">tuple</span>(<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x.get_lr()[-<span class="hljs-number">1</span>], self.schedulers))<br>        lr = <span class="hljs-string">&#x27; %.5f&#x27;</span> * self.num<br>        res = lr % lrs<br>        <span class="hljs-keyword">return</span> res<br><br><span class="hljs-comment"># build dataset</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sentence_split</span>(<span class="hljs-params">text, vocab, max_sent_len=<span class="hljs-number">256</span>, max_segment=<span class="hljs-number">16</span></span>):<br>    words = text.strip().split()<br>    document_len = <span class="hljs-built_in">len</span>(words)<br><br>    index = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, document_len, max_sent_len))<br>    index.append(document_len)<br><br>    segments = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(index) - <span class="hljs-number">1</span>):<br>        segment = words[index[i]: index[i + <span class="hljs-number">1</span>]]<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(segment) &gt; <span class="hljs-number">0</span><br>        segment = [word <span class="hljs-keyword">if</span> word <span class="hljs-keyword">in</span> vocab._id2word <span class="hljs-keyword">else</span> <span class="hljs-string">&#x27;&lt;UNK&gt;&#x27;</span> <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> segment]<br>        segments.append([<span class="hljs-built_in">len</span>(segment), segment])<br><br>    <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(segments) &gt; <span class="hljs-number">0</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(segments) &gt; max_segment:<br>        segment_ = <span class="hljs-built_in">int</span>(max_segment / <span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> segments[:segment_] + segments[-segment_:]<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> segments<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_examples</span>(<span class="hljs-params">data, word_encoder, vocab, max_sent_len=<span class="hljs-number">256</span>, max_segment=<span class="hljs-number">8</span></span>):<br>    label2id = vocab.label2id<br>    examples = []<br><br>    <span class="hljs-keyword">for</span> text, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(data[<span class="hljs-string">&#x27;text&#x27;</span>], data[<span class="hljs-string">&#x27;label&#x27;</span>]):<br>        <span class="hljs-comment"># label</span><br>        <span class="hljs-built_in">id</span> = label2id(label)<br><br>        <span class="hljs-comment"># words</span><br>        sents_words = sentence_split(text, vocab, max_sent_len-<span class="hljs-number">2</span>, max_segment)<br>        doc = []<br>        <span class="hljs-keyword">for</span> sent_len, sent_words <span class="hljs-keyword">in</span> sents_words:<br>            token_ids = word_encoder.encode(sent_words)<br>            sent_len = <span class="hljs-built_in">len</span>(token_ids)<br>            token_type_ids = [<span class="hljs-number">0</span>] * sent_len<br>            doc.append([sent_len, token_ids, token_type_ids])<br>        examples.append([<span class="hljs-built_in">id</span>, <span class="hljs-built_in">len</span>(doc), doc])<br><br>    logging.info(<span class="hljs-string">&#x27;Total %d docs.&#x27;</span> % <span class="hljs-built_in">len</span>(examples))<br>    <span class="hljs-keyword">return</span> examples<br><br><br><span class="hljs-comment"># build loader</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_slice</span>(<span class="hljs-params">data, batch_size</span>):<br>    batch_num = <span class="hljs-built_in">int</span>(np.ceil(<span class="hljs-built_in">len</span>(data) / <span class="hljs-built_in">float</span>(batch_size)))<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_num):<br>        cur_batch_size = batch_size <span class="hljs-keyword">if</span> i &lt; batch_num - <span class="hljs-number">1</span> <span class="hljs-keyword">else</span> <span class="hljs-built_in">len</span>(data) - batch_size * i<br>        docs = [data[i * batch_size + b] <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(cur_batch_size)]<br><br>        <span class="hljs-keyword">yield</span> docs<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">data_iter</span>(<span class="hljs-params">data, batch_size, shuffle=<span class="hljs-literal">True</span>, noise=<span class="hljs-number">1.0</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    randomly permute data, then sort by source length, and partition into batches</span><br><span class="hljs-string">    ensure that the length of  sentences in each batch</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    batched_data = []<br>    <span class="hljs-keyword">if</span> shuffle:<br>        np.random.shuffle(data)<br><br>        lengths = [example[<span class="hljs-number">1</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> data]<br>        noisy_lengths = [- (l + np.random.uniform(- noise, noise)) <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> lengths]<br>        sorted_indices = np.argsort(noisy_lengths).tolist()<br>        sorted_data = [data[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sorted_indices]<br>    <span class="hljs-keyword">else</span>:<br>        sorted_data = data<br><br>    batched_data.extend(<span class="hljs-built_in">list</span>(batch_slice(sorted_data, batch_size)))<br><br>    <span class="hljs-keyword">if</span> shuffle:<br>        np.random.shuffle(batched_data)<br><br>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> batched_data:<br>        <span class="hljs-keyword">yield</span> batch<br><br><span class="hljs-comment"># some function</span><br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> f1_score, precision_score, recall_score<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_score</span>(<span class="hljs-params">y_ture, y_pred</span>):<br>    y_ture = np.array(y_ture)<br>    y_pred = np.array(y_pred)<br>    f1 = f1_score(y_ture, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>) * <span class="hljs-number">100</span><br>    p = precision_score(y_ture, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>) * <span class="hljs-number">100</span><br>    r = recall_score(y_ture, y_pred, average=<span class="hljs-string">&#x27;macro&#x27;</span>) * <span class="hljs-number">100</span><br><br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">str</span>((reformat(p, <span class="hljs-number">2</span>), reformat(r, <span class="hljs-number">2</span>), reformat(f1, <span class="hljs-number">2</span>))), reformat(f1, <span class="hljs-number">2</span>)<br><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">reformat</span>(<span class="hljs-params">num, n</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">float</span>(<span class="hljs-built_in">format</span>(num, <span class="hljs-string">&#x27;0.&#x27;</span> + <span class="hljs-built_in">str</span>(n) + <span class="hljs-string">&#x27;f&#x27;</span>))<br><br><br><span class="hljs-comment"># build trainer</span><br><br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> classification_report<br><br>clip = <span class="hljs-number">5.0</span><br>epochs = <span class="hljs-number">1</span><br>early_stops = <span class="hljs-number">3</span><br>log_interval = <span class="hljs-number">50</span><br><br>test_batch_size = <span class="hljs-number">16</span><br>train_batch_size = <span class="hljs-number">16</span><br><br>save_model = <span class="hljs-string">&#x27;./bert.bin&#x27;</span><br>save_test = <span class="hljs-string">&#x27;./bert.csv&#x27;</span><br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Trainer</span>():<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, model, vocab</span>):<br>        self.model = model<br>        self.report = <span class="hljs-literal">True</span><br><br>        self.train_data = get_examples(train_data, model.word_encoder, vocab)<br>        self.batch_num = <span class="hljs-built_in">int</span>(np.ceil(<span class="hljs-built_in">len</span>(self.train_data) / <span class="hljs-built_in">float</span>(train_batch_size)))<br>        self.dev_data = get_examples(dev_data, model.word_encoder, vocab)<br>        self.test_data = get_examples(test_data, model.word_encoder, vocab)<br><br>        <span class="hljs-comment"># criterion</span><br>        self.criterion = nn.CrossEntropyLoss()<br><br>        <span class="hljs-comment"># label name</span><br>        self.target_names = vocab.target_names<br><br>        <span class="hljs-comment"># optimizer</span><br>        self.optimizer = Optimizer(model.all_parameters, steps=self.batch_num * epochs)<br><br>        <span class="hljs-comment"># count</span><br>        self.step = <span class="hljs-number">0</span><br>        self.early_stop = -<span class="hljs-number">1</span><br>        self.best_train_f1, self.best_dev_f1 = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        self.last_epoch = epochs<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">train</span>(<span class="hljs-params">self</span>):<br>        logging.info(<span class="hljs-string">&#x27;Start training...&#x27;</span>)<br>        <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, epochs + <span class="hljs-number">1</span>):<br>            train_f1 = self._train(epoch)<br><br>            dev_f1 = self._<span class="hljs-built_in">eval</span>(epoch)<br><br>            <span class="hljs-keyword">if</span> self.best_dev_f1 &lt;= dev_f1:<br>                logging.info(<br>                    <span class="hljs-string">&quot;Exceed history dev = %.2f, current dev = %.2f&quot;</span> % (self.best_dev_f1, dev_f1))<br>                torch.save(self.model.state_dict(), save_model)<br><br>                self.best_train_f1 = train_f1<br>                self.best_dev_f1 = dev_f1<br>                self.early_stop = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">else</span>:<br>                self.early_stop += <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> self.early_stop == early_stops:<br>                    logging.info(<br>                        <span class="hljs-string">&quot;Eearly stop in epoch %d, best train: %.2f, dev: %.2f&quot;</span> % (<br>                            epoch - early_stops, self.best_train_f1, self.best_dev_f1))<br>                    self.last_epoch = epoch<br>                    <span class="hljs-keyword">break</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">test</span>(<span class="hljs-params">self</span>):<br>        self.model.load_state_dict(torch.load(save_model))<br>        self._<span class="hljs-built_in">eval</span>(self.last_epoch + <span class="hljs-number">1</span>, test=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_train</span>(<span class="hljs-params">self, epoch</span>):<br>        self.optimizer.zero_grad()<br>        self.model.train()<br><br>        start_time = time.time()<br>        epoch_start_time = time.time()<br>        overall_losses = <span class="hljs-number">0</span><br>        losses = <span class="hljs-number">0</span><br>        batch_idx = <span class="hljs-number">1</span><br>        y_pred = []<br>        y_true = []<br>        <span class="hljs-keyword">for</span> batch_data <span class="hljs-keyword">in</span> data_iter(self.train_data, train_batch_size, shuffle=<span class="hljs-literal">True</span>):<br>            torch.cuda.empty_cache()<br>            batch_inputs, batch_labels = self.batch2tensor(batch_data)<br>            batch_outputs = self.model(batch_inputs)<br>            loss = self.criterion(batch_outputs, batch_labels)<br>            loss.backward()<br><br>            loss_value = loss.detach().cpu().item()<br>            losses += loss_value<br>            overall_losses += loss_value<br><br>            y_pred.extend(torch.<span class="hljs-built_in">max</span>(batch_outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].cpu().numpy().tolist())<br>            y_true.extend(batch_labels.cpu().numpy().tolist())<br><br>            nn.utils.clip_grad_norm_(self.optimizer.all_params, max_norm=clip)<br>            <span class="hljs-keyword">for</span> optimizer, scheduler <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(self.optimizer.optims, self.optimizer.schedulers):<br>                optimizer.step()<br>                scheduler.step()<br>            self.optimizer.zero_grad()<br><br>            self.step += <span class="hljs-number">1</span><br><br>            <span class="hljs-keyword">if</span> batch_idx % log_interval == <span class="hljs-number">0</span>:<br>                elapsed = time.time() - start_time<br><br>                lrs = self.optimizer.get_lr()<br>                logging.info(<br>                    <span class="hljs-string">&#x27;| epoch &#123;:3d&#125; | step &#123;:3d&#125; | batch &#123;:3d&#125;/&#123;:3d&#125; | lr&#123;&#125; | loss &#123;:.4f&#125; | s/batch &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(<br>                        epoch, self.step, batch_idx, self.batch_num, lrs,<br>                        losses / log_interval,<br>                        elapsed / log_interval))<br><br>                losses = <span class="hljs-number">0</span><br>                start_time = time.time()<br><br>            batch_idx += <span class="hljs-number">1</span><br><br>        overall_losses /= self.batch_num<br>        during_time = time.time() - epoch_start_time<br><br>        <span class="hljs-comment"># reformat</span><br>        overall_losses = reformat(overall_losses, <span class="hljs-number">4</span>)<br>        score, f1 = get_score(y_true, y_pred)<br><br>        logging.info(<br>            <span class="hljs-string">&#x27;| epoch &#123;:3d&#125; | score &#123;&#125; | f1 &#123;&#125; | loss &#123;:.4f&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, score, f1,<br>                                                                                  overall_losses,<br>                                                                                  during_time))<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">set</span>(y_true) == <span class="hljs-built_in">set</span>(y_pred) <span class="hljs-keyword">and</span> self.report:<br>            report = classification_report(y_true, y_pred, digits=<span class="hljs-number">4</span>, target_names=self.target_names)<br>            logging.info(<span class="hljs-string">&#x27;\n&#x27;</span> + report)<br><br>        <span class="hljs-keyword">return</span> f1<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_eval</span>(<span class="hljs-params">self, epoch, test=<span class="hljs-literal">False</span></span>):<br>        self.model.<span class="hljs-built_in">eval</span>()<br>        start_time = time.time()<br>        data = self.test_data <span class="hljs-keyword">if</span> test <span class="hljs-keyword">else</span> self.dev_data<br>        y_pred = []<br>        y_true = []<br>        <span class="hljs-keyword">with</span> torch.no_grad():<br>            <span class="hljs-keyword">for</span> batch_data <span class="hljs-keyword">in</span> data_iter(data, test_batch_size, shuffle=<span class="hljs-literal">False</span>):<br>                torch.cuda.empty_cache()<br>                batch_inputs, batch_labels = self.batch2tensor(batch_data)<br>                batch_outputs = self.model(batch_inputs)<br>                y_pred.extend(torch.<span class="hljs-built_in">max</span>(batch_outputs, dim=<span class="hljs-number">1</span>)[<span class="hljs-number">1</span>].cpu().numpy().tolist())<br>                y_true.extend(batch_labels.cpu().numpy().tolist())<br><br>            score, f1 = get_score(y_true, y_pred)<br><br>            during_time = time.time() - start_time<br><br>            <span class="hljs-keyword">if</span> test:<br>                df = pd.DataFrame(&#123;<span class="hljs-string">&#x27;label&#x27;</span>: y_pred&#125;)<br>                df.to_csv(save_test, index=<span class="hljs-literal">False</span>, sep=<span class="hljs-string">&#x27;,&#x27;</span>)<br>            <span class="hljs-keyword">else</span>:<br>                logging.info(<br>                    <span class="hljs-string">&#x27;| epoch &#123;:3d&#125; | dev | score &#123;&#125; | f1 &#123;&#125; | time &#123;:.2f&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(epoch, score, f1,<br>                                                                                  during_time))<br>                <span class="hljs-keyword">if</span> <span class="hljs-built_in">set</span>(y_true) == <span class="hljs-built_in">set</span>(y_pred) <span class="hljs-keyword">and</span> self.report:<br>                    report = classification_report(y_true, y_pred, digits=<span class="hljs-number">4</span>, target_names=self.target_names)<br>                    logging.info(<span class="hljs-string">&#x27;\n&#x27;</span> + report)<br><br>        <span class="hljs-keyword">return</span> f1<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">batch2tensor</span>(<span class="hljs-params">self, batch_data</span>):<br>        <span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">            [[label, doc_len, [[sent_len, [sent_id0, ...], [sent_id1, ...]], ...]]</span><br><span class="hljs-string">        &#x27;&#x27;&#x27;</span><br>        batch_size = <span class="hljs-built_in">len</span>(batch_data)<br>        doc_labels = []<br>        doc_lens = []<br>        doc_max_sent_len = []<br>        <span class="hljs-keyword">for</span> doc_data <span class="hljs-keyword">in</span> batch_data:<br>            doc_labels.append(doc_data[<span class="hljs-number">0</span>])<br>            doc_lens.append(doc_data[<span class="hljs-number">1</span>])<br>            sent_lens = [sent_data[<span class="hljs-number">0</span>] <span class="hljs-keyword">for</span> sent_data <span class="hljs-keyword">in</span> doc_data[<span class="hljs-number">2</span>]]<br>            max_sent_len = <span class="hljs-built_in">max</span>(sent_lens)<br>            doc_max_sent_len.append(max_sent_len)<br><br>        max_doc_len = <span class="hljs-built_in">max</span>(doc_lens)<br>        max_sent_len = <span class="hljs-built_in">max</span>(doc_max_sent_len)<br><br>        batch_inputs1 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)<br>        batch_inputs2 = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.int64)<br>        batch_masks = torch.zeros((batch_size, max_doc_len, max_sent_len), dtype=torch.float32)<br>        batch_labels = torch.LongTensor(doc_labels)<br><br>        <span class="hljs-keyword">for</span> b <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>            <span class="hljs-keyword">for</span> sent_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(doc_lens[b]):<br>                sent_data = batch_data[b][<span class="hljs-number">2</span>][sent_idx]<br>                <span class="hljs-keyword">for</span> word_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(sent_data[<span class="hljs-number">0</span>]):<br>                    batch_inputs1[b, sent_idx, word_idx] = sent_data[<span class="hljs-number">1</span>][word_idx]<br>                    batch_inputs2[b, sent_idx, word_idx] = sent_data[<span class="hljs-number">2</span>][word_idx]<br>                    batch_masks[b, sent_idx, word_idx] = <span class="hljs-number">1</span><br><br>        <span class="hljs-keyword">if</span> use_cuda:<br>            batch_inputs1 = batch_inputs1.to(device)<br>            batch_inputs2 = batch_inputs2.to(device)<br>            batch_masks = batch_masks.to(device)<br>            batch_labels = batch_labels.to(device)<br><br>        <span class="hljs-keyword">return</span> (batch_inputs1, batch_inputs2, batch_masks), batch_labels<br><br><span class="hljs-comment"># train</span><br>trainer = Trainer(model, vocab)<br>trainer.train()<br><br><span class="hljs-comment"># test</span><br>trainer.test()<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NLP-Assignment4</title>
    <link href="/2022/05/15/NLP-Assignment4/"/>
    <url>/2022/05/15/NLP-Assignment4/</url>
    
    <content type="html"><![CDATA[<h1 id="assignment4讲解">Assignment4讲解</h1><h2 id="rnn和神经网络机器翻译">RNN和神经网络机器翻译</h2><p>机器翻译是指，构建一个系统完成源语言到目标语言的变换映射，比如给定一个源句子（比如西班牙语），输出一个目标句子（比如英语）。本次作业中要实现的是一个带注意力机制的Seq2Seq神经模型，用于构建神经网络机器翻译（NMT）系统。首先我们来看NMT系统的训练过程，它用到了双向LSTM作为编码器（encoder）和单向LSTM作为解码器（decoder）。</p><p><img src="http://ww1.sinaimg.cn/large/0060yMmAly1gsjzdzvwbaj30pv0hzjuy.jpg" referrerpolicy="no-referrer"/></p><p>给定长度为m的源语言句子（source），经过嵌入层，得到输入序列 <spanclass="math inline">\(x_1, x_2, …, x_m \in R^{e \times1}\)</span>，<spanclass="math inline">\(e\)</span>是词向量大小。经过双向Encoder后，得到前向（→）和反向（←）LSTM的隐藏层和神经元状态，将两个方向的状态连接起来得到时间步<span class="math inline">\(i\)</span> 的隐藏状态 <spanclass="math inline">\(h_i^{enc}\)</span> 和 <spanclass="math inline">\(c_i^{enc}\)</span> ： <spanclass="math display">\[\mathbf{h}_{i}^{\text {enc }}=\left[\overleftarrow{\mathbf{h}_{i}^{\text{enc }}} ; \overrightarrow{\mathbf{h}_{i}^{\text {enc }}}\right] \text {where } \mathbf{h}_{i}^{\text {enc }} \in \mathbb{R}^{2 h \times 1},\overleftarrow{\mathbf{h}_{i}^{\text {enc }}},\overrightarrow{\mathbf{h}_{i}^{\text {en }}} \in \mathbb{R}^{h \times1} \quad 1 \leq i \leq m\]</span></p><p><span class="math display">\[\mathbf{c}_{i}^{\text {enc }}=\left[\overleftarrow{\mathbf{c}_{i}^{\text{enc }}} ; \overrightarrow{\mathbf{c}_{i}^{\text {enc }}}\right] \text {where } \mathbf{c}_{i}^{\text {enc }} \in \mathbb{R}^{2 h \times 1},\overleftarrow{\mathbf{c}_{i}^{\text {enc }}},\overrightarrow{\mathbf{c}_{i}^{\text {en }}} \in \mathbb{R}^{h \times1} \quad 1 \leq i \leq m\]</span></p><p>接着我们使用一个线性层来初始化Decoder的初始隐藏、神经元的状态： <spanclass="math display">\[\mathbf{h}_{0}^{\text {dec}}=\mathbf{W}_{h}\left[\overleftarrow{\mathbf{h}_{1}^{\text {enc }}} ;\overrightarrow{\mathbf{h}_{m}^{\text {enc }}}\right] \text { where }\mathbf{h}_{0}^{\text {dec }} \in \mathbb{R}^{h \times 1},\mathbf{W}_{h} \in \mathbb{R}^{h \times 2 h}\]</span></p><p><span class="math display">\[\mathbf{c}_{0}^{\text {dec}}=\mathbf{W}_{c}\left[\overleftarrow{\mathbf{c}_{1}^{\text {enc }}} ;\overrightarrow{\mathbf{c}_{m}^{\text {enc }}}\right] \text { where }\mathbf{c}_{0}^{\text {dec }} \in \mathbb{R}^{h \times 1},\mathbf{W}_{c} \in \mathbb{R}^{h \times 2 h}\]</span></p><p>Decoder的时间步<span class="math inline">\(t\)</span> 的输入为 <spanclass="math inline">\(\bar{y}_t\)</span> ，它由目标语言句子 <spanclass="math inline">\(y_t\)</span>和上一神经元的输出和上一神经元的输出<spanclass="math inline">\(o_{t-1}\)</span>经过连接得到，经过连接得到，<spanclass="math inline">\(o_0\)</span>是0向量，所以 <spanclass="math inline">\(\bar{y}_t \in R^{(e + h) \times 1}\)</span> <spanclass="math display">\[\mathbf{h}_{t}^{\mathrm{dec}},\mathbf{c}_{t}^{\mathrm{dec}}=\operatorname{Decoder}\left(\overline{\mathbf{y}_{t}},\mathbf{h}_{t-1}^{\mathrm{dec}}, \mathbf{c}_{t-1}^{\mathrm{dec}}\right)\text { where } \mathbf{h}_{t}^{\mathrm{dec}} \in \mathbb{R}^{h \times1}, \mathbf{c}_{t}^{\mathrm{dec}} \in \mathbb{R}^{h \times 1}\]</span> 接着我们使用 <span class="math inline">\(h^{dec}_t\)</span>来计算在 <span class="math inline">\(h^{enc}_0, h^{enc}_1, …,h^{enc}_m\)</span> 的乘积注意力（multiplicative attention）： <spanclass="math display">\[\begin{array}{c}\mathbf{e}_{t, i}=\left(\mathbf{h}_{t}^{\mathrm{dec}}\right)^{T}\mathbf{W}_{\text {attProj }} \mathbf{h}_{i}^{\text {enc }} \text {where } \mathbf{e}_{t} \in \mathbb{R}^{m \times 1}, \mathbf{W}_{\text{attProj }} \in \mathbb{R}^{h \times 2 h} \quad 1 \leq i \leq m \\\alpha_{t}=\operatorname{softmax}\left(\mathbf{e}_{t}\right) \text {where } \alpha_{t} \in \mathbb{R}^{m \times 1} \\\mathbf{a}_{t}=\sum_{i=1}^{m} \alpha_{t, i} \mathbf{h}_{i}^{\text {enc}} \text { where } \mathbf{a}_{t} \in \mathbb{R}^{2 h \times 1}\end{array}\]</span> 然后将注意力 <span class="math inline">\(a_t\)</span>和解码器的隐藏状态 <span class="math inline">\(h^{dec}_t\)</span>连接，送入线性层，得到 <em>combined-output</em> 向量 <spanclass="math inline">\(o_t\)</span> <span class="math display">\[\begin{array}{r}\mathbf{u}_{t}=\left[\mathbf{a}_{t} ;\mathbf{h}_{t}^{\mathrm{dec}}\right] \text { where } \mathbf{u}_{t} \in\mathbb{R}^{3 h \times 1} \\\mathbf{v}_{t}=\mathbf{W}_{u} \mathbf{u}_{t} \text { where }\mathbf{v}_{t} \in \mathbb{R}^{h \times 1}, \mathbf{W}_{u} \in\mathbb{R}^{h \times 3 h} \\\mathbf{o}_{t}=\operatorname{Dropout}\left(\tanh\left(\mathbf{v}_{t}\right)\right) \text { where } \mathbf{o}_{t} \in\mathbb{R}^{h \times 1}\end{array}\]</span> 这样以来，目标词的概率分布则为： <span class="math display">\[\mathbf{P}_{t}=\operatorname{softmax}\left(\mathbf{W}_{\text {vocab }}\mathbf{o}_{t}\right) \text { where } \mathbf{P}_{t} \in\mathbb{R}^{V_{t} \times 1}, \mathbf{W}_{\text {vocab }} \in\mathbb{R}^{V_{t} \times h}\]</span> 使用交叉熵做目标函数即可 <span class="math display">\[J_{t}(\theta)=\text { CrossEntropy }\left(\mathbf{P}_{t},\mathbf{g}_{t}\right)\]</span></p><p>代码实现部分，关键在于过程中的向量维度，向量维度匹配没有问题，整个过程的实现就比较清晰。</p><h2 id="part1-神经网络翻译系统代码实现">part1神经网络翻译系统代码实现</h2><h3 id="a-pad_sents">(a) pad_sents</h3><p>In order to apply tensor operations, we must ensure that thesentences in a given batch are of the same length. Thus, we mustidentify the longest sentence in a batch and pad others to be the samelength. Implement the pad sents function in utils.py, which shallproduce these padded sentences.</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pad_sents</span>(<span class="hljs-params">sents, pad_token</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Pad list of sentences according to the longest sentence in the batch.</span><br><span class="hljs-string">    @param sents (list[list[str]]): list of sentences, where each sentence</span><br><span class="hljs-string">                                    is represented as a list of words</span><br><span class="hljs-string">    @param pad_token (str): padding token</span><br><span class="hljs-string">    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter</span><br><span class="hljs-string">        than the max length sentence are padded out with the pad_token, such that</span><br><span class="hljs-string">        each sentences in the batch now has equal length.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    sents_padded = []<br><br>    <span class="hljs-comment"># YOUR CODE HERE (~6 Lines)</span><br>    corpus_size = <span class="hljs-built_in">len</span>(sents)<br>    lens = [<span class="hljs-built_in">len</span>(i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sents]  <span class="hljs-comment"># every sentence&#x27;s length</span><br>    max_lens = <span class="hljs-built_in">max</span>(lens)<br>    sents_padded = [sents[i] + [pad_token] * (max_lens - lens[i]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(corpus_size)]      <span class="hljs-comment"># shape N x max_lens</span><br>    <span class="hljs-comment"># END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> sents_padded<br></code></pre></div></td></tr></table></figure><h3 id="b-modelembeddings">(b) ModelEmbeddings</h3><p>Implement the <code>__init__</code> function in model embeddings.pyto initialize the necessary source and target embeddings.</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ModelEmbeddings</span>(nn.Module): <br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Class that converts input words to their embeddings.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embed_size, vocab</span>):<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Init the Embedding layers.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param embed_size (int): Embedding size (dimensionality)</span><br><span class="hljs-string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span><br><span class="hljs-string">                              See vocab.py for documentation.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(ModelEmbeddings, self).__init__()<br>        self.embed_size = embed_size<br><br>        <span class="hljs-comment"># default values</span><br>        self.source = <span class="hljs-literal">None</span><br>        self.target = <span class="hljs-literal">None</span><br><br>        src_pad_token_idx = vocab.src[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>]<br>        tgt_pad_token_idx = vocab.tgt[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>]<br><br>        <span class="hljs-comment"># YOUR CODE HERE (~2 Lines)</span><br>        <span class="hljs-comment"># TODO - Initialize the following variables:</span><br>        <span class="hljs-comment">#     self.source (Embedding Layer for source language)</span><br>        <span class="hljs-comment">#     self.target (Embedding Layer for target langauge)</span><br>        <span class="hljs-comment">#</span><br>        <span class="hljs-comment"># Note:</span><br>        <span class="hljs-comment">#     1. `vocab` object contains two vocabularies:</span><br>        <span class="hljs-comment">#            `vocab.src` for source</span><br>        <span class="hljs-comment">#            `vocab.tgt` for target</span><br>        <span class="hljs-comment">#     2. You can get the length of a specific vocabulary by running:</span><br>        <span class="hljs-comment">#             `len(vocab.&lt;specific_vocabulary&gt;)`</span><br>        <span class="hljs-comment">#     3. Remember to include the padding token for the specific vocabulary</span><br>        <span class="hljs-comment">#        when creating your Embedding.</span><br>        <span class="hljs-comment">#</span><br>        <span class="hljs-comment"># Use the following docs to properly initialize these variables:</span><br>        <span class="hljs-comment">#     Embedding Layer:</span><br>        <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding</span><br><br>        self.source = nn.Embedding(<span class="hljs-built_in">len</span>(vocab.src), embed_size, padding_idx=src_pad_token_idx)<br>        self.target = nn.Embedding(<span class="hljs-built_in">len</span>(vocab.tgt), embed_size, padding_idx=tgt_pad_token_idx)<br><br>        <span class="hljs-comment"># END YOUR CODE</span><br></code></pre></div></td></tr></table></figure><h3 id="c-nmt">(c) NMT</h3><p>Implement the <code>__init__</code> function in nmt model.py toinitialize the necessary model embeddings (using the ModelEmbeddingsclass from model embeddings.py) and layers (LSTM, projection, anddropout) for the NMT system</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">NMT</span>(nn.Module):<br>    <span class="hljs-string">&quot;&quot;&quot; Simple Neural Machine Translation Model:</span><br><span class="hljs-string">        - Bidrectional LSTM Encoder</span><br><span class="hljs-string">        - Unidirection LSTM Decoder</span><br><span class="hljs-string">        - Global Attention Model (Luong, et al. 2015)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, embed_size, hidden_size, vocab, dropout_rate=<span class="hljs-number">0.2</span></span>):<br>        <span class="hljs-string">&quot;&quot;&quot; Init NMT Model.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param embed_size (int): Embedding size (dimensionality)</span><br><span class="hljs-string">        @param hidden_size (int): Hidden Size (dimensionality)</span><br><span class="hljs-string">        @param vocab (Vocab): Vocabulary object containing src and tgt languages</span><br><span class="hljs-string">                              See vocab.py for documentation.</span><br><span class="hljs-string">        @param dropout_rate (float): Dropout probability, for attention</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-built_in">super</span>(NMT, self).__init__()<br>        self.model_embeddings = ModelEmbeddings(embed_size, vocab)<br>        self.hidden_size = hidden_size<br>        self.dropout_rate = dropout_rate<br>        self.vocab = vocab<br><br>        <span class="hljs-comment"># default values</span><br>        self.encoder = <span class="hljs-literal">None</span> <br>        self.decoder = <span class="hljs-literal">None</span><br>        self.h_projection = <span class="hljs-literal">None</span><br>        self.c_projection = <span class="hljs-literal">None</span><br>        self.att_projection = <span class="hljs-literal">None</span><br>        self.combined_output_projection = <span class="hljs-literal">None</span><br>        self.target_vocab_projection = <span class="hljs-literal">None</span><br>        self.dropout = <span class="hljs-literal">None</span><br><br>        <span class="hljs-comment"># YOUR CODE HERE (~8 Lines)</span><br>        <span class="hljs-comment"># TODO - Initialize the following variables:</span><br>        <span class="hljs-comment">#     self.encoder (Bidirectional LSTM with bias)</span><br>        <span class="hljs-comment">#     self.decoder (LSTM Cell with bias)</span><br>        <span class="hljs-comment">#     self.h_projection (Linear Layer with no bias), called W_&#123;h&#125; in the PDF.</span><br>        <span class="hljs-comment">#     self.c_projection (Linear Layer with no bias), called W_&#123;c&#125; in the PDF.</span><br>        <span class="hljs-comment">#     self.att_projection (Linear Layer with no bias), called W_&#123;attProj&#125; in the PDF.</span><br>        <span class="hljs-comment">#     self.combined_output_projection (Linear Layer with no bias), called W_&#123;u&#125; in the PDF.</span><br>        <span class="hljs-comment">#     self.target_vocab_projection (Linear Layer with no bias), called W_&#123;vocab&#125; in the PDF.</span><br>        <span class="hljs-comment">#     self.dropout (Dropout Layer)</span><br>        <span class="hljs-comment">#</span><br>        <span class="hljs-comment"># Use the following docs to properly initialize these variables:</span><br>        <span class="hljs-comment">#     LSTM:</span><br>        <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM</span><br>        <span class="hljs-comment">#     LSTM Cell:</span><br>        <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell</span><br>        <span class="hljs-comment">#     Linear Layer:</span><br>        <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</span><br>        <span class="hljs-comment">#     Dropout Layer:</span><br>        <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout</span><br><br>        self.encoder = nn.LSTM(embed_size, hidden_size, bias=<span class="hljs-literal">True</span>, bidirectional=<span class="hljs-literal">True</span>)<br>        self.decoder = nn.LSTMCell(embed_size + hidden_size, hidden_size, bias=<span class="hljs-literal">True</span>)<br>        self.h_projection = nn.Linear(hidden_size * <span class="hljs-number">2</span>, hidden_size, bias=<span class="hljs-literal">False</span>) <span class="hljs-comment"># prj output of last h_state of encode (R^2h) to R^h</span><br>        self.c_projection = nn.Linear(hidden_size * <span class="hljs-number">2</span>, hidden_size, bias=<span class="hljs-literal">False</span>)<br>        self.att_projection = nn.Linear(hidden_size * <span class="hljs-number">2</span>, hidden_size, bias=<span class="hljs-literal">False</span>) <span class="hljs-comment"># 1 x 2h (h_encode_i) * 2h x h (W) * h * 1 (h_decode_t) = 1 x 1 = e_t,i</span><br>        self.combined_output_projection = nn.Linear(hidden_size * <span class="hljs-number">3</span>, hidden_size, bias=<span class="hljs-literal">False</span>) <span class="hljs-comment"># use after combined attention output and h_decode</span><br>        self.target_vocab_projection    = nn.Linear(hidden_size, <span class="hljs-built_in">len</span>(vocab.tgt), bias=<span class="hljs-literal">False</span>) <span class="hljs-comment"># for softmax of last</span><br>        self.dropout = nn.Dropout(self.dropout_rate)<br>        <span class="hljs-comment"># END YOUR CODE</span><br><br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, source: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]], target: <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]]</span>) -&gt; torch.Tensor:<br>        <span class="hljs-string">&quot;&quot;&quot; Take a mini-batch of source and target sentences, compute the log-likelihood of</span><br><span class="hljs-string">        target sentences under the language models learned by the NMT system.</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @param source (List[List[str]]): list of source sentence tokens</span><br><span class="hljs-string">        @param target (List[List[str]]): list of target sentence tokens, wrapped by `&lt;s&gt;` and `&lt;/s&gt;`</span><br><span class="hljs-string"></span><br><span class="hljs-string">        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the</span><br><span class="hljs-string">                                    log-likelihood of generating the gold-standard target sentence for</span><br><span class="hljs-string">                                    each example in the input batch. Here b = batch size.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        <span class="hljs-comment"># Compute sentence lengths</span><br>        source_lengths = [<span class="hljs-built_in">len</span>(s) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> source]<br><br>        <span class="hljs-comment"># Convert list of lists into tensors</span><br>        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)   <span class="hljs-comment"># Tensor: (src_len, b)</span><br>        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)   <span class="hljs-comment"># Tensor: (tgt_len, b)</span><br><br>        <span class="hljs-comment">#     Run the network forward:</span><br>        <span class="hljs-comment">#     1. Apply the encoder to `source_padded` by calling `self.encode()`</span><br>        <span class="hljs-comment">#     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`</span><br>        <span class="hljs-comment">#     3. Apply the decoder to compute combined-output by calling `self.decode()`</span><br>        <span class="hljs-comment">#     4. Compute log probability distribution over the target vocabulary using the</span><br>        <span class="hljs-comment">#        combined_outputs returned by the `self.decode()` function.</span><br><br>        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)<br>        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)<br>        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)<br>        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-<span class="hljs-number">1</span>)<br><br>        <span class="hljs-comment"># Zero out, probabilities for which we have nothing in the target text</span><br>        target_masks = (target_padded != self.vocab.tgt[<span class="hljs-string">&#x27;&lt;pad&gt;&#x27;</span>]).<span class="hljs-built_in">float</span>()<br>        <br>        <span class="hljs-comment"># Compute log probability of generating true target words</span><br>        target_gold_words_log_prob = torch.gather(P, index=target_padded[<span class="hljs-number">1</span>:].unsqueeze(-<span class="hljs-number">1</span>), dim=-<span class="hljs-number">1</span>).squeeze(-<span class="hljs-number">1</span>) * target_masks[<span class="hljs-number">1</span>:]<br>        scores = target_gold_words_log_prob.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> scores<br></code></pre></div></td></tr></table></figure><h3 id="d-encode">(d) encode</h3><p>Implement the <code>encode</code> function in nmt model.py. Thisfunction converts the padded source sentences into the tensor<spanclass="math inline">\(X\)</span>, generates<spanclass="math inline">\(h^{enc}_1,...,h^{enc}_m\)</span> , and computesthe initial state<span class="math inline">\(\ h^{dec}_0\)</span>andinitial cell<span class="math inline">\(\ c^{dec}_0\)</span>for theDecoder. You can run a non-comprehensive sanity check byexecuting:<code>python sanity_check.py 1d</code></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, source_padded: torch.Tensor, source_lengths: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">Tuple</span>[torch.Tensor, <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor]]:<br>   <span class="hljs-string">&quot;&quot;&quot; Apply the encoder to source sentences to obtain encoder hidden states.</span><br><span class="hljs-string">       Additionally, take the final states of the encoder and project them to obtain initial states for decoder.</span><br><span class="hljs-string"></span><br><span class="hljs-string">   @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where</span><br><span class="hljs-string">                                   b = batch_size, src_len = maximum source sentence length. Note that </span><br><span class="hljs-string">                                  these have already been sorted in order of longest to shortest sentence.</span><br><span class="hljs-string">   @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch</span><br><span class="hljs-string">   @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where</span><br><span class="hljs-string">                                   b = batch size, src_len = maximum source sentence length, h = hidden size.</span><br><span class="hljs-string">   @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder&#x27;s initial</span><br><span class="hljs-string">                                           hidden state and cell.</span><br><span class="hljs-string">   &quot;&quot;&quot;</span><br>   enc_hiddens, dec_init_state = <span class="hljs-literal">None</span>, <span class="hljs-literal">None</span><br><br>   <span class="hljs-comment"># YOUR CODE HERE (~ 8 Lines)</span><br>   <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span></span><br>   <span class="hljs-comment">#     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.</span><br>   <span class="hljs-comment">#         src_len = maximum source sentence length, b = batch size, e = embedding size. Note</span><br>   <span class="hljs-comment">#         that there is no initial hidden state or cell for the decoder.</span><br>   <span class="hljs-comment">#     2. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.</span><br>   <span class="hljs-comment">#         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.</span><br>   <span class="hljs-comment">#         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.</span><br>   <span class="hljs-comment">#         - Note that the shape of the tensor returned by the encoder is (src_len b, h*2) and we want to</span><br>   <span class="hljs-comment">#           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`.</span><br>   <span class="hljs-comment">#     3. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):</span><br>   <span class="hljs-comment">#         - `init_decoder_hidden`:</span><br>   <span class="hljs-comment">#             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span><br>   <span class="hljs-comment">#             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span><br>   <span class="hljs-comment">#             Apply the h_projection layer to this in order to compute init_decoder_hidden.</span><br>   <span class="hljs-comment">#             This is h_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span><br>   <span class="hljs-comment">#         - `init_decoder_cell`:</span><br>   <span class="hljs-comment">#             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.</span><br>   <span class="hljs-comment">#             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).</span><br>   <span class="hljs-comment">#             Apply the c_projection layer to this in order to compute init_decoder_cell.</span><br>   <span class="hljs-comment">#             This is c_0^&#123;dec&#125; in the PDF. Here b = batch size, h = hidden size</span><br>   <span class="hljs-comment">#</span><br>   <span class="hljs-comment"># See the following docs, as you may need to use some of the following functions in your implementation:</span><br>   <span class="hljs-comment">#     Pack the padded sequence X before passing to the encoder:</span><br>   <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pack_padded_sequence</span><br>   <span class="hljs-comment">#     Pad the packed sequence, enc_hiddens, returned by the encoder:</span><br>   <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_packed_sequence</span><br>   <span class="hljs-comment">#     Tensor Concatenation:</span><br>   <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.cat</span><br>   <span class="hljs-comment">#     Tensor Permute:</span><br>   <span class="hljs-comment">#         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute</span><br>   X = self.model_embeddings.source(source_padded) <span class="hljs-comment"># (src_len, b, e)</span><br>   X = pack_padded_sequence(X, lengths=source_lengths) <span class="hljs-comment"># if feed pack to RNN, it will not calculate output for pad element</span><br>   <span class="hljs-comment"># pack_padded_sequence and pad_packed_sequence example:</span><br>   <span class="hljs-comment"># https://github.com/HarshTrivedi/packing-unpacking-pytorch-minimal-tutorial</span><br>   <span class="hljs-comment"># PackedSequence: Named Tuple with 2 attribute data &amp; batch_size</span><br>   <span class="hljs-comment"># data: shape (batch_sum_len x embed_dim)</span><br>   <span class="hljs-comment"># batch_size: each columns when feed to lstm (max = batch_size (start word of all sentence), min = 1 (only one word in this column))</span><br><br>   <span class="hljs-comment"># After feed PackedSequence to LSTM, return PackedSequence with the same attributes : data &amp; batch_size</span><br>   enc_hiddens, (last_hidden, last_cell) = self.encoder(X)<br>   <span class="hljs-comment"># pad_packed_sequence will unpack PackedSequence, which transform (data &amp; batch_size) -&gt; (max_len, b, h * 2)</span><br>   <span class="hljs-comment"># padded indice will be 0s</span><br>   enc_hiddens, _ = pad_packed_sequence(enc_hiddens)<br>   enc_hiddens = enc_hiddens.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)<span class="hljs-comment"># (b, max_len, h* 2) 维度互换</span><br><br>   last_hidden = torch.cat((last_hidden[<span class="hljs-number">0</span>], last_hidden[<span class="hljs-number">1</span>]), <span class="hljs-number">1</span>)<span class="hljs-comment"># (2, b, h) -&gt; (b, h * 2)</span><br>   init_decoder_hidden = self.h_projection(last_hidden)<br><br>   last_cell = torch.cat((last_cell[<span class="hljs-number">0</span>], last_cell[<span class="hljs-number">1</span>]), <span class="hljs-number">1</span>)<br>   init_decoder_cell = self.c_projection(last_cell)<br><br>   dec_init_state = (init_decoder_hidden, init_decoder_cell)<br>   <span class="hljs-comment"># END YOUR CODE</span><br><br>   <span class="hljs-keyword">return</span> enc_hiddens, dec_init_state<br></code></pre></div></td></tr></table></figure><h3 id="e-decode">(e) decode</h3><p>Implement the <code>decode</code> function in nmt model.py. Thisfunction constructs y¯ and runs the step function over every timestepfor the input. You can run a non-comprehensive sanity check byexecuting:<code>python sanity_check.py 1e</code></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">decode</span>(<span class="hljs-params">self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,</span><br><span class="hljs-params">            dec_init_state: <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor], target_padded: torch.Tensor</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot;Compute combined output vectors for a batch.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where</span><br><span class="hljs-string">                                 b = batch size, src_len = maximum source sentence length, h = hidden size.</span><br><span class="hljs-string">    @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where</span><br><span class="hljs-string">                                 b = batch size, src_len = maximum source sentence length.</span><br><span class="hljs-string">    @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder</span><br><span class="hljs-string">    @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where</span><br><span class="hljs-string">                                   tgt_len = maximum target sentence length, b = batch size. </span><br><span class="hljs-string"></span><br><span class="hljs-string">    @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where</span><br><span class="hljs-string">                                    tgt_len = maximum target sentence length, b = batch_size,  h = hidden size</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-comment"># Chop of the &lt;END&gt; token for max length sentences.</span><br>    target_padded = target_padded[:-<span class="hljs-number">1</span>]<br><br>    <span class="hljs-comment"># Initialize the decoder state (hidden and cell)</span><br>    dec_state = dec_init_state<br><br>    <span class="hljs-comment"># Initialize previous combined output vector o_&#123;t-1&#125; as zero</span><br>    batch_size = enc_hiddens.size(<span class="hljs-number">0</span>)<br>    o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)<br><br>    <span class="hljs-comment"># Initialize a list we will use to collect the combined output o_t on each step</span><br>    combined_outputs = []<br><br>    <span class="hljs-comment"># YOUR CODE HERE (~9 Lines)</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">#     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,</span><br>    <span class="hljs-comment">#         which should be shape (b, src_len, h),</span><br>    <span class="hljs-comment">#         where b = batch size, src_len = maximum source length, h = hidden size.</span><br>    <span class="hljs-comment">#         This is applying W_&#123;attProj&#125; to h^enc, as described in the PDF.</span><br>    <span class="hljs-comment">#     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.</span><br>    <span class="hljs-comment">#         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.</span><br>    <span class="hljs-comment">#     3. Use the torch.split function to iterate over the time dimension of Y.</span><br>    <span class="hljs-comment">#         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.</span><br>    <span class="hljs-comment">#             - Squeeze Y_t into a tensor of dimension (b, e).</span><br>    <span class="hljs-comment">#             - Construct Ybar_t by concatenating Y_t with o_prev.</span><br>    <span class="hljs-comment">#             - Use the step function to compute the the Decoder&#x27;s next (cell, state) values</span><br>    <span class="hljs-comment">#               as well as the new combined output o_t.</span><br>    <span class="hljs-comment">#             - Append o_t to combined_outputs</span><br>    <span class="hljs-comment">#             - Update o_prev to the new o_t.</span><br>    <span class="hljs-comment">#     4. Use torch.stack to convert combined_outputs from a list length tgt_len of</span><br>    <span class="hljs-comment">#         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)</span><br>    <span class="hljs-comment">#         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># Note:</span><br>    <span class="hljs-comment">#    - When using the squeeze() function make sure to specify the dimension you want to squeeze</span><br>    <span class="hljs-comment">#      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># Use the following docs to implement this functionality:</span><br>    <span class="hljs-comment">#     Zeros Tensor:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.zeros</span><br>    <span class="hljs-comment">#     Tensor Splitting (iteration):</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.split</span><br>    <span class="hljs-comment">#     Tensor Dimension Squeezing:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span><br>    <span class="hljs-comment">#     Tensor Concatenation:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.cat</span><br>    <span class="hljs-comment">#     Tensor Stacking:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.stack</span><br><br>    <span class="hljs-comment"># 1,</span><br>    enc_hiddens_proj = self.att_projection(enc_hiddens) <span class="hljs-comment"># enc_hiddens: (b, l, h * 2)  dot (h * 2, h) -&gt; b, l, h</span><br>    <span class="hljs-comment"># 2,</span><br>    Y = self.model_embeddings.target(target_padded) <span class="hljs-comment"># (tgt_len, b, h)</span><br>    <span class="hljs-comment"># 3,</span><br>    <span class="hljs-keyword">for</span> Y_t <span class="hljs-keyword">in</span> torch.split(Y, <span class="hljs-number">1</span>, dim=<span class="hljs-number">0</span>):<br>        squeezed = torch.squeeze(Y_t) <span class="hljs-comment"># shape (b, e)</span><br>        Ybar_t = torch.cat((squeezed, o_prev), dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># shape (b, e + h)</span><br>        dec_state, o_t, _ = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks)<br>        combined_outputs.append(o_t)<br>        o_prev = o_t<br>    <span class="hljs-comment"># 4,</span><br>    combined_outputs = torch.stack(combined_outputs, dim=<span class="hljs-number">0</span>)<br>    <span class="hljs-comment"># END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> combined_outputs<br></code></pre></div></td></tr></table></figure><h3 id="f-step">(f) step</h3><p>Implement the <code>step</code> function in nmt model.py. Thisfunction applies the Decoder’s LSTM cell for a single timestep,computing the encoding of the target word h dec t , the attention scoreset, attention distribution αt, the attention output at, and finally thecombined output ot. You can run a non-comprehensive sanity check byexecuting:<code>python sanity_check.py 1f</code></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">step</span>(<span class="hljs-params">self, Ybar_t: torch.Tensor,</span><br><span class="hljs-params">        dec_state: <span class="hljs-type">Tuple</span>[torch.Tensor, torch.Tensor],</span><br><span class="hljs-params">        enc_hiddens: torch.Tensor,</span><br><span class="hljs-params">        enc_hiddens_proj: torch.Tensor,</span><br><span class="hljs-params">        enc_masks: torch.Tensor</span>) -&gt; <span class="hljs-type">Tuple</span>[<span class="hljs-type">Tuple</span>, torch.Tensor, torch.Tensor]:<br>    <span class="hljs-string">&quot;&quot;&quot; Compute one forward step of the LSTM decoder, including the attention computation.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,</span><br><span class="hljs-string">                            where b = batch size, e = embedding size, h = hidden size.</span><br><span class="hljs-string">    @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.</span><br><span class="hljs-string">            First tensor is decoder&#x27;s prev hidden state, second tensor is decoder&#x27;s prev cell.</span><br><span class="hljs-string">    @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,</span><br><span class="hljs-string">                                src_len = maximum source length, h = hidden size.</span><br><span class="hljs-string">    @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),</span><br><span class="hljs-string">                                where b = batch size, src_len = maximum source length, h = hidden size.</span><br><span class="hljs-string">    @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),</span><br><span class="hljs-string">                                where b = batch size, src_len is maximum source length. </span><br><span class="hljs-string"></span><br><span class="hljs-string">    @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.</span><br><span class="hljs-string">            First tensor is decoder&#x27;s new hidden state, second tensor is decoder&#x27;s new cell.</span><br><span class="hljs-string">    @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.</span><br><span class="hljs-string">    @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.</span><br><span class="hljs-string">                            Note: You will not use this outside of this function.</span><br><span class="hljs-string">                                  We are simply returning this value so that we can sanity check</span><br><span class="hljs-string">                                  your implementation.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    combined_output = <span class="hljs-literal">None</span><br><br>    <span class="hljs-comment"># YOUR CODE HERE (~3 Lines)</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">#     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.</span><br>    <span class="hljs-comment">#     2. Split dec_state into its two parts (dec_hidden, dec_cell)</span><br>    <span class="hljs-comment">#     3. Compute the attention scores e_t, a Tensor shape (b, src_len).</span><br>    <span class="hljs-comment">#        Note: b = batch_size, src_len = maximum source length, h = hidden size.</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment">#       Hints:</span><br>    <span class="hljs-comment">#         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)</span><br>    <span class="hljs-comment">#         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_&#123;attProj&#125; h^enc (batched).</span><br>    <span class="hljs-comment">#         - Use batched matrix multiplication (torch.bmm) to compute e_t.</span><br>    <span class="hljs-comment">#         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.</span><br>    <span class="hljs-comment">#         - When using the squeeze() function make sure to specify the dimension you want to squeeze</span><br>    <span class="hljs-comment">#             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># Use the following docs to implement this functionality:</span><br>    <span class="hljs-comment">#     Batch Multiplication:</span><br>    <span class="hljs-comment">#        https://pytorch.org/docs/stable/torch.html#torch.bmm</span><br>    <span class="hljs-comment">#     Tensor Unsqueeze:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze</span><br>    <span class="hljs-comment">#     Tensor Squeeze:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.squeeze</span><br><br>    <span class="hljs-comment"># 1,</span><br>    dec_state = self.decoder(Ybar_t, dec_state)<br>    (dec_hidden, dec_cell) = dec_state<br>    <span class="hljs-comment"># 3, (b, src_len, h) .dot(b, h, 1) -&gt; (b, src_len, 1) -&gt; (b, src_len)</span><br>    e_t = enc_hiddens_proj.bmm(dec_hidden.unsqueeze(<span class="hljs-number">2</span>)).squeeze(<span class="hljs-number">2</span>)<br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-comment"># Set e_t to -inf where enc_masks has 1</span><br>    <span class="hljs-keyword">if</span> enc_masks <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        e_t.data.masked_fill_(enc_masks.byte(), -<span class="hljs-built_in">float</span>(<span class="hljs-string">&#x27;inf&#x27;</span>)) <span class="hljs-comment"># mask the 0s with -inf, so e^x = 0</span><br><br>    <span class="hljs-comment"># YOUR CODE HERE (~6 Lines)</span><br>    <span class="hljs-comment"># <span class="hljs-doctag">TODO:</span></span><br>    <span class="hljs-comment">#     1. Apply softmax to e_t to yield alpha_t</span><br>    <span class="hljs-comment">#     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the</span><br>    <span class="hljs-comment">#         attention output vector, a_t.</span><br>    <span class="hljs-comment">#     Hints:</span><br>    <span class="hljs-comment">#           - alpha_t is shape (b, src_len)</span><br>    <span class="hljs-comment">#           - enc_hiddens is shape (b, src_len, 2h)</span><br>    <span class="hljs-comment">#           - a_t should be shape (b, 2h)</span><br>    <span class="hljs-comment">#           - You will need to do some squeezing and unsqueezing.</span><br>    <span class="hljs-comment">#     Note: b = batch size, src_len = maximum source length, h = hidden size.</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment">#     3. Concatenate dec_hidden with a_t to compute tensor U_t</span><br>    <span class="hljs-comment">#     4. Apply the combined output projection layer to U_t to compute tensor V_t</span><br>    <span class="hljs-comment">#     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.</span><br>    <span class="hljs-comment">#</span><br>    <span class="hljs-comment"># Use the following docs to implement this functionality:</span><br>    <span class="hljs-comment">#     Softmax:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/nn.html#torch.nn.functional.softmax</span><br>    <span class="hljs-comment">#     Batch Multiplication:</span><br>    <span class="hljs-comment">#        https://pytorch.org/docs/stable/torch.html#torch.bmm</span><br>    <span class="hljs-comment">#     Tensor View:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view</span><br>    <span class="hljs-comment">#     Tensor Concatenation:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.cat</span><br>    <span class="hljs-comment">#     Tanh:</span><br>    <span class="hljs-comment">#         https://pytorch.org/docs/stable/torch.html#torch.tanh</span><br><br>    <span class="hljs-comment"># 1, apply softmax to e_t</span><br>    alpha_t = F.softmax(e_t, dim=<span class="hljs-number">1</span>) <span class="hljs-comment"># (b, src_len)</span><br>    <span class="hljs-comment"># 2, (b, 1, src_len) x (b, src_len, 2h) = (b, 1, 2h) -&gt; (b, 2h)</span><br>    <span class="hljs-comment"># a_t = e_t.unsqueeze(1).bmm(enc_hiddens).squeeze(1)</span><br>    att_view = (alpha_t.size(<span class="hljs-number">0</span>), <span class="hljs-number">1</span>, alpha_t.size(<span class="hljs-number">1</span>))<br>    a_t = torch.bmm(alpha_t.view(*att_view), enc_hiddens).squeeze(<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># 3, concate a_t (b, 2h) and dec_hidden (b, h) to U_t (b, 3h)</span><br>    U_t = torch.cat((a_t, dec_hidden), dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-comment"># 4, apply combined output to U_T -&gt; V_t, shape (b, h)</span><br>    V_t = self.combined_output_projection(U_t)<br>    O_t = self.dropout(torch.tanh(V_t))<br><br>    <span class="hljs-comment"># END YOUR CODE</span><br><br>    combined_output = O_t<br>    <span class="hljs-keyword">return</span> dec_state, combined_output, e_t<br></code></pre></div></td></tr></table></figure><h3 id="g-generate_sent_masks">(g) generate_sent_masks</h3><p>The generate sent masks() function in nmt model.py produces a tensorcalled enc masks. It has shape (batch size, max source sentence length)and contains 1s in positions corresponding to ‘pad’ tokens in the input,and 0s for non-pad tokens. Look at how the masks are used during theattention computation in the step() function (lines 295-296). Firstexplain (in around three sentences) what effect the masks have on theentire attention computation. Then explain (in one or two sentences) whyit is necessary to use the masks in this way.</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">generate_sent_masks</span>(<span class="hljs-params">self, enc_hiddens: torch.Tensor, source_lengths: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; torch.Tensor:<br>    <span class="hljs-string">&quot;&quot;&quot; Generate sentence masks for encoder hidden states.</span><br><span class="hljs-string"></span><br><span class="hljs-string">    @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,</span><br><span class="hljs-string">                                 src_len = max source length, h = hidden size. </span><br><span class="hljs-string">    @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">    @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),</span><br><span class="hljs-string">                                where src_len = max source length, h = hidden size.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    enc_masks = torch.zeros(enc_hiddens.size(<span class="hljs-number">0</span>), enc_hiddens.size(<span class="hljs-number">1</span>), dtype=torch.<span class="hljs-built_in">float</span>)<br>    <span class="hljs-keyword">for</span> e_id, src_len <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(source_lengths):<br>        enc_masks[e_id, src_len:] = <span class="hljs-number">1</span><br>    <span class="hljs-keyword">return</span> enc_masks.to(self.device)<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第八讲-机器翻译、seq2seq与注意力机制</title>
    <link href="/2022/05/14/%E7%AC%AC%E5%85%AB%E8%AE%B2-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E3%80%81seq2seq%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    <url>/2022/05/14/%E7%AC%AC%E5%85%AB%E8%AE%B2-%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E3%80%81seq2seq%E4%B8%8E%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="机器翻译与smt统计机器翻译">1.机器翻译与SMT（统计机器翻译）</h1><h2 id="机器翻译">1.1 机器翻译</h2><p>机器翻译(MT)是将一个句子<spanclass="math inline">\(x\)</span>从一种语言(<strong>源语言</strong>)转换为另一种语言(<strong>目标语言</strong>)的句子<spanclass="math inline">\(y\)</span>的任务。</p><h2 id="s早期机器翻译">1.2 1950s：早期机器翻译</h2><p>机器翻译研究始于20世纪50年代初。</p><ul><li>俄语 → 英语(冷战的推动)</li><li>系统主要是<strong>基于规则</strong>的，使用双语词典来讲俄语单词映射为对应的英语部分</li></ul><h2 id="s-2010s统计机器翻译">1.3 1990s-2010s：统计机器翻译</h2><p><strong>核心想法</strong>：从<strong>数据</strong>中学习<strong>概率模型</strong></p><p>假设我们正在翻译法语 → 英语，对于给定法语句子<spanclass="math inline">\(x\)</span>，我们想要找到<strong>最好的英语句子</strong><spanclass="math inline">\(y\  argmax_yP(y|x)\)</span>，使用Bayes规则将其分解为<strong>两个组件</strong>从而分别学习<span class="math display">\[argmaxx_yP(x|y)P(y)\]</span></p><ul><li><span class="math inline">\(P(x|y)\)</span>：<strong>TranslationModel / 翻译模型</strong><ul><li>分析单词和短语应该如何翻译(逼真)</li><li>从并行数据中学习</li></ul></li><li><span class="math inline">\(P(y)\)</span>：<strong>Language Model /语言模型</strong><ul><li>模型如何写出好英语(流利)</li><li>从单语数据中学习</li></ul></li></ul><h2 id="s-2010s统计机器翻译-1">1.4 1990s-2010s：统计机器翻译</h2><p><strong>问题</strong>：如何学习翻译模型<spanclass="math inline">\(P(x|y)\)</span>？</p><p>首先，需要大量的<strong>并行数据</strong>(例如成对的人工翻译的法语/英语句子)</p><h2 id="smt的学习对齐">1.5SMT的学习对齐</h2><p><strong>问题</strong>：如何从并行语料库中学习翻译模型<spanclass="math inline">\(P(x|y)\)</span>？</p><p>进一步分解：我们实际上想要考虑</p><p><span class="math display">\[P(x,a|y)\]</span></p><ul><li><span class="math inline">\(a\)</span>是对齐</li><li>即法语句子<span class="math inline">\(x\)</span>和英语句子<spanclass="math inline">\(y\)</span>之间的单词级对应</li></ul><h2 id="对齐">1.6 对齐</h2><p>对齐是翻译句子中特定词语之间的对应关系</p><ul><li>注意：有些词没有对应词</li></ul><p><img src="/img/nlp/第八讲/1.png" /></p><h2 id="对齐是复杂的">1.7 对齐是复杂的</h2><p>对齐可以是多对一的</p><p><img src="/img/nlp/第八讲/2.png" /></p><p>对齐可以是一对多的</p><p><img src="/img/nlp/第八讲/3.png" /></p><p>有些词很丰富</p><p>对齐可以是<strong>多对多</strong>(短语级)</p><p>我们学习很多因素的组合，包括</p><ul><li>特定单词对齐的概率(也取决于发送位置)</li><li>特定单词具有特定多词对应的概率(对应单词的数量)</li></ul><h2 id="smt的学习对齐-1">1.8 SMT的学习对齐</h2><ul><li>问题：如何计算argmax<ul><li>我们可以列举所有可能的<spanclass="math inline">\(y\)</span>并计算概率？→ 计算成本太高</li></ul></li><li>回答：使用启发式搜索算法搜索最佳翻译，丢弃概率过低的假设<ul><li>这个过程称为<strong>解码</strong></li></ul></li></ul><h2 id="smt解码">1.9 SMT解码</h2><p><img src="/img/nlp/第八讲/4.png" /></p><p><img src="/img/nlp/第八讲/5.png" /></p><h2 id="s-2010s统计机器翻译-2">1.10 1990s-2010s：统计机器翻译</h2><p>SMT是一个<strong>巨大的研究领域</strong></p><p>最好的系统非常复杂</p><ul><li>数以百计的重要细节我们还没有提到</li><li>系统有许多<strong>独立设计子组件工程</strong></li><li>大量特征工程<ul><li>很多功能需要设计特性来获取特定的语言现象</li></ul></li><li>需要编译和维护额外的资源<ul><li>比如双语短语对应表</li></ul></li><li>需要大量的人力来维护<ul><li>对于每一对语言都需要重复操作</li></ul></li></ul><h1 id="神经网络机器翻译">2.神经网络机器翻译</h1><h2 id="神经机器翻译neural-machine-translationnmt">2.1神经机器翻译Neural Machine Translation(NMT)</h2><p><strong>神经机器翻译</strong>(NMT)是利用单个神经网络进行机器翻译的一种方法</p><p>神经网络架构称为 <strong>sequence-to-sequence</strong>(又名seq2seq)，它包含两个RNNs</p><ul><li>编码器RNN生成源语句的编码</li><li>源语句的编码为解码器RNN提供初始隐藏状态</li><li>解码器RNN是一种以编码为条件生成目标句的语言模型</li><li><strong>注意</strong>：此图显示了测试时行为 →解码器输出作为下一步的输入</li></ul><p><img src="/img/nlp/第八讲/6.png" /></p><h2 id="sequence-to-sequence是多功能的">2.2Sequence-to-sequence是多功能的！</h2><p>序列到序列不仅仅对机器翻译有用</p><p>许多NLP任务可以按照顺序进行表达</p><ul><li><strong>摘要</strong>(长文本 → 短文本)</li><li><strong>对话</strong>(前一句话 → 下一句话)</li><li><strong>解析</strong>(输入文本 → 输出解析为序列)</li><li><strong>代码生成</strong>(自然语言 → Python代码)</li></ul><h2 id="神经机器翻译nmt">2.3 神经机器翻译(NMT)</h2><p><strong>sequence-to-sequence</strong>模型是条件语言模型的一个例子</p><ul><li>语言模型(Language Model)，因为解码器正在预测目标句的下一个单词<spanclass="math inline">\(y\)</span></li><li>条件约束的(Conditional)，因为预测也取决于源句<spanclass="math inline">\(x\)</span></li></ul><p>NMT直接计算<span class="math inline">\(P(y|x)\)</span>： <spanclass="math display">\[P(y \mid x)=P\left(y_{1} \mid x\right) P\left(y_{2} \mid y_{1}, x\right)P\left(y_{3} \mid y_{1}, y_{2}, x\right) \ldots P\left(y_{T} \mid y_{1},\ldots, y_{T-1}, x\right)\]</span></p><ul><li>上式中最后一项为，给定到目前为止的目标词和源句<spanclass="math inline">\(x\)</span>，下一个目标词的概率</li></ul><p><strong>问题</strong>：如何训练NMT系统？</p><p><strong>回答</strong>：找一个大的平行语料库</p><h2 id="训练一个机器翻译系统">2.4 训练一个机器翻译系统</h2><p>Seq2seq被优化为一个单一的系统。反向传播运行在“端到端”中</p><p><img src="/img/nlp/第八讲/7.png" /></p><h1 id="机器翻译解码">3.机器翻译解码</h1><h2 id="贪婪解码">3.1 贪婪解码</h2><p>我们了解了如何生成(或“解码”)目标句，通过对解码器的每个步骤使用argmax</p><p>这是<strong>贪婪解码</strong>(每一步都取最可能的单词)</p><p><strong>这种方法有问题吗</strong>？</p><p><img src="/img/nlp/第八讲/8.png" /></p><h2 id="贪婪解码的问题">3.2 贪婪解码的问题</h2><p>贪婪解码没有办法撤销决定</p><p><img src="/img/nlp/第八讲/9.png" /></p><p>如何修复？</p><h2 id="穷举搜索解码">3.3 穷举搜索解码</h2><p>理想情况下，我们想要找到一个(长度为<spanclass="math inline">\(T\)</span>)的翻译<spanclass="math inline">\(y\)</span>使其最大化 <span class="math display">\[\begin{aligned}P(y \mid x) &amp;=P\left(y_{1} \mid x\right) P\left(y_{2} \mid y_{1},x\right) P\left(y_{3} \mid y_{1}, y_{2}, x\right) \ldots, P\left(y_{T}\mid y_{1}, \ldots, y_{T-1}, x\right) \\&amp;=\prod_{t=1}^{T} P\left(y_{t} \mid y_{1}, \ldots, y_{t-1}, x\right)\end{aligned}\]</span></p><p>我们可以尝试计算所有可能的序列 - 这意味着在解码器的每一步<spanclass="math inline">\(t\)</span>，我们跟踪<spanclass="math inline">\(V^t\)</span>个可能的部分翻译，其中<spanclass="math inline">\(V\)</span>是vocab 大小 - 这种<spanclass="math inline">\(O(V^t)\)</span>的复杂性<strong>太昂贵</strong>了！</p><h2 id="集束搜索解码">3.4 集束搜索解码</h2><ul><li>核心思想：在解码器的每一步，跟踪个最可能的部分翻译(我们称之为假设[hypotheses] )<ul><li><spanclass="math inline">\(k\)</span>是Beam的大小(实际中大约是5到10)</li></ul></li><li>假设<spanclass="math inline">\(y_1,y_2,...,y_t\)</span>有一个<strong>分数</strong>，即它的对数概率</li></ul><p><span class="math display">\[\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\logP_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} \mid x\right)=\sum_{i=1}^{t}\log P_{\mathrm{LM}}\left(y_{i} \mid y_{1}, \ldots, y_{i-1}, x\right)\]</span></p><ul><li><p>分数都是负数，分数越高越好</p></li><li><p>我们寻找得分较高的假设，跟踪每一步的 top k 个部分翻译</p></li><li><p>波束搜索 <strong>不一定能</strong> 找到最优解</p></li><li><p>但比穷举搜索<strong>效率高得多</strong></p></li></ul><h2 id="集束搜索解码示例">3.5 集束搜索解码：示例</h2><p>Beam size = k = 2，蓝色的数字是 <span class="math display">\[\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\sum_{i=1}^{t}\log P_{\mathrm{LM}}\left(y_{i} \mid y_{1}, \ldots, y_{i-1}, x\right)\]</span> <img src="/img/nlp/第八讲/10.png" /></p><ul><li>计算下一个单词的概率分布</li><li>取前个单词并计算分数<ul><li>对于每一次的<spanclass="math inline">\(k\)</span>个假设，找出最前面的<spanclass="math inline">\(k\)</span>个单词并计算分数</li><li>在的假设中，保留个最高的分值<ul><li><span class="math inline">\(t=2\)</span>时，保留分数最高的<code>hit</code> 和 <code>was</code></li><li><span class="math inline">\(t=3\)</span>时，保留分数最高的<code>a</code> 和 <code>me</code></li><li><span class="math inline">\(t=4\)</span>时，保留分数最高的<code>pie</code> 和 <code>with</code></li><li><span class="math inline">\(t=5\)</span>时，保留分数最高的<code>a</code> 和 <code>one</code></li><li><span class="math inline">\(t=6\)</span>时，保留分数最高的<code>pie</code></li></ul></li></ul></li><li>这是最高得分的假设</li><li>回溯以获得完整的假设</li></ul><h2 id="集束搜索解码停止判据">3.6 集束搜索解码：停止判据</h2><ul><li>在贪心解码中，我们通常解码到模型产生一个令牌<ul><li>例如： he hit me with a pie</li></ul></li><li>在集束搜索解码中，不同的假设可能在<strong>不同的时间步长</strong>上产生令牌<ul><li>当一个假设生成了令牌，该假设<strong>完成</strong></li><li><strong>把它放在一边</strong>，通过 Beam Search继续探索其他假设</li></ul></li><li>通常我们继续进行 Beam Search ，直到<ul><li>我们到达时间步长<span class="math inline">\(T\)</span>(其中<spanclass="math inline">\(T\)</span>是预定义截止点)</li><li>我们至少有<spanclass="math inline">\(n\)</span>个已完成的假设(其中<spanclass="math inline">\(n\)</span>是预定义截止点)</li></ul></li></ul><h2 id="集束搜索解码完成">3.7 集束搜索解码：完成</h2><p>我们有完整的假设列表,如何选择得分最高的？</p><p>我们清单上的每个假设<spanclass="math inline">\(y_1,...,y_t\)</span>都有一个分数 <spanclass="math display">\[\operatorname{score}\left(y_{1}, \ldots, y_{t}\right)=\logP_{\mathrm{LM}}\left(y_{1}, \ldots, y_{t} \mid x\right)=\sum_{i=1}^{t}\log P_{\mathrm{LM}}\left(y_{i} \mid y_{1}, \ldots, y_{i-1}, x\right)\]</span> <strong>问题在于</strong> ：较长的假设得分较低</p><p><strong>修正</strong>：按长度标准化。用下式来选择top one <spanclass="math display">\[\frac{1}{t} \sum_{i=1}^{t} \log P_{\mathrm{LM}}\left(y_{i} \mid y_{1},\ldots, y_{i-1}, x\right)\]</span></p><h2 id="神经机器翻译nmt的优点">3.8 神经机器翻译(NMT)的优点</h2><p>与SMT相比，NMT有很多<strong>优点</strong></p><p>更好的性能</p><ul><li>更流利</li><li>更好地使用上下文</li><li>更好地使用短语相似性</li></ul><p>单个神经网络端到端优化</p><ul><li>没有子组件需要单独优化</li></ul><p>需要更少的人类工程付出</p><ul><li>无特征工程</li><li>所有语言对的方法相同</li></ul><h2 id="神经机器翻译nmt的缺点">3.9 神经机器翻译(NMT)的缺点</h2><p>SMT相比，NMT的<strong>缺点</strong></p><p>NMT的可解释性较差</p><ul><li>难以调试</li></ul><p>NMT很难控制</p><ul><li>例如，不能轻松指定翻译规则或指南</li><li>安全问题</li></ul><h1 id="机器翻译评估">4.机器翻译评估</h1><h2 id="如何评估机器翻译质量">4.1 如何评估机器翻译质量</h2><p>BLEU(Bilingual Evaluation Understudy)</p><ul><li>你将会在 Assignment 4 中看到BLEU的细节</li></ul><p>BLEU将机器翻译和人工翻译(一个或多个)，并计算一个相似的分数</p><ul><li>n-gram 精度 (n通常为1-4)</li><li>对过于短的机器翻译的加上惩罚</li></ul><p>BLEU很有用，但不完美</p><ul><li>有很多有效的方法来翻译一个句子</li><li>所以一个<strong>好的</strong>翻译可以得到一个糟糕的BLEUscore，因为它与人工翻译的n-gram重叠较低</li></ul><h2 id="机器翻译问题完美解决了吗">4.2 机器翻译问题完美解决了吗？</h2><p>没有！</p><p>许多困难仍然存在</p><ul><li><strong>词表外</strong>的单词处理</li><li>训练和测试数据之间的<strong>领域不匹配</strong></li><li>在较长文本上维护上下文</li><li><strong>资源较低</strong>的语言对</li></ul><p>使用常识仍然很难</p><ul><li>NMT在训练数据中发现偏差</li><li>无法解释的系统会做一些奇怪的事情</li></ul><h1 id="注意力机制">5.注意力机制</h1><h2 id="sequence-to-sequence瓶颈问题">5.1Sequence-to-sequence：瓶颈问题</h2><ul><li>源语句的编码</li><li>需要捕获关于源语句的所有信息</li><li>信息瓶颈！</li></ul><p><img src="/img/nlp/第八讲/11.png" /></p><h2 id="注意力">5.2 注意力</h2><p><strong>注意力</strong>为瓶颈问题提供了一个解决方案</p><p><strong>核心理念</strong>：在解码器的每一步，使用与<strong>编码器的直接连接</strong>来专注于源序列的<strong>特定部分</strong></p><p>首先我们将通过图表展示(没有方程)，然后我们将用方程展示</p><h2 id="带注意力机制的序列到序列模型">5.3带注意力机制的序列到序列模型</h2><ul><li>将解码器部分的第一个token 与源语句中的每一个时间步的隐藏状态进行 DotProduct 得到每一时间步的分数</li></ul><p><img src="/img/nlp/第八讲/15.png" /></p><p><img src="/img/nlp/第八讲/16.png" /></p><ul><li>通过softmax将分数转化为概率分布</li><li>在这个解码器时间步长上，我们主要关注第一个编码器隐藏状态(“he”)</li></ul><p><img src="/img/nlp/第八讲/17.png" /></p><ul><li>利用<strong>注意力分布</strong>对编码器的隐藏状态进行<strong>加权求和</strong></li><li>注意力输出主要包含来自于受到<strong>高度关注</strong>的<strong>隐藏状态</strong>的信息</li></ul><p><img src="/img/nlp/第八讲/18.png" /></p><p>连接的<strong>注意力输出</strong>与<strong>解码器隐藏状态</strong>，然后用来计算<span class="math inline">\(\hat{y_1}\)</span></p><p><img src="/img/nlp/第八讲/19.png" /></p><p>有时，我们从前面的步骤中提取注意力输出，并将其输入解码器(连同通常的解码器输入)。我们在作业4中做这个。</p><p><img src="/img/nlp/第八讲/20.png" /></p><p><img src="/img/nlp/第八讲/21.png" /></p><p><img src="/img/nlp/第八讲/12.png" /></p><h2 id="注意力公式">5.4 注意力：公式</h2><p>我们有编码器隐藏状态<span class="math inline">\(h_{1}, \ldots, h_{N}\in \mathbb{R}^{h}\)</span></p><p>在时间步<spanclass="math inline">\(t\)</span>上，我们有解码器隐藏状态<spanclass="math inline">\(s_{t} \in \mathbb{R}^{h}\)</span></p><p>我们得到这一步的注意分数 <span class="math display">\[e^{t}=\left[s_{t}^{T} \boldsymbol{h}_{1}, \ldots, \boldsymbol{s}_{t}^{T}\boldsymbol{h}_{N}\right] \in \mathbb{R}^{N}\]</span> 我们使用softmax得到这一步的注意分布<spanclass="math inline">\(\alpha^t\)</span>(这是一个概率分布，和为1) <spanclass="math display">\[\alpha^{t}=\operatorname{softmax}\left(e^{t}\right) \in \mathbb{R}^{N}\]</span> 我们使用<spanclass="math inline">\(\alpha^t\)</span>来获得编码器隐藏状态的加权和，得到注意力输出<spanclass="math inline">\(\alpha^{t} \boldsymbol{a}_{t}=\sum_{i=1}^{N}\alpha_{i}^{t} \boldsymbol{h}_{i} \in \mathbb{R}^{h}\)</span></p><p>最后，我们将注意输出<spanclass="math inline">\(\alpha^t\)</span>与解码器隐藏状态连接起来，并按照非注意seq2seq 模型继续进行 <span class="math display">\[\left[\boldsymbol{a}_{t} ; \boldsymbol{s}_{t}\right] \in \mathbb{R}^{2h}\]</span></p><h2 id="注意力很棒">5.5 注意力很棒！</h2><p>注意力显著提高了NMT性能</p><ul><li>这是非常有用的，让解码器专注于某些部分的源语句</li></ul><p>注意力解决瓶颈问题</p><ul><li>注意力允许解码器直接查看源语句；绕过瓶颈</li></ul><p>注意力帮助消失梯度问题</p><ul><li>提供了通往遥远状态的捷径</li></ul><p>注意力提供了一些可解释性</p><ul><li>通过检查注意力的分布，我们可以看到解码器在关注什么</li><li>我们可以免费得到(软)对齐</li><li>这很酷，因为我们从来没有明确训练过对齐系统</li><li>网络只是自主学习了对齐</li></ul><h2 id="注意力是一种普遍的深度学习技巧">5.6注意力是一种普遍的深度学习技巧</h2><p>我们已经看到，注意力是改进机器翻译的序列到序列模型的一个很好的方法</p><p>然而：你可以在<strong>许多结构</strong>(不仅仅是seq2seq)和许多任务(不仅仅是MT)中使用注意力</p><p>我们有时说 <strong>query attends to the values</strong></p><p>例如，在seq2seq +attention模型中，每个解码器的隐藏状态(查询)关注所有编码器的隐藏状态(值)</p><h2 id="注意力是一种普遍的深度学习技巧-1">5.7注意力是一种普遍的深度学习技巧</h2><p>注意力的更一般定义</p><ul><li>给定一组向量<strong>值</strong>和一个向量<strong>查询</strong>，注意力是一种根据查询，计算值的加权和的技术</li></ul><p>直觉</p><ul><li>加权和是值中包含的信息的<strong>选择性汇总</strong>，查询在其中确定要关注哪些值</li><li>注意是一种获取<strong>任意一组表示(值)的固定大小表示的方法</strong>，依赖于其他一些表示(查询)。</li></ul><h2 id="有几种注意力的变体">5.8 有几种注意力的变体</h2><p><img src="/img/nlp/第八讲/13.png" /></p><p><img src="/img/nlp/第八讲/14.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第七讲-梯度消失问题与RNN变种</title>
    <link href="/2022/05/13/%E7%AC%AC%E4%B8%83%E8%AE%B2-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%E4%B8%8ERNN%E5%8F%98%E7%A7%8D/"/>
    <url>/2022/05/13/%E7%AC%AC%E4%B8%83%E8%AE%B2-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98%E4%B8%8ERNN%E5%8F%98%E7%A7%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="梯度消失">1.梯度消失</h1><h2 id="梯度消失问题">1.1 梯度消失问题</h2><p>梯度消失问题：当这些梯度很小的时候，反向传播的越深入，梯度信号就会变得越来越小</p><p><img src="/img/nlp/第七讲/1.png" /></p><h2 id="梯度消失证明简述">1.2 梯度消失证明简述</h2><p><span class="math display">\[\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h}\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}\boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\]</span></p><p>因此通过<strong>链式法则</strong>得到： <span class="math display">\[\frac{\partial \boldsymbol{h}^{(t)}}{\partial\boldsymbol{h}^{(t-1)}}=\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h}\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}\boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right) \boldsymbol{W}_{h}\]</span> 考虑第<spanclass="math inline">\(i\)</span>步上的损失梯度<spanclass="math inline">\(J^{(i)}(\theta)\)</span>，相对于第<spanclass="math inline">\(j\)</span>步上的隐藏状态<spanclass="math inline">\(h^{(j)}\)</span>。如果权重矩阵<spanclass="math inline">\(W_h\)</span>很小，那么这一项也会随着<spanclass="math inline">\(i\)</span>和<spanclass="math inline">\(j\)</span>的距离越来越远而变得越来越小</p><p>考虑矩阵的 L2 范数 <span class="math display">\[\left\|\frac{\partial J^{(i)}(\theta)}{\partial\boldsymbol{h}^{(j)}}\right\| \leq\left\|\frac{\partialJ^{(i)}(\theta)}{\partial\boldsymbol{h}^{(i)}}\right\|\left\|\boldsymbol{W}_{h}\right\|^{(i-j)}\prod_{j&lt;t \leqi}\left\|\operatorname{diag}\left(\sigma^{\prime}\left(\boldsymbol{W}_{h}\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}\boldsymbol{x}^{(t)}+\boldsymbol{b}_{1}\right)\right)\right\|\]</span> Pascanu et al 表明，如果<spanclass="math inline">\(W_h\)</span>的<strong>最大特征值</strong>&lt;1，梯度<spanclass="math inline">\(\left\|\frac{\partial J^{(i)}(\theta)}{\partial\boldsymbol{h}^{(j)}}\right\|\)</span>将呈<strong>指数衰减</strong></p><ul><li>这里的界限是1因为我们使用的非线性函数是sigmoid</li></ul><p>有一个类似的证明将一个<strong>最大的特征值</strong> &gt;1与<strong>梯度爆炸</strong>联系起来</p><h2 id="为什么梯度消失是个问题">1.3 为什么梯度消失是个问题？</h2><p>来自远处的梯度信号会丢失，因为它比来自近处的梯度信号小得多。因此，模型权重只会根据近期效应而不是长期效应进行更新。</p><p><img src="/img/nlp/第七讲/2.png" /></p><p>另一种解释：<strong>梯度</strong>可以被看作是<strong>过去对未来的影响</strong>的衡量标准</p><p>如果梯度在较长一段距离内(从时间步<spanclass="math inline">\(t\)</span>到<spanclass="math inline">\(t+n\)</span>)变得越来越小，那么我们就不能判断：</p><ul><li>在数据中，步骤<span class="math inline">\(t\)</span>和<spanclass="math inline">\(t+n\)</span>之间<strong>没有依赖关系</strong></li><li>我们用<strong>错误的参数</strong>来捕获<spanclass="math inline">\(t\)</span>和<spanclass="math inline">\(t+n\)</span>之间的真正依赖关系</li></ul><h2 id="梯度消失对rnn语言模型的影响">1.4梯度消失对RNN语言模型的影响</h2><p><code>LM task: When she tried to print her tickets, she found that the printer was out of toner. She went to the stationery store to buy more toner. It was very overpriced. After installing the toner into the printer, she finally printed her ________</code></p><p>为了从这个训练示例中学习，RNN-LM需要对第7步的 <code>tickets</code>和最后的目标单词 <code>tickets</code>之间的<strong>依赖关系建模</strong></p><p>但是如果梯度很小，模型就<strong>不能学习这种依赖关系</strong></p><ul><li>因此模型无法在测试时<strong>预测类似的长距离依赖关系</strong></li></ul><p><img src="/img/nlp/第七讲/3.png" /></p><p>Correct answer: The writer of the books <u>is</u> planning asequel</p><ul><li><strong>语法近因</strong>：The <u>writer</u> of the books <u>is</u>（正确）</li><li><strong>顺序近因</strong>：The writer of the <u>books</u> <u>is</u>（不正确）</li></ul><p>由于梯度的消失，RNN-LMs更善于从<strong>顺序近因</strong>学习而不是<strong>语法近因</strong>，所以他们犯这种错误的频率比我们希望的要高[Linzenet al . 2016]</p><h2 id="如何解决梯度消失问题">1.5 如何解决梯度消失问题？</h2><p>主要问题是RNN很难学习在多个时间步长的情况下保存信息，在普通的RNN中，隐藏状态不断被重写。<span class="math display">\[\boldsymbol{h}^{(t)}=\sigma\left(\boldsymbol{W}_{h}\boldsymbol{h}^{(t-1)}+\boldsymbol{W}_{x}\boldsymbol{x}^{(t)}+\boldsymbol{b}\right)\]</span> 有没有更好结构的RNN？</p><h1 id="梯度爆炸">2.梯度爆炸</h1><h2 id="为什么梯度爆炸是个问题">2.1 为什么梯度爆炸是个问题？</h2><p>如果梯度过大，则SGD更新步骤过大，这可能导致<strong>错误的更新</strong>：我们更新的太多，导致错误的参数配置(损失很大)。</p><p>在最坏的情况下，这将导致网络中的 <strong>Inf</strong> 或<strong>NaN</strong>(然后你必须从较早的检查点重新启动训练)</p><h2 id="梯度剪裁梯度爆炸的解决方案">2.2梯度剪裁：梯度爆炸的解决方案</h2><p><strong>梯度裁剪</strong>：如果梯度的范数大于某个阈值，在应用SGD更新之前将其缩小</p><p><img src="/img/nlp/第七讲/4.png" /></p><p><strong>直觉</strong>：朝着同样的方向迈出一步，但要小一点</p><p>这显示了一个简单RNN的损失面(隐藏层状态是一个标量不是一个向量)，悬崖是危险的，因为有陡坡</p><p><img src="/img/nlp/第七讲/5.png" /></p><p>在左边，由于陡坡，梯度下降有<strong>两个非常大的步骤</strong>，导致攀登悬崖然后向右射击(都是<strong>坏的更新</strong>)</p><p>在右边，梯度剪裁减少了这些步骤的大小，所以参数调整不会有剧烈的波动</p><h1 id="长短时记忆网络lstm">3.长短时记忆网络(LSTM)</h1><h2 id="长短时记忆lstm">3.1 长短时记忆(LSTM)</h2><p>Hochreiter和Schmidhuber在1997年提出了一种RNN，用于解决梯度消失问题。</p><p>在第<spanclass="math inline">\(t\)</span>步，有一个<strong>隐藏状态</strong><spanclass="math inline">\(h^{(t)}\)</span>和一个<strong>单元状态</strong><spanclass="math inline">\(c^{(t)}\)</span></p><ul><li>都是长度为<span class="math inline">\(n\)</span>的向量</li><li>单元存储长期信息</li><li>LSTM可以从单元中<strong>擦除</strong>、<strong>写入</strong>和<strong>读取信息</strong></li></ul><p>信息被 擦除 / 写入 / 读取 的选择由三个对应的门控制</p><ul><li>门也是长度为<span class="math inline">\(n\)</span>的向量</li><li>在每个时间步长上，门的每个元素可以<strong>打开</strong>(1)、<strong>关闭</strong>(0)或介于两者之间</li><li><strong>门是动态的</strong>：它们的值是基于当前上下文计算的</li></ul><p><img src="/img/nlp/第七讲/6.png" /></p><p>我们有一个输入序列<spanclass="math inline">\(x^{(t)}\)</span>，我们将计算一个隐藏状态<spanclass="math inline">\(h^{(t)}\)</span>和单元状态<spanclass="math inline">\(c^{(t)}\)</span>的序列。在时间步<spanclass="math inline">\(t\)</span>时</p><ul><li><p><strong>遗忘门</strong>：控制上一个单元状态的保存与遗忘</p></li><li><p><strong>输入门</strong>：控制写入单元格的新单元内容的哪些部分</p></li><li><p><strong>输出门</strong>：控制单元的哪些内容输出到隐藏状态</p></li><li><p><strong>新单元内容</strong>：这是要写入单元的新内容</p></li><li><p><strong>单元状态</strong>：删除(“忘记”)上次单元状态中的一些内容，并写入(“输入”)一些新的单元内容</p></li><li><p><strong>隐藏状态</strong>：从单元中读取(“output”)一些内容</p></li><li><p><strong>Sigmoid函数</strong>：所有的门的值都在0到1之间</p></li><li><p>通过逐元素的乘积来应用门</p></li><li><p>这些是长度相同(<spanclass="math inline">\(n\)</span>)的向量</p></li></ul><p><img src="/img/nlp/第七讲/7.png" /></p><p><img src="/img/nlp/第七讲/8.png" /></p><h2 id="lstm如何解决梯度消失">3.2 LSTM如何解决梯度消失</h2><p>RNN的LSTM架构更容易保存许多时间步上的信息</p><ul><li>如果忘记门设置为记得每一时间步上的所有信息，那么单元中的信息被无限地保存</li><li>相比之下，普通RNN更难学习重复使用并且在隐藏状态中保存信息的矩阵<spanclass="math inline">\(W_h\)</span></li></ul><p>LSTM并不保证没有<strong>梯度消失/爆炸</strong>，但它确实为模型提供了一种更容易的方法来学习远程依赖关系</p><h2 id="lstms现实世界的成功">3.3 LSTMs：现实世界的成功</h2><p>2013-2015年，LSTM开始实现最先进的结果</p><ul><li>成功的任务包括：手写识别、语音识别、机器翻译、解析、图像字幕</li><li><strong>LSTM成为主导方法</strong></li></ul><p>现在(2019年)，其他方法(<strong>如Transformers</strong>)在某些任务上变得更加主导</p><ul><li>例如在WMT(a MT conference + competition)中</li><li>在2016年WMT中，总结报告包含“RNN”44次</li><li>在2018年WMT中，总结报告包含“RNN”9次，“Transformers” 63次</li></ul><h1 id="gru网络">4.GRU网络</h1><h2 id="gated-recurrent-unitsgru">4.1 Gated Recurrent Units(GRU)</h2><p>Cho等人在2014年提出了LSTM的一个更简单的替代方案，在每个时间步<spanclass="math inline">\(t\)</span>上，我们都有输入<spanclass="math inline">\(x^{(t)}\)</span>和隐藏状态<spanclass="math inline">\(h^{(t)}\)</span>(没有单元状态)</p><ul><li><p><strong>更新门</strong>：控制隐藏状态的哪些部分被更新，或者被保留</p></li><li><p><strong>重置门</strong>：控制之前隐藏状态的哪些部分被用于计算新内容</p></li><li><p><strong>新的隐藏状态内容</strong>：重置门选择之前隐藏状态的有用部分。使用这一部分和当前输入来计算新的隐藏状态内容</p></li><li><p><strong>隐藏状态</strong>：更新门同时控制从以前的隐藏状态保留的内容，以及更新到新的隐藏状态内容的内容</p></li></ul><p>这如何解决消失梯度？</p><ul><li>与LSTM类似，GRU使长期保存信息变得更容易(例如，将updategate设置为0)</li></ul><p><img src="/img/nlp/第七讲/9.png" /></p><h2 id="lstm-vs-gru">4.2 LSTM vs GRU</h2><p>研究人员提出了许多门控RNN变体，其中LSTM和GRU的应用最为广泛</p><p>最大的区别是<strong>GRU计算速度更快</strong>，参数更少。没有确凿的证据表明其中一个总是比另一个表现得更好。<strong>LSTM</strong>是一个<strong>很好的默认选择</strong>(特别是当你的数据具有非常长的依赖关系，或者你有很多训练数据时)</p><p><strong>经验法则</strong>：从LSTM开始，但是如果你想要更有效率，就切换到GRU</p><h2 id="梯度消失爆炸只是rnn问题吗">4.3 梯度消失/爆炸只是RNN问题吗？</h2><p><strong>梯度消失/爆炸只是RNN问题吗</strong>？</p><p>并不是，这对于所有的神经结构(包括前馈和卷积网络)都是一个问题，尤其是对于深度结构</p><ul><li>由于链式法则/选择非线性函数，反向传播时梯度可以变得很小很小</li><li>因此，较低层次的学习非常缓慢(难以训练)</li><li>解决方案：大量新的深层前馈 /卷积架构，<strong>添加更多的直接连接</strong>(从而使梯度可以流动)</li></ul><p>例如：</p><ul><li>残差连接又名“ResNet”,也称为跳转连接</li><li>默认情况下，标识连接保存信息</li><li>这使得深层网络更容易训练</li></ul><p>例如：</p><ul><li>密集连接又名“DenseNet”</li><li>直接将所有内容连接到所有内容</li></ul><p>例如：</p><ul><li>Highway连接又称“高速网络”</li><li>类似于残差连接，但标识连接与转换层由动态门控制</li><li>灵感来自LSTMs，但适用于深度前馈/卷积网络</li></ul><p>结论：虽然梯度消失/爆炸是一个普遍的问题，但由于重复乘以相同的权矩阵，RNN尤其不稳定[Bengioet al, 1994]</p><h2 id="双向rnn动机">4.4 双向RNN：动机</h2><p><img src="/img/nlp/第七讲/10.png" /></p><p>我们可以把这种隐藏状态看作是这个句子中单词“terribly”的一种表示。我们称之为上下文表示。</p><p>这些上下文表示只包含关于左上下文的信息(例如“the movie was”)。</p><p>那么正确的上下文呢?</p><ul><li>在这个例子中，“exciting”在右上下文中，它修饰了“terribly”的意思(从否定变为肯定)</li></ul><p><img src="/img/nlp/第七讲/11.png" /></p><p>“terribly”的上下文表示同时具有左上下文和右上下文</p><p><img src="/img/nlp/第七讲/12.png" /></p><p>这是一个表示“计算RNN的一个向前步骤”的通用符号——它可以是普通的、LSTM或GRU计算</p><p>我们认为这是一个双向RNN的“隐藏状态”。这就是我们传递给网络下一部分的东西</p><p>一般来说，这两个RNNs有各自的权重</p><p><img src="/img/nlp/第七讲/13.png" /></p><p>双向箭头表示双向性，所描述的隐藏状态是正向+反向状态的连接</p><p>注意：双向RNNs只适用于访问整个输入序列的情况</p><ul><li>它们不适用于语言建模，因为在LM中，你只有左侧的上下文可用</li></ul><p>如果你有完整的输入序列(例如任何一种编码)，<strong>双向性是强大的</strong>(默认情况下你应该使用它)</p><p>例如，BERT(来自transformer的双向编码器表示)是一个基于双向性的强大的预训练的上下文表示系统</p><ul><li>你会在课程的后面学到更多关于BERT的知识!</li></ul><h2 id="深层rnn">4.5 深层RNN</h2><p>RNNs在一个维度上已经是“deep”(它们展开到许多时间步长)</p><p>我们还可以通过应用<strong>多个RNN</strong>使它们“深入”到另一个维度：这是一个多层RNN</p><p><strong>较低的RNN</strong>应该计算<strong>较低级别的特性</strong>，而<strong>较高的RNN</strong>应该计算<strong>较高级别的特性</strong></p><p>多层RNN也称为堆叠RNN</p><p><img src="/img/nlp/第七讲/14.png" /></p><p>RNN层<span class="math inline">\(i\)</span>的隐藏状态是RNN层<spanclass="math inline">\(i+1\)</span>的输入</p><h2 id="深层rnn在实践中的应用">4.6 深层RNN在实践中的应用</h2><p>高性能的RNNs通常是多层的(但没有卷积或前馈网络那么深)</p><p>例如：在2017年的一篇论文，Britz et al发现在神经机器翻译中，2到4层RNN编码器是最好的,和4层RNN解码器</p><ul><li>但是，<strong>skip-connections</strong> /<strong>dense-connections</strong> 需要训练更深RNNs(例如8层)</li><li>RNN无法并行化，计算代价过大，所以不会过深</li></ul><p>Transformer-based 的网络(如BERT)可以多达24层</p><ul><li>BERT 有很多skipping-like的连接</li></ul><h2 id="总结">4.7 总结</h2><ul><li>LSTM功能强大，但GRU速度更快</li><li>剪裁你的梯度</li><li>尽可能使用双向性</li><li>多层RNN功能强大，但如果很深可能需要跳接/密集连接</li></ul><p><img src="/img/nlp/第七讲/15.png" /></p><p>参考：</p><p>http://www.showmeai.tech/article-detail/241</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第六讲-循环神经网络与语言模型</title>
    <link href="/2022/05/12/%E7%AC%AC%E5%85%AD%E8%AE%B2-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/05/12/%E7%AC%AC%E5%85%AD%E8%AE%B2-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="语言模型">1.语言模型</h1><h2 id="语言模型-1">1.1 语言模型</h2><p>1、<strong>语言建模</strong>的任务是预测下一个单词是什么</p><p>更正式的说法是：给定一个单词序列<spanclass="math inline">\(x^{(1)},x^{(2)},...,x^{(t)}\)</span>，计算下一个单词<spanclass="math inline">\(x^{(t+1)}\)</span>的概率分布： <spanclass="math display">\[P(x^{(t+1)}| x^{(t)}, ..., x^{(1)})\]</span></p><ul><li>其中，<spanclass="math inline">\(x^{(t+1)}\)</span>可以是词表中的任意单词<spanclass="math inline">\(V={w_1,...,w_v}\)</span></li><li>这样做的系统称为 Language Model 语言模型</li></ul><p>2、还可以将语言模型看作<strong>评估一段文本是自然句子（通顺度）的概率</strong></p><p>例如，如果我们有一段文本<spanclass="math inline">\(x^{(1)},x^{(2)},...,x^{(T)}\)</span>，则这段文本的概率(根据语言模型)为<span class="math display">\[\begin{aligned}P\left(\boldsymbol{x}^{(1)}, \ldots, \boldsymbol{x}^{(T)}\right)&amp;=P\left(\boldsymbol{x}^{(1)}\right) \timesP\left(\boldsymbol{x}^{(2)} \mid \boldsymbol{x}^{(1)}\right) \times\cdots \times P\left(\boldsymbol{x}^{(T)} \mid \boldsymbol{x}^{(T-1)},\ldots, \boldsymbol{x}^{(1)}\right) \\&amp;=\prod_{t=1}^{T} P\left(\boldsymbol{x}^{(t)} \mid\boldsymbol{x}^{(t-1)}, \ldots, \boldsymbol{x}^{(1)}\right)\end{aligned}\]</span></p><ul><li>语言模型提供的是<span class="math inline">\(\prod_{t=1}^{T}P\left(\boldsymbol{x}^{(t)} \mid \boldsymbol{x}^{(t-1)}, \ldots,\boldsymbol{x}^{(1)}\right)\)</span></li></ul><h2 id="n-gram-语言模型">1.2 n-gram 语言模型</h2><figure class="highlight fortran"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs fortran">the students <span class="hljs-keyword">opened</span> their __<br></code></pre></div></td></tr></table></figure><p><strong>问题</strong>：如何学习一个语言模型？</p><p><strong>回答</strong>(深度学习之前的时期)：学习一个 n-gram语言模型</p><p><strong>定义</strong>：n-gram是一个由<spanclass="math inline">\(n\)</span>个连续单词组成的块</p><ul><li><strong>uni</strong>grams: <code>the</code>, <code>students</code>,<code>opened</code>, <code>their</code></li><li><strong>bi</strong>grams: <code>the students</code>,<code>students opened</code>, <code>opened their</code></li><li><strong>tri</strong>grams: <code>the students opened</code>,<code>students opened their</code></li><li><strong>4-</strong>grams:<code>the students opened their</code></li></ul><p><strong>想法</strong>：收集关于不同 n-gram出现频率的统计数据，并使用这些数据预测下一个单词</p><p>首先，我们做一个简化假设：<spanclass="math inline">\(x^{(t+1)}\)</span>只依赖于前面的<spanclass="math inline">\(n-1\)</span>个单词 <span class="math display">\[P\left(\boldsymbol{x}^{(t+1)} \mid \boldsymbol{x}^{(t)}, \ldots,\boldsymbol{x}^{(1)}\right)=P(\boldsymbol{x}^{(t+1)} \mid\overbrace{\left.\boldsymbol{x}^{(t)}, \ldots,\boldsymbol{x}^{(t-n+2)}\right)}^{n-1 \text { words }}\]</span> <strong>问题</strong>：如何得到n-gram和(n-1)-gram的概率？</p><p><strong>回答</strong>：通过在一些大型文本语料库中计算它们(统计近似)<span class="math display">\[\approx \frac{\operatorname{count}\left(\boldsymbol{x}^{(t+1)},\boldsymbol{x}^{(t)}, \ldots,\boldsymbol{x}^{(t-n+2)}\right)}{\operatorname{count}\left(\boldsymbol{x}^{(t)},\ldots, \boldsymbol{x}^{(t-n+2)}\right)}\]</span></p><h2 id="n-gram-语言模型示例">1.3 n-gram 语言模型：示例</h2><p>假设我们正在学习一个 <strong>4-gram</strong> 的语言模型</p><p><code>as the proctor started the clock, the students opened their ____</code>我们只需要考虑<code>students opened their _____</code></p><ul><li>例如，假设在语料库中：<ul><li><code>students opened their</code> 出现了1000次</li><li><code>students opened their books</code> 出现了400次</li></ul></li></ul><p><span class="math display">\[P(books|students \ opened \ their) = 0.4\]</span></p><ul><li><code>students opened their exams</code> 出现了100次</li></ul><p><span class="math display">\[P(exams|students \ opened \ their) = 0.1\]</span></p><ul><li>我们应该忽视上下文中的proctor吗？<ul><li>在本例中，上下文里出现了 <code>proctor</code>，所以<code>exams</code> 在这里的上下文中应该是比 <code>books</code>概率更大的。</li></ul></li></ul><h2 id="n-gram语言模型的稀疏性问题">1.4 n-gram语言模型的稀疏性问题</h2><p><strong>问题1</strong>：如果<code>students open their ww</code>从未出现在数据中，那么概率值为0</p><ul><li>(Partial)<strong>解决方案</strong>：为每个<spanclass="math inline">\(w\in V\)</span>添加极小数<spanclass="math inline">\(\delta\)</span>，这叫做平滑。这使得词表中的每个单词都至少有很小的概率。</li></ul><p><strong>问题2</strong>：如果<code>students open their</code>从未出现在数据中，那么我们将无法计算任何单词<spanclass="math inline">\(w\)</span>的概率值</p><ul><li><p>(Partial)<strong>解决方案</strong>：将条件改为<code>open their</code>，也叫做后退处理。</p></li><li><p>Note/注意: <spanclass="math inline">\(n\)</span>的增加使稀疏性问题变得更糟。一般情况下<spanclass="math inline">\(n\)</span>不能大于5。</p></li></ul><h2 id="n-gram语言模型的存储问题">1.5 n-gram语言模型的存储问题</h2><p><strong>问题</strong>：需要存储你在语料库中看到的所有 n-grams的计数</p><p>增加<spanclass="math inline">\(n\)</span>或增加语料库都会增加模型大小</p><h2 id="n-gram语言模型的生成文本">1.6 n-gram语言模型的生成文本</h2><ul><li><p>可以使用语言模型来生成文本</p></li><li><p>使用trigram运行以上生成过程时，会得到上图左侧的文本</p></li><li><p>令人惊讶的是其具有语法但是是不连贯的。如果我们想要很好地模拟语言，我们需要同时考虑三个以上的单词。但增加<spanclass="math inline">\(n\)</span>使模型的稀疏性问题恶化，模型尺寸增大</p></li></ul><h2 id="如何搭建一个神经语言模型">1.7 如何搭建一个神经语言模型？</h2><p>回忆一下语言模型任务</p><ul><li><strong>输入</strong>：单词序列<span class="math inline">\(x^{(1)},x^{(2)}, ...,x^{(t)}\)</span></li><li><strong>输出</strong>：下一次单词的概率<spanclass="math inline">\(P\left(\boldsymbol{x}^{(t+1)} \mid\boldsymbol{x}^{(t)}, \ldots, \boldsymbol{x}^{(1)}\right)\)</span></li></ul><p><img src="/img/nlp/第六讲/1.png" /></p><h2 id="固定窗口的神经语言模型">1.8 固定窗口的神经语言模型</h2><p><img src="/img/nlp/第六讲/2.png" /></p><p>超越 n-gram 语言模型的<strong>改进</strong></p><ul><li>没有稀疏性问题</li><li>不需要观察到所有的n-grams</li></ul><p>NNLM存在的<strong>问题</strong></p><ul><li>固定窗口太小</li><li>扩大窗口就需要扩大权重矩阵<spanclass="math inline">\(W\)</span></li><li>窗口再大也不够用</li><li><span class="math inline">\(x^{(1)}\)</span>和<spanclass="math inline">\(x^{(2)}\)</span>乘以完全不同的权重。输入的处理不对称</li></ul><p>我们需要一个神经结构，可以处理任何长度的输入</p><h1 id="循环神经网络rnn">2.循环神经网络(RNN)</h1><h2 id="循环神经网络rnn-1">2.1 循环神经网络(RNN)</h2><p><img src="/img/nlp/第六讲/3.png" /></p><p><img src="/img/nlp/第六讲/4.png" /></p><ul><li>RNN的优点<ul><li>可以处理<strong>任意长度</strong>的输入</li><li>步骤<spanclass="math inline">\(t\)</span>的计算(理论上)可以使用<strong>许多步骤前</strong>的信息</li><li><strong>模型大小不会</strong>随着输入的增加而<strong>增加</strong></li><li>在每个时间步上应用相同的权重，因此在处理输入时具有<strong>对称性</strong></li></ul></li><li>RNN的缺点<ul><li>循环串行计算速度慢</li><li>在实践中，很难从许多步骤前返回信息</li></ul></li></ul><h2 id="训练一个rnn语言模型">2.2 训练一个RNN语言模型</h2><p>获取一个<strong>较大的文本语料库</strong>，该语料库是一个单词序列</p><p>输入RNN-LM；计算每个步骤的输出分布</p><ul><li>即预测到目前为止给定的每个单词的概率分布</li></ul><p>步骤<spanclass="math inline">\(t\)</span>上的<strong>损失函</strong>数为预测概率分布<spanclass="math inline">\(\hat{y}^{(t)}\)</span>与真实下一个单词<spanclass="math inline">\(y^{(t)}\)</span>(<spanclass="math inline">\(x^{(t+1)}\)</span>的独热向量)之间的<strong>交叉熵</strong><span class="math display">\[J^{(t)}(\theta)=C E\left(\boldsymbol{y}^{(t)},\hat{\boldsymbol{y}}^{(t)}\right)=-\sum_{w \in V}\boldsymbol{y}_{w}^{(t)} \log \hat{\boldsymbol{y}}_{w}^{(t)}=-\log\hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\]</span> 将其平均，得到整个训练集的<strong>总体损失</strong> <spanclass="math display">\[J(\theta)=\frac{1}{T} \sum_{t=1}^{T} J^{(t)}(\theta)=\frac{1}{T}\sum_{t=1}^{T}-\log \hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\]</span> <img src="/img/nlp/第六讲/5.png" /></p><p>然而：计算整个语料库<spanclass="math inline">\(x^{(1)},...,x^{(T)}\)</span>的损失和梯度太昂贵了，所以在实践中通常把<spanclass="math inline">\(x^{(1)},...,x^{(T)}\)</span>看成一个句子或是文档</p><p>回忆：随机梯度下降允许我们计算小块数据的损失和梯度，并进行更新</p><p>计算一个句子的损失<spanclass="math inline">\(J(\theta)\)</span>(实际上是一批句子)，计算梯度和更新权重。重复上述操作。</p><h2 id="rnn的反向传播">2.3 RNN的反向传播</h2><p><strong>问题</strong>：关于重复的权重矩阵<spanclass="math inline">\(W_h\)</span>的偏导数<spanclass="math inline">\(J^{(t)}(\theta)\)</span></p><p><strong>回答</strong>：重复权重的梯度是每次其出现时的梯度的总和 <spanclass="math display">\[\frac{\partial J^{(t)}}{\partial\boldsymbol{W}_{\boldsymbol{h}}}=\left.\sum_{i=1}^{t} \frac{\partialJ^{(t)}}{\partial \boldsymbol{W}_{\boldsymbol{h}}}\right|_{(i)}\]</span> <img src="/img/nlp/第六讲/6.png" /></p><p><strong>问题</strong>：如何计算？</p><p><strong>回答</strong>：反向传播的时间步长<spanclass="math inline">\(i=t,...,0\)</span>。累加梯度。这个算法叫做“backpropagation through time”</p><h2 id="rnn语言模型的生成文本">2.4 RNN语言模型的生成文本</h2><p>就像n-gram语言模型一样，你可以使用RNN语言模型通过重复采样来生成文本。采样输出是下一步的输入。</p><p><img src="/img/nlp/第六讲/7.png" /></p><h1 id="评估语言模型">3.评估语言模型</h1><h2 id="评估语言模型-1">3.1 评估语言模型</h2><p>标准语言模型评估指标是 perplexity 困惑度这等于交叉熵损失<spanclass="math inline">\(J(\theta)\)</span>的指数 <spanclass="math display">\[=\prod_{t=1}^{T}\left(\frac{1}{\hat{y}_{x_{t+1}}^{(t)}}\right)^{1 /T}=\exp \left(\frac{1}{T} \sum_{t=1}^{T}-\log\hat{\boldsymbol{y}}_{\boldsymbol{x}_{t+1}}^{(t)}\right)=\exp(J(\theta))\]</span> 困惑度越低效果越好</p><h2 id="rnn极大地改善了困惑度">3.2 RNN极大地改善了困惑度</h2><p>语言模型是一项基准测试任务，它帮助我们衡量我们在理解语言方面的进展</p><ul><li>生成下一个单词，需要语法，句法，逻辑，推理，现实世界的知识等</li></ul><p>语言建模是许多NLP任务的子组件，尤其是那些涉及生成文本或估计文本概率的任务</p><ul><li>预测性打字、语音识别、手写识别、拼写/语法纠正、作者识别、机器翻译、摘要、对话等等</li></ul><h2 id="要点回顾">3.3 要点回顾</h2><p><strong>语言模型</strong>（LM: LanguageModel）：预测下一个单词的系统</p><p>循环神经网络：一系列神经网络</p><ul><li>采用任意长度的顺序输入</li><li>在每一步上应用相同的权重</li><li>可以选择在每一步上生成输出</li></ul><p>循环神经网络<spanclass="math inline">\(\ne\)</span>语言模型，我们已经证明，RNNs是构建LM的一个很好的方法，但RNNs的用处要大得多!</p><h2 id="rnn可用于打标签">3.4 RNN可用于打标签</h2><p><img src="/img/nlp/第六讲/8.png" /></p><h2 id="rnn可用于句子分类">3.5 RNN可用于句子分类</h2><p>如何计算句子编码</p><p><strong>基础方式</strong>：使用最终隐层状态</p><p><img src="/img/nlp/第六讲/9.png" /></p><p>通常更好的方式：使用所有隐层状态的逐元素最值或均值，Encoder的结构在NLP中非常常见</p><p><img src="/img/nlp/第六讲/10.png" /></p><h2 id="rnn语言模型可用于生成文本">3.6 RNN语言模型可用于生成文本</h2><p>这是一个条件语言模型的示例。我们使用语言模型组件，并且最关键的是，我们根据条件来调整它</p><p><img src="/img/nlp/第六讲/11.png" /></p><p>参考：</p><p>http://www.showmeai.tech/article-detail/240</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第五讲-句法分析与依存解析</title>
    <link href="/2022/05/12/%E7%AC%AC%E4%BA%94%E8%AE%B2-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BE%9D%E5%AD%98%E8%A7%A3%E6%9E%90/"/>
    <url>/2022/05/12/%E7%AC%AC%E4%BA%94%E8%AE%B2-%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E4%B8%8E%E4%BE%9D%E5%AD%98%E8%A7%A3%E6%9E%90/</url>
    
    <content type="html"><![CDATA[<h1 id="句法结构成分与依赖">1.句法结构：成分与依赖</h1><h2 id="语言结构的两种观点无上下文语法">1.1语言结构的两种观点：无上下文语法</h2><p>句子是使用逐步嵌套的单元构建的</p><ul><li>可以通过CFG（context-free grammars 无上下文语法）规则表示语法</li></ul><p><strong>起步单元</strong>：单词被赋予一个类别（part of speech = pos词性）</p><ul><li>the, cat, cuddly, by, door</li><li>Det, N, Adj, P, N</li></ul><p><strong>单词</strong>按类别组合成<strong>短语</strong></p><ul><li>the cuddly cat ：NP =&gt; Det Adj N</li><li>by the door ：PP =&gt;P N P</li></ul><p><strong>短语</strong>可以递归地组合成<strong>更大的短语</strong></p><ul><li>the cuddly cat by the door：=&gt; NP → NP PP</li></ul><p>补充：</p><ul><li><strong>Det</strong> 指的是<code>Determiner</code>，在语言学中的含义为 <strong>限定词</strong></li><li><strong>NP</strong> 指的是<code>Noun Phrase</code>，在语言学中的含义为<strong>名词短语</strong></li><li><strong>VP</strong> 指的是<code>Verb Phrase</code>，在语言学中的含义为<strong>动词短语</strong></li><li><strong>P</strong> 指的是<code>Preposition</code>，在语言学中的含义为 <strong>介词</strong></li><li><strong>PP</strong> 指的是<code>Prepositional Phrase</code>，在语言学中的含义为<strong>介词短语</strong></li></ul><p><img src="/img/nlp/第五讲/1.png" /></p><h2 id="语言结构的两种观点依赖结构">1.2语言结构的两种观点：依赖结构</h2><p>不是使用各种类型的短语，而是直接通过单词与其他的单词关系表示句子的结构，显示哪些单词依赖于(修饰或是其参数)哪些其他单词</p><p><img src="/img/nlp/第五讲/2.png" /></p><ul><li><p>look是整个句子的根源，look依赖于crate(或者说crate是look的依赖</p><ul><li><p><code>in</code>，<code>the</code>，<code>large</code> 都是<code>crate</code> 的依赖</p></li><li><p><code>in the kitchen</code> 是 <code>crate</code> 的修饰</p></li><li><p><code>in</code>，<code>the</code> 都是 <code>kitchen</code>的依赖</p></li><li><p><code>by the door</code> 是 <code>crate</code> 的依赖</p></li></ul></li></ul><h2 id="为什么我们需要句子结构">1.3 为什么我们需要句子结构？</h2><p>为了能够正确地解释语言，我们需要理解句子结构</p><p>人类通过将单词组合成更大的单元来传达复杂的意思，从而交流复杂的思想</p><p>我们需要知道什么与什么相关联</p><h2 id="介词短语依附歧义">1.4 介词短语依附歧义</h2><p>1、<code>San Jose cops kill man with knife</code></p><ul><li>警察用刀杀了那个男子<ul><li><code>cops</code> 是 <code>kill</code> 的 <code>subject</code>(subject 指 <strong>主语</strong>)</li><li><code>man</code> 是 <code>kill</code> 的 <code>object</code> (object指 <strong>宾语</strong>)</li><li><code>knife</code> 是 <code>kill</code> 的 <code>modifier</code>(modifier 指 <strong>修饰符</strong>)</li></ul></li><li>警察杀了那个有刀的男子<ul><li><code>knife</code> 是 <code>man</code> 的 <code>modifier</code>(名词修饰符，简称为 <code>nmod</code>)</li></ul></li></ul><p>2、<code>Scientists count whales from space</code></p><ul><li><code>from space</code>这一介词短语修饰的是前面的动词<code>count</code>还是名词<code>whales</code>?<ul><li>这就是人类语言和编程语言中不同的地方</li></ul></li></ul><h2 id="介词短语附加歧义成倍增加">1.5 介词短语附加歧义成倍增加</h2><p>关键的解析决策是我们如何“依存”各种成分</p><ul><li>介词短语、状语或分词短语、不定式、协调等。</li></ul><p><code>The board approved [ its acquisition ] [ by Royal Trustco Ltd. ] [ of Toronto ] [ for $27 a share ] [ at its monthly meeting ].</code></p><ul><li><code>board</code> 是 <code>approved</code>的主语，<code>acquisition</code> 是 <code>approved</code> 的谓语</li><li><code>by Royal Trustco Ltd.</code> 是修饰 <code>acquisition</code>的，即董事会批准了这家公司的收购</li><li><code>of Toronto</code> 可以修饰<code>approved</code>，<code>acquisition</code>，<code>Royal Trustco Ltd.</code>之一，经过分析可以得知是修饰<code>Royal Trustco Ltd.</code>，即表示这家公司的位置</li><li><code>for $27 a share</code> 修饰 <code>acquisition</code></li><li><code>at its monthly meeting</code> 修饰<code>approved</code>，即表示批准的时间地点</li></ul><p>面对这样复杂的句子结构，我们需要考虑 指数级的可能结构，这个序列被称为 <strong>卡特兰数/Catalan numbers</strong><span class="math display">\[C_{n}=(2 n) ! /[(n+1) ! n !]\]</span></p><h2 id="协调范围模糊">1.6 协调范围模糊</h2><p>1、<code>Shuttle veteran and longtime NASA executive Fred Gregory appointed to board</code></p><ul><li>一个人：<code>[[Shuttle veteran and longtime NASA executive] Fred Gregory] appointed to board</code></li><li>两个人：<code>[Shuttle veteran] and [longtime NASA executive Fred Gregory] appointed to board</code></li></ul><p>2、<code>Students get first hand job experience</code></p><ul><li><code>first hand</code>表示第一手的，直接的，即学生获得了直接的工作经验<ul><li><code>first</code> 是 <code>hand</code> 的形容词修饰语(amod)</li></ul></li><li><code>first</code> 修饰 <code>experience</code>，<code>hand</code>修饰 <code>job</code></li></ul><h2 id="动词短语vp依存歧义">1.7 动词短语(VP)依存歧义</h2><p><code>Mutilated body washes up on Rio beach to be used for Olympic beach volleyball</code></p><ul><li><code>to be used for Olympic beach volleyball</code> 是 动词短语(VP)</li><li>修饰的是 <code>body</code> 还是 <code>beach</code></li></ul><h1 id="依赖语法与树库">2.依赖语法与树库</h1><h2 id="论文解读-依赖路径识别语义关系">2.1 #论文解读#依赖路径识别语义关系</h2><p><img src="/img/nlp/第五讲/3.png" /></p><h2 id="依存文法和依存结构">2.2 依存文法和依存结构</h2><p>关联语法假设句法结构包括词汇项之间的关系，通常是二元不对称关系(“箭头”)，称为<strong>依赖关系</strong></p><p><strong>Dependency Structure有两种表现形式</strong></p><ol type="1"><li><p><strong>一种是直接在句子上标出依存关系箭头及语法关系</strong></p></li><li><p><strong>另一种是将其做成树状机构(Dependency TreeGraph)</strong></p></li></ol><ul><li>箭头通常标记(type)为语法关系的名称(主题、介词对象、apposition等)</li><li>箭头连接头部(head)(调速器，上级，regent)和一个依赖(修饰词，下级，下属)</li><li>通常，依赖关系形成一棵树(单头，无环，连接图)</li></ul><h2 id="依存语法解析历史">2.3 依存语法/解析历史</h2><ul><li>依赖结构的概念可以追溯到很久以前<ul><li>Paṇini的语法(公元前5世纪)</li><li>一千年，阿拉伯语的语法的基本方法</li></ul></li><li>选区/上下文无关文法是一个新奇的发明<ul><li>20世纪发明(R.S.Wells,1947; then Chomsky)</li></ul></li><li>现代依赖工作经常源于 L. Tesnière(1959)<ul><li>是20世纪“东方”的主导方法(俄罗斯，中国，…)<ul><li>有利于更自由的语序语言</li></ul></li></ul></li><li>NLP中最早类型的解析器在美国<ul><li>David Hays是美国计算语言学的创始人之一，他很早就(第一个?)构建了依赖解析器(Hays1962)</li></ul></li></ul><h2 id="依存语法和依赖结构">2.4 依存语法和依赖结构</h2><ul><li>人们对箭头指向的方式不一致：有些人把箭头朝一个方向画；有人是反过来的<ul><li>Tesnière 从头开始指向依赖，本课使用此种方式</li></ul></li><li>通常添加一个伪根指向整个句子的头部，这样每个单词都精确地依赖于另一个节点</li></ul><p><img src="/img/nlp/第五讲/4.png" /></p><h2 id="带注释数据的兴起通用依存句法树库">2.5带注释数据的兴起：通用依存句法树库</h2><p><img src="/img/nlp/第五讲/5.png" /></p><h2 id="带注释数据的兴起">2.6 带注释数据的兴起</h2><ul><li><p><strong>从一开始，构建 treebank似乎比构建语法慢得多，也没有那么有用</strong></p></li><li><p>但是 treebank 给我们提供了许多东西</p><ul><li>可重用性<ul><li>许多解析器、词性标记器等可以构建在它之上</li><li>语言学的宝贵资源</li></ul></li><li>广泛的覆盖面，而不仅仅是一些直觉</li><li>频率和分布信息</li><li>一种评估系统的方法</li></ul></li></ul><h2 id="依赖条件首选项">2.7 依赖条件首选项</h2><p><strong>依赖项解析的信息来源是什么</strong>？</p><p>1.<strong>Bilexical affinities</strong> (两个单词间的密切关系) -[discussion → issues] 是看上去有道理的</p><p>2.<strong>Dependency distance 依赖距离</strong> - 主要是与相邻词</p><p>3.<strong>Intervening material 介于中间的物质</strong> -依赖很少跨越介于中间的动词或标点符号</p><p>4.<strong>Valency of heads</strong> - How many dependents on whichside are usual for a head?</p><p><img src="/img/nlp/第五讲/6.png" /></p><h2 id="依赖关系分析">2.8 依赖关系分析</h2><ul><li><p>通过为每个单词选择它所依赖的其他单词(包括根)来解析一个句子</p></li><li><p>通常有一些限制</p><ul><li>只有一个单词是依赖于根的</li><li>不存在循环 A→B，B→A</li></ul></li><li><p>这使得依赖项成为树</p></li><li><p>最后一个问题是箭头是否可以交叉(非投影的 non-projective)</p><ul><li>没有交叉的就是non-projectice</li></ul></li></ul><p><img src="/img/nlp/第五讲/7.png" /></p><h2 id="射影性">2.9 射影性</h2><ul><li><p>定义：当单词按线性顺序排列时，没有交叉的依赖弧，所有的弧都在单词的上方</p></li><li><p>与CFG树并行的依赖关系必须是投影的</p><ul><li>通过将每个类别的一个子类别作为头来形成依赖关系</li></ul></li><li><p>但是依赖理论通常允许非投射结构来解释移位的成分</p><ul><li>如果没有这些非投射依赖关系，就不可能很容易获得某些结构的语义</li></ul></li></ul><h2 id="依存分析方法">2.10 依存分析方法</h2><p>1.<strong>Dynamic programming</strong></p><ul><li>Eisner(1996)提出了一种复杂度为 O(n3)的聪明算法，它生成头部位于末尾而不是中间的解析项</li></ul><p>2.<strong>Graph algorithms</strong></p><ul><li>为一个句子创建一个最小生成树</li><li>McDonald et al.’s (2005) MSTParser使用ML分类器独立地对依赖项进行评分(他使用MIRA进行在线学习，但它也可以是其他东西)</li></ul><p>3.<strong>Constraint Satisfaction</strong></p><ul><li>去掉不满足硬约束的边 Karlsson(1990), etc.</li></ul><p>4.<strong>“Transition-based parsing” or “deterministic dependencyparsing”</strong></p><ul><li>良好的机器学习分类器 MaltParser(Nivreet al. 2008)指导下的依存贪婪选择。已证明非常有效。</li></ul><h1 id="基于转换的依存分析模型">3.基于转换的依存分析模型</h1><h2 id="论文解读-greedy-transition-based-parsing-nivre-2003">3.1#论文解读# Greedy transition-based parsing [Nivre 2003]</h2><ul><li>贪婪判别依赖解析器一种简单形式</li><li>解析器执行一系列自底向上的操作<ul><li>大致类似于shift-reduce解析器中的“shift”或“reduce”，但“reduce”操作专门用于创建头在左或右的依赖项</li></ul></li><li>解析器如下：<ul><li>栈<span class="math inline">\(\sigma\)</span>以 ROOT符号开始，由若干组<span class="math inline">\(w_i\)</span>成</li><li>缓存<spanclass="math inline">\(\beta\)</span>以输入序列开始，由若干<spanclass="math inline">\(w_i\)</span>组成</li><li>一个依存弧的集合<spanclass="math inline">\(A\)</span>，一开始为空。每条边的形式是<spanclass="math inline">\((w_i,r,w_j)\)</span>，其中<spanclass="math inline">\(r\)</span>描述了节点的依存关系</li><li>一组操作</li></ul></li></ul><h2 id="基本的基于转换的依存关系解析器">3.2基本的基于转换的依存关系解析器</h2>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第四讲-神经网络反向传播与计算图</title>
    <link href="/2022/05/08/%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE/"/>
    <url>/2022/05/08/%E7%AC%AC%E5%9B%9B%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE/</url>
    
    <content type="html"><![CDATA[<h1id="简单神经网络的梯度矩阵与建议">1.简单神经网络的梯度矩阵与建议</h1><h2 id="权重矩阵的导数">1.1 权重矩阵的导数</h2><ul><li><p>让我们仔细看看计算<span class="math inline">\(\frac{\partials}{\partial W}\)</span></p></li><li><p>再次使用链式法则:</p></li><li><p><span class="math display">\[\frac{\partial s}{\partial W}=\frac{\partial s}{\partial h}\frac{\partial h}{\partial z} \frac{\partial z}{\partial W}\]</span></p></li><li><p><span class="math display">\[s=u^{T} h, \ \ \ h=f(z),\ \ \ z=Wx+b\]</span></p></li></ul><h2 id="反向传播梯度求导">1.2 反向传播梯度求导</h2><ul><li><p>这个函数(从上次开始)</p></li><li><p><span class="math display">\[\frac{\partial s}{\partial W}=\delta \frac{\partial z}{\partialW}=\delta \frac{\partial}{\partial W} W x+b\]</span></p></li><li><p>考虑单个权重<spanclass="math inline">\(W_{ij}\)</span>的导数</p></li><li><p><span class="math inline">\(W_{ij}\)</span>只对<spanclass="math inline">\(z_i\)</span>有贡献</p></li></ul><p><span class="math display">\[\begin{aligned}\frac{\partial z_{i}}{\partial W_{i j}} &amp;=\frac{\partial}{\partialW_{i j}} W_{i .} x+b_{i} \\&amp;=\frac{\partial}{\partial W_{i j}} \sum_{k=1}^{d} W_{i k}x_{k}=x_{j}\end{aligned}\]</span></p><p><img src="/img/nlp/第四讲/1.png" /></p><ul><li><p>对于单个<spanclass="math inline">\(W_{ij}\)</span>的导数</p></li><li><p><span class="math display">\[\frac{\partial s}{\partial W_{i j}}=\delta_{i} x_{j}\]</span></p></li><li><p>我们想要整个<spanclass="math inline">\(W\)</span>的梯度，但是每种情况都是一样的</p></li><li><p>解决方法：<strong>外积</strong></p></li><li><p><span class="math display">\[\begin{aligned}\frac{\partial s}{\partial W} &amp;=\delta^{T} x^{T} \\{[n \times m] } &amp;=[n \times 1][1 \times m]\end{aligned}\]</span></p></li></ul><h2 id="为窗口模型推导梯度">1.3 为窗口模型推导梯度</h2><ul><li><p>到达并更新单词向量的梯度可以简单地分解为每个单词向量的梯度</p></li><li><p>令<span class="math inline">\(\nabla_{x} J=W^{T}\delta=\delta_{x_{\text {window }}}\)</span></p></li><li><p><span class="math inline">\(X_{\text {window}}=\left[\begin{array}{lllll} X_{\text {museums }} &amp; X_{\text {in }}&amp; X_{\text {Paris }} &amp; X_{\text {are }} &amp; X_{\text {amazing}} \end{array}\right]\)</span></p></li><li><p>则得到<span class="math inline">\(\delta_{\text {window}}=\left[\begin{array}{c} \nabla_{x_{\text {museums }}} \\\nabla_{x_{\text {in }}} \\ \nabla_{x_{\text {Pare }}} \\\nabla_{x_{\text {are }}} \\ \nabla_{x_{\text {amazing }}}\end{array}\right] \in \mathbb{R}^{5 d}\)</span></p></li><li><p>我们将根据梯度逐个更新对应的词向量矩阵中的词向量，所以实际上是对词向量矩阵的更新是非常稀疏的</p></li></ul><h2 id="在窗口模型中更新单词梯度">1.4 在窗口模型中更新单词梯度</h2><ul><li>当我们将梯度更新到词向量中时，这将更新单词向量，使它们(理论上)在确定命名实体时更有帮助。</li><li>例如，模型可以了解到，当看到<spanclass="math inline">\(x_{in}\)</span>是中心词之前的单词时，指示中心词是一个Location</li></ul><h2 id="重新训练词向量时的陷阱">1.5 重新训练词向量时的陷阱</h2><ul><li><p><strong>背景</strong>：我们正在训练一个单词电影评论情绪的逻辑回归分类模型</p></li><li><p>在<strong>训练数据</strong>中，我们有“TV”和“telly”</p></li><li><p>在<strong>测试数据</strong>中我们有“television”</p></li><li><p><strong>预训练</strong>的单词向量有三个相似之处：</p><p><img src="/img/nlp/第四讲/2.png" /></p></li><li><p><strong>问题</strong>：当我们更新向量时会发生什么</p></li><li><p>回答：</p><ul><li>那些在训练数据中出现的单词会四处移动</li><li>“TV”和“telly”</li><li>没有包含在训练数据中的词汇保持原样</li><li>“television”</li></ul></li></ul><p><img src="/img/nlp/第四讲/3.png" /></p><h2 id="关于再训练的建议">1.6 关于再训练的建议</h2><ul><li><p><strong>问题</strong>：应该使用可用的“预训练”词向量吗？</p></li><li><p>回答：</p><ul><li>几乎总是「应该用」</li><li>他们接受了大量的数据训练，所以他们会知道训练数据中没有的单词，也会知道更多关于训练数据中的单词</li><li>拥有上亿的数据语料吗？那可以随机初始化开始训练</li></ul></li><li><p><strong>问题</strong>：我应该更新(“finetune”)我自己的单词向量吗？</p></li><li><p>回答：</p><ul><li>如果你只有一个小的训练数据集，不要对预训练词向量做再训练</li><li>如果您有一个大型数据集，那么基于任务训练更新词向量（ train = update= fine-tune ）效果会更好</li></ul></li></ul><h1 id="计算图与反向传播">2.计算图与反向传播</h1><h2 id="反向传播">2.1 反向传播</h2><ul><li>我们几乎已经向你们展示了反向传播<ul><li>求导并使用(广义)链式法则</li></ul></li><li>另一个技巧：在计算较低层的导数时，我们重用对较深层计算的导数，以减小计算量</li></ul><h2 id="计算图和反向传播">2.2 计算图和反向传播</h2><ul><li>我们把神经网络方程表示成一个图<ul><li>源节点：输入</li><li>内部节点：操作</li><li>边传递操作的结果</li></ul></li></ul><p><span class="math display">\[\begin{array}{l}s=u^{T} h \\h=f(z) \\z=W x+b \\x \quad(\text { input) }\end{array}\]</span></p><p><img src="/img/nlp/第四讲/4.png" /></p><h2 id="反向传播单神经元视角">2.3 反向传播：单神经元视角</h2><ul><li>节点接收“上游梯度”<ul><li>目标是传递正确的“下游梯度”</li></ul></li><li>每个节点都有局部梯度 local gradient<ul><li>它输出的梯度是与它的输入有关</li></ul></li><li>下游梯度 = 上游梯度<spanclass="math inline">\(\times\)</span>局部梯度</li></ul><p><img src="/img/nlp/第四讲/5.png" /></p><ul><li>有多个输入的节点呢？</li></ul><p><img src="/img/nlp/第四讲/6.png" /></p><h2 id="反向传播计算图示例">2.4 反向传播计算图示例</h2><p><img src="D:\blog\source\img\nlp\第四讲\7.png" /></p><h2 id="求和形态的梯度计算">2.5 求和形态的梯度计算</h2><p>上图中的<span class="math inline">\(\frac{\partial f}{\partialy}\)</span>的梯度的计算 <span class="math display">\[\begin{array}{c}a=x+y \\b=\max (y, z) \\f=a b \\\frac{\partial f}{\partial y}=\frac{\partial f}{\partial a}\frac{\partial a}{\partial y}+\frac{\partial f}{\partial b}\frac{\partial b}{\partial y}\end{array}\]</span></p><h2 id="直挂理解神经元的梯度传递">2.6 直挂理解神经元的梯度传递</h2>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第三讲-神经网络知识</title>
    <link href="/2022/05/06/%E7%AC%AC%E4%B8%89%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86/"/>
    <url>/2022/05/06/%E7%AC%AC%E4%B8%89%E8%AE%B2-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络基础">1 神经网络基础</h1><h2 id="分类问题基础">1.1 分类问题基础</h2><p>对于分类问题，我们有训练<strong>数据集</strong>：它由一些<strong>样本</strong>组成：<span class="math display">\[\{x_i,y_i\}^N_{i=1}\]</span></p><ul><li><spanclass="math inline">\(x_i\)</span>是<strong>输入</strong>，例如单词(索引或是向量)，句子，文档等等(维度为d)</li><li><spanclass="math inline">\(y_i\)</span>是我们尝试预测的标签(C个类别中的一个)，例如：<ul><li>类别：感情，命名实体，购买/售出的决定</li><li>其他单词</li><li>多词序列(之后会提到)</li></ul></li></ul><h2 id="分类问题直观理解">1.2 分类问题直观理解</h2><ul><li><p>用一个最简单的2维词向量分类问题作为案例</p><ul><li>使用softmax / logistic回归</li><li>构建线性决策边界</li></ul></li><li><p><strong>传统的机器学习/统计学方法</strong>：假设<spanclass="math inline">\(x_i\)</span>是固定的，训练 softmax/logistic回归的权重<span class="math inline">\(W \in R^{C \timesd}\)</span>来决定决定边界(超平面)</p></li><li><p><strong>预测阶段</strong>，对每个<spanclass="math inline">\(x\)</span>，预测:</p></li></ul><p><span class="math display">\[p(y \mid x)=\frac{\exp \left(W_{y} \cdot x\right)}{\sum_{c=1}^{C} \exp\left(W_{c} \cdot x\right)}\]</span></p><h2 id="softmax分类器的细节">1.3 softmax分类器的细节</h2><p>我们可以将预测函数分为<strong>两个步骤</strong>：</p><h3 id="将w的yth行和x中的对应行相乘得到分数">1 将<spanclass="math inline">\(W\)</span>的<spanclass="math inline">\(y^{th}\)</span>行和<spanclass="math inline">\(x\)</span>中的对应行相乘得到分数：</h3><p><span class="math display">\[W_{y} \cdot x=\sum_{i=1}^{d} W_{y i} x_{i}=f_{y}\]</span></p><p>对c=1，2，...，C计算所有的<spanclass="math inline">\(f_y\)</span></p><h3 id="使用softmax函数获得归一化的概率">2使用softmax函数获得归一化的概率：</h3><p><span class="math display">\[p(y \mid x)=\frac{\exp \left(f_{y}\right)}{\sum_{c=1}^{C} \exp\left(f_{c}\right)}=\operatorname{softmax}\left(f_{y}\right)\]</span></p><h2 id="softmax和交叉熵损失">1.4 softmax和交叉熵损失</h2><p>在softmax分类器中最常用到交叉熵损失，也是负对数概率形态。</p><p>对于每个训练样本<spanclass="math inline">\((x,y)\)</span>，我们的目标<strong>是最大化正确类</strong><spanclass="math inline">\(y\)</span>的概率，或者我们可以<strong>最小化该类的负对数概率</strong>:<span class="math display">\[-\log p(y \mid x)=-\log \left(\frac{\exp\left(f_{y}\right)}{\sum_{c=1}^{C} \exp \left(f_{c}\right)}\right)\]</span></p><h2 id="交叉熵损失理解">1.5 交叉熵损失理解</h2><ul><li>交叉熵的概念来源于信息论，衡量两个分布之间的差异，交叉熵越大，变量的取值越不确定，反之则越确定。</li><li>令真实概率分布为<spanclass="math inline">\(p\)</span>，我们计算的模型概率分布为<spanclass="math inline">\(q\)</span></li><li>交叉熵为：</li></ul><p><span class="math display">\[H(p, q)=-\sum_{c=1}^{C} p(c) \log q(c)\]</span></p><ul><li>假设标准答案的概率分布是，在正确的类上为1，在其他类别上为0：</li></ul><p><span class="math display">\[p=[0,...,0,1,...,0]\]</span></p><ul><li>因为是独热向量(one-hotvector)，所以唯一剩下的项是真实类的负对数概率。</li></ul><h2 id="完整数据集上的分类">1.6 完整数据集上的分类</h2><p>在整个数据集<spanclass="math inline">\(\{x_i,y_i\}_{i=1}^N\)</span>上的交叉熵损失函数，是所有样本的交叉熵的均值<span class="math display">\[J(\theta)=\frac{1}{N} \sum_{i=1}^{N}-\log\left(\frac{e^{f_{y_{i}}}}{\sum_{c=1}^{C} e^{f_{c}}}\right)\]</span> 不使用 <span class="math display">\[f_{y}=f_{y}(x)=W_{y} \cdot x=\sum_{j=1}^{d} W_{y j} x_{j}\]</span> 而是使用向量化的形态，基于矩阵来表示<spanclass="math inline">\(f:f=W_x\)</span></p><h2 id="传统的机器学习优化算法">1.7 传统的机器学习优化算法</h2><p>对于传统的机器学习算法（如逻辑回归）来说，一般机器学习的参数<spanclass="math inline">\(\theta\)</span>通常只由<spanclass="math inline">\(W\)</span>的列组成 <span class="math display">\[\theta=\left[\begin{array}{c}W_{\cdot 1} \\\vdots \\W_{\cdot d}\end{array}\right]=W(:) \in \mathbb{R}^{C d}\]</span> 因此，我们只通过以下方式<strong>更新决策边界</strong> <spanclass="math display">\[\nabla_{\theta} J(\theta)=\left[\begin{array}{c}\nabla_{W_{1}} \\\vdots \\\nabla_{W_{d}}\end{array}\right] \in \mathbb{R}^{C d}\]</span></p><h2 id="神经网络分类器">1.8 神经网络分类器</h2><ul><li>单独使用线性分类器Softmax( ≈ logistic回归)并不十分强大</li></ul><p><img src="/img/nlp/第三讲/1.png" /></p><ul><li>如上图所示，Softmax得到的是线性决策边界<ul><li>对于复杂问题来说，它的表达能力是有限的</li><li>有一些分错的点，需要更强的非线性表达能力来区分</li></ul></li></ul><h2 id="神经网络非线性切分">1.9 神经网络非线性切分</h2><ul><li><p>神经网络可以学习更复杂的函数和非线性决策边界</p></li><li><p>tip ：更高级的分类需要</p><ul><li>词向量</li><li>更深层次的深层神经网络</li></ul></li></ul><h2 id="基于词向量的分类差异">1.10 基于词向量的分类差异</h2><ul><li>一般在NLP深度学习中：<ul><li>我们学习了矩阵<span class="math inline">\(W\)</span>和词向量<spanclass="math inline">\(x\)</span>。</li><li>我们学习传统参数和表示。</li><li>词向量是对独热向量的重新表示——在中间层向量空间中移动它们——以便(线性)softmax分类器可以更好地分类。</li></ul></li><li>即将词向量理解为一层神经网络，输入单词的独热向量并获得单词的词向量表示，并且我们需要对其进行更新。</li></ul><p><span class="math display">\[\nabla_{\theta} J(\theta)=\left[\begin{array}{c}\nabla_{W_{1}} \\\vdots \\\nabla_{W_{\text {dardvark }}} \\\vdots \\\nabla_{x_{z e b r a}}\end{array}\right] \in \mathbb{R}^{C d+V d}\]</span></p><ul><li>其中，<span class="math inline">\(Vd\)</span>是数量很大的参数。</li></ul><h2 id="神经计算">1.11 神经计算</h2><ul><li>An artificial neuron<ul><li>神经网络有自己的术语包</li><li>但如果你了解 softmax模型是如何工作的，那么你就可以很容易地理解神经元的操作</li></ul></li><li>Neural computation：神经计算</li><li>Neural selectivity：神经选择性</li><li>Hierarchy of neural processing：神经处理层次</li></ul><h2 id="单个神经元可视作二元逻辑回归单元">1.12单个神经元：可视作二元逻辑回归单元</h2><p><span class="math display">\[h_{w, b}(x)=f\left(w^{T} x+b\right)\]</span></p><p><span class="math display">\[f(z)=\frac{1}{1+e^{-z}}\]</span></p><ul><li><spanclass="math inline">\(b\)</span>：我们可以有一个“总是打开”的特性，它给出一个先验类，或者将它作为一个偏向项分离出来。</li><li><span class="math inline">\(w\)</span>，<spanclass="math inline">\(b\)</span>是神经元的参数。</li></ul><h2 id="一个神经网络多个逻辑回归组合">1.13一个神经网络：多个逻辑回归组合</h2><ul><li>如果我们输入一个向量通过一系列逻辑回归函数，那么我们得到一个输出向量。</li><li>但是我们不需要提前决定这些逻辑回归试图预测的变量是什么。</li></ul><p><img src="/img/nlp/第三讲/2.png" /></p><ul><li>我们可以输入另一个logistic回归函数。</li><li>损失函数将指导中间隐藏变量应该是什么，以便更好地预测下一层的目标。</li></ul><p><img src="/img/nlp/第三讲/3.png" /></p><p>我们添加更多层的神经网络，就得到了多层感知器。</p><p><img src="/img/nlp/第三讲/4.png" /></p><h2 id="单层神经网络的矩阵形态表示">1.14 单层神经网络的矩阵形态表示</h2><p>我们有： <span class="math display">\[a_{1}=f\left(W_{11} x_{1}+W_{12} x_{2}+W_{13} x_{3}+b_{1}\right)\]</span></p><p><span class="math display">\[a_{2}=f\left(W_{21} x_{1}+W_{22} x_{2}+W_{23} x_{3}+b_{2}\right)\]</span></p><p>在矩阵中： <span class="math display">\[z=Wx+b\]</span> 激活函数<spanclass="math inline">\(f\)</span>在运算时是element-wise逐元素的 <spanclass="math display">\[f\left(\left[z_{1}, z_{2},z_{3}\right]\right)=\left[f\left(z_{1}\right), f\left(z_{2}\right),f\left(z_{3}\right)\right]\]</span> <img src="/img/nlp/第三讲/5.png" /></p><h2 id="非线性变换的必要性">1.15 非线性变换的必要性</h2><ul><li>例如：函数近似，如回归或分类<ul><li>没有非线性，深度神经网络只能做线性变换</li><li>多个线性变换，也还是组成一个线性变换<spanclass="math inline">\(W_1W_2x=Wx\)</span></li></ul></li><li>因为线性变换是以某种方式旋转和拉伸空间，多次的旋转和拉伸可以融合为一次线性变换</li><li>对于非线性函数而言，使用更多的层，他们可以近似更复杂的函数</li></ul><h1 id="命名实体识别">2 命名实体识别</h1><h2 id="命名实体识别ner">2.1 命名实体识别(NER)</h2><ul><li>可能的用途<ul><li>跟踪文档中提到的特定实体(组织、个人、地点、歌曲名、电影名等)</li><li>对于问题回答，答案通常是命名实体</li><li>许多需要的信息实际上是命名实体之间的关联</li><li>同样的技术可以扩展到其他 slot-filling 槽填充分类</li></ul></li><li>通常后面是命名实体链接/规范化到知识库</li></ul><h2 id="句子中的命名实体识别">2.2 句子中的命名实体识别</h2><p>我们通过在上下文中对单词进行分类，然后将实体提取为单词子序列来预测实体。</p><p><img src="/img/nlp/第三讲/6.png" /></p><h2 id="ner的难点">2.3 NER的难点</h2><ul><li>很难计算出实体的边界<ul><li>第一个实体是 “First National Bank” 还是 “National Bank”</li></ul></li><li>很难知道某物是否是一个实体<ul><li>是一所名为“Future School” 的学校，还是这是一所未来的学校？</li></ul></li><li>很难知道未知/新奇实体的类别<ul><li>“Zig Ziglar” ? 一个人</li></ul></li><li>实体类是模糊的，依赖于上下文<ul><li>这里的“Charles Schwab” 是 PER 不是 ORG</li></ul></li></ul><h1 id="基于窗口数据的分类预测">3.基于窗口数据的分类预测</h1><h2 id="词-窗分类">3.1. 词-窗分类</h2><ul><li>思路：为在上下文中的语言构建分类器<ul><li>一般来说，很少对单个单词进行分类</li></ul></li><li>例如，上下文中一个单词的命名实体分类<ul><li>人、地点、组织、没有</li></ul></li><li>在上下文中对单词进行分类的一个简单方法，可能是对窗口中的单词向量进行平均，并对平均向量进行分类<ul><li>问题：这会丢失位置信息</li></ul></li></ul><h2 id="窗口分类器softmax">3.2 窗口分类器：softmax</h2><ul><li><p>训练softmax分类器对中心词进行分类，方法是在一个窗口内将中心词周围的词向量串联起来</p></li><li><p>例子：在这句话的上下文中对“Paris”进行分类，窗口长度为2</p></li><li><p>结果向量<span class="math inline">\(x_{w i n d o w}=x \in R^{5d}\)</span>是一个列向量</p></li></ul><h2 id="最简单的窗口分类器softmax">3.3 最简单的窗口分类器：Softmax</h2><p>对于<spanclass="math inline">\(x=x_{window}\)</span>，我们可以使用与之前相同的softmax分类器<span class="math display">\[\hat{y}_{y}=p(y \mid x)=\frac{\exp \left(\sqrt{W_{y} \cdotx}\right)}{\sum_{c=1}^{C} \exp \left(W_{c} \cdot x\right)}\]</span> <strong>如何更新向量</strong>？</p><ul><li>简而言之：就像之前讲的那样，求导和优化</li></ul><h2 id="稍微复杂一点多层感知器">3.4 稍微复杂一点：多层感知器</h2><ul><li><p>假设我们要对中心词是否为一个地点，进行分类</p></li><li><p>与word2vec类似，我们将遍历语料库中的所有位置。但这一次，它将受到监督，只有一些位置能够得到高分。</p><ul><li>例如，在他们的中心有一个实际的NERLocation的位置是“真实的”位置会获得高分</li></ul></li></ul><h2 id="神经网络前馈计算">3.5 神经网络前馈计算</h2><p>使用神经激活<spanclass="math inline">\(a\)</span>简单地给出一个非标准化的分数 <spanclass="math display">\[\operatorname{score}(x)=U^{T} a \in \mathbb{R}\]</span> 我们用一个三层神经网络计算一个窗口的得分</p><ul><li>s = score(“museums in Paris are amazing”)</li></ul><p><img src="/img/nlp/第三讲/7.png" /></p><h2 id="附加层">3.6 附加层</h2><p>中间层学习输入词向量之间的非线性交互</p><p><img src="/img/nlp/第三讲/8.png" /></p><p>例如：只有当“museum”是第一个向量时，“in”放在第二个位置才重要</p><h1 id="基于pytorch实现的分类器">4.基于pytorch实现的分类器</h1><h2 id="替代最大边缘损失">4.1 替代：最大边缘损失</h2><p>关于训练目标的想法：让真实窗口的得分更高，而其他窗口的得分更低(直到足够好为止)</p><ul><li><span class="math inline">\(s = score(museums\ in\ Paris\ are\amazing)\)</span></li><li><span class="math inline">\(s_c = socre(Not\ all\ museums\ in\Paris)\)</span></li></ul><p><strong>最小化</strong>： <span class="math display">\[J=max(0,1-s-s_c)\]</span> 这是不可微的，但它是连续的 → 我们可以用SGD</p><p><strong>补充解析</strong></p><ul><li>单窗口的目标函数为<spanclass="math inline">\(J=max(0,1-s-s_c)\)</span></li><li>每个中心有NER位置的窗口的得分应该比中心没有位置的窗口高1分</li><li>要获得完整的目标函数：为每个真窗口采样几个损坏的窗口。对所有训练样本窗口求和</li><li>类似于word2vec中的负抽样</li></ul><h2 id="随机梯度下降">4.2 随机梯度下降</h2>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NLP-Assignment2</title>
    <link href="/2022/05/06/NLP-Assignment2/"/>
    <url>/2022/05/06/NLP-Assignment2/</url>
    
    <content type="html"><![CDATA[<h1 id="assignment2">Assignment2</h1><h2 id="解答理解词向量23分">解答：理解词向量（23分）</h2><p>我们先快速回顾一下word2vec算法，它的核心思想是“一个词的含义可以由它周围的词来表示”。具体来说，我们有一个中心词（centerword） <em>c</em>，和这个词 <em>c</em>周围上下文构成的窗口，这个窗口内的除了 <em>c</em>之外的词叫做外围词（outsidewords）。比如下图中，中心词是“banking”，窗口大小为2，所以上下文窗口是：“turning”、”into“、”crises“和”as“。</p><p><img src="/img/nlp/assignment2/1.jpg"/></p><p>Skip-gram模型（word2vec比较常用的一种实现，另一种是cbow）目的是学习概率分布<spanclass="math inline">\(𝑃(𝑂|𝐶)\)</span>。这样一来，就能计算给定的一个词<span class="math inline">\(o\)</span> 和词 <spanclass="math inline">\(c\)</span> 的概率 <spanclass="math inline">\(𝑃(𝑂=𝑜|𝐶=𝑐)\)</span>（即，在已知词 <spanclass="math inline">\(c\)</span> 出现的情况下，词 <spanclass="math inline">\(o\)</span> 出现的概率）， 其中<spanclass="math inline">\(c\)</span> 是中心词，<spanclass="math inline">\(o\)</span> 是窗口中非中心的外围词。</p><p>在word2vec中，这个条件概率分布是通过计算向量点积（dot-products），再应用naive-softmax函数得到的：<span class="math display">\[P(O=o \mid C=c)=\frac{\exp \left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)}{\sum_{w \in \operatorname{Vocab}} \exp\left(\boldsymbol{u}_{w}^{\top} \boldsymbol{v}_{c}\right)}\]</span></p><p>这里，<span class="math inline">\(𝑢_o\)</span>向量代表外围词，<spanclass="math inline">\(v_c\)</span>向量代表中心词。为了包含这些向量，我们有两个矩阵<span class="math inline">\(𝑈\)</span> 和 <spanclass="math inline">\(𝑉\)</span> 。 <spanclass="math inline">\(𝑈\)</span> 的列是外围词， <spanclass="math inline">\(V\)</span> 的列是中心词，这两矩阵都有所有词<spanclass="math inline">\(w \in Vocabulary\)</span>的表示 。</p><p>对于词 <span class="math inline">\(c\)</span> 和词 <spanclass="math inline">\(o\)</span>，损失函数为对数几率： <spanclass="math display">\[\boldsymbol{J}_{naive-softmax}(v_c, o, \boldsymbol{U}) = -log P(O=o \midC=c)\]</span></p><p>可以从交叉熵的角度看这个损失函数。真实值为 <spanclass="math inline">\(y\)</span> ，是一个独热向量，预测值 <spanclass="math inline">\(\hat{y}\)</span> 计算得到。具体来说， <spanclass="math inline">\(y\)</span>如果是第k个单词，那么它的第k维为1，其余维都是0，而 <spanclass="math inline">\(\hat{y}\)</span>的第k维表示这是第k个词的概率大小。</p><h3 id="问题a-3分">问题(a) (3分)</h3><p>证明公式(2)给出的naive-softmax的损失函数，和 <spanclass="math inline">\(y\)</span> 与 <spanclass="math inline">\(\hat{y}\)</span>的交叉熵损失函数是一样的，均如下所示（答案控制在一行）</p><p><span class="math display">\[-\sum_{w \in V o c a b} y_{w} \log\left(\hat{y}_{w}\right)=-\log \left(\hat{y}_{o}\right)\]</span></p><p><strong>答：</strong>因为除了 <span class="math inline">\(o\)</span>之外的词都不在窗口内，所以只有词 <span class="math inline">\(o\)</span>对损失函数有贡献</p><h3 id="问题b-5分">问题(b) (5分)</h3><p>计算损失函数 <spanclass="math inline">\(\boldsymbol{J}_{naive-softmax}(v_c, o,\boldsymbol{U})\)</span> 对中心词 <spanclass="math inline">\(v_c\)</span> 的偏导数，用 <spanclass="math inline">\(y\)</span> ，<spanclass="math inline">\(\hat{y}\)</span>和 <spanclass="math inline">\(𝑈\)</span> 来表示。</p><p><strong>答：</strong> <span class="math display">\[\begin{aligned}J_{\text {naive-softmax} }\left(\boldsymbol{v}_{c}, o,\boldsymbol{U}\right)&amp;=-\log P(O=o | C=c) \\&amp;= -\log \frac{\exp \left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)}{\sum_{w \in \operatorname{Vocab} } \exp\left(\boldsymbol{u}_{\boldsymbol{w} }^{\top} \boldsymbol{v}_{c}\right)}\\&amp;= - {u}_{o}^{\top}{v}_{c} + \log \sum_{w \in \operatorname{Vocab} }\exp \left(\boldsymbol{u}_{\boldsymbol{w} }^{\top}\boldsymbol{v}_{c}\right)\end{aligned}\]</span></p><p><span class="math display">\[\begin{aligned}\frac{\partial J_{\text {naive-softmax} }\left(\boldsymbol{v}_{c}, o,\boldsymbol{U}\right)}{\partial  v_c}&amp;= -u_o + \sum_{o \in \operatorname{Vocab} }\frac{\exp(u_o^\topv_c)}{\sum_{w \in \operatorname{Vocab} } \exp\left(\boldsymbol{u}_{\boldsymbol{w} }^{\top} \boldsymbol{v}_{c}\right)}\frac{\partial (u_o^\top v_c)}{\partial v_c}\\&amp;=-u_o + \sum_{o \in \operatorname{Vocab} } P(O=o | C=c)  u_o \\&amp;=- U y + U \hat y \\&amp;= U(\hat y - y)\end{aligned}\]</span></p><h3 id="问题c-5分">问题(c) (5分)</h3><p>计算损失函数 <spanclass="math inline">\(\boldsymbol{J}_{naive-softmax}(v_c, o,\boldsymbol{U})\)</span> 对上下文窗口内的词 <spanclass="math inline">\(w\)</span> 的偏导数，考虑两种情况，即 <spanclass="math inline">\(w\)</span> 是外围词 <spanclass="math inline">\(o\)</span>，和 <spanclass="math inline">\(w\)</span> 不是 <spanclass="math inline">\(o\)</span>，用 <spanclass="math inline">\(y\)</span> ，<spanclass="math inline">\(\hat{y}\)</span>和 <spanclass="math inline">\(v_c\)</span> 来表示。</p><p><strong>答：</strong> <span class="math display">\[\begin{aligned}\frac{\partial J_{\text {naive-softmax} }\left(\boldsymbol{v}_{c}, o,\boldsymbol{U}\right)}{\partial  u_w}&amp;= -v_c 1_{\lbrace w=o \rbrace } + \frac{\exp(u_w^\top v_c)}{\sum_{w\in \operatorname{Vocab} } \exp \left(\boldsymbol{u}_{\boldsymbol{w}}^{\top} \boldsymbol{v}_{c}\right)}\frac{\partial (u_w^\top v_c)}{\partial u_w}\\&amp;=-v_c  1_{\lbrace w=o \rbrace } +  P(O=w | C=c)  v_c \\&amp;=v_c( \hat y_w -  y_w)\end{aligned}\]</span></p><h3 id="问题d-3分">问题(d) (3分)</h3><p>sigmoid函数如公式所示 <span class="math display">\[\sigma(\boldsymbol{x})=\frac{1}{1+e^{-\boldsymbol{x}}}=\frac{e^{\boldsymbol{x}}}{e^{\boldsymbol{x}}+1}\]</span> 请计算出它对于 <span class="math inline">\(x\)</span> 的导数，<span class="math inline">\(x\)</span> 是一个向量</p><p><strong>答：</strong></p><p>计算雅克比矩阵可得 <span class="math display">\[\begin{aligned}\frac{\partial \sigma(x_i )}{\partial x_j }&amp;= \sigma (x_i) (1 -\sigma(x_i)) 1_{\lbrace i=j\rbrace  }\end{aligned}\]</span> 所以有 <span class="math display">\[\frac{\partial \sigma(x)}{\partial  x}=\text{diag}(\sigma(x) (1- \sigma(x)))\]</span></p><h3 id="问题e-4分">问题(e) (4分)</h3><p>现在我们考虑负采样的损失函数。假设有K个负样本，表示为<spanclass="math inline">\(w_1, w_2, …, w_K\)</span>，它们对应的向量为 <spanclass="math inline">\(u_1, u_2, …, u_K\)</span>，外围词 <spanclass="math inline">\(o \not\in {w_1, w_2, …, w_K}\)</span>，则外围词<span class="math inline">\(o\)</span> 在中心词是 <spanclass="math inline">\(c\)</span> 时产生的损失函数如公式所示。 <spanclass="math display">\[\boldsymbol{J}_{\text {neg-sample }}\left(\boldsymbol{v}_{c}, o,\boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log\left(\sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\right)\]</span> 根据该损失函数，重新计算问题(b)、问题(c)的偏导数，用 <spanclass="math inline">\(\boldsymbol{u}_o\)</span>，<spanclass="math inline">\(\boldsymbol{v}_c\)</span>，<spanclass="math inline">\(\boldsymbol{u}_k\)</span> 来表示。</p><p>完成计算后，简要解释为什么这个损失函数比naive-softmax效率更高。</p><p>注意：你可以用问题(d)的答案来帮助你计算导数</p><p><strong>答：</strong> <span class="math display">\[\begin{aligned}\frac{\partial J_{\text {neg-sample} }\left(v_{c}, o,U\right)}{\partial  v_c}&amp;=-\frac{\sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\right)}{\sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)}u _o-\sum_{k=1}^K\frac{\sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\right)}{\sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)}(-u_k)\\&amp;= -\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\right)u_o+ \sum_{k=1}^K  \left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\right)u_k\\\frac{\partial J_{\text {neg-sample} }\left(v_{c}, o,U\right)}{\partial  u_o}&amp;=-\frac{\sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\right)}{\sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)}v _c\\&amp;= -\left(1- \sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\right)v_c \\\frac{\partial J_{\text {neg-sample} }\left(v_{c}, o,U\right)}{\partial  u_k}&amp;=-\frac{\sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\right)}{\sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)}(-v_c)\\&amp;= \left(1- \sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\right)v_c\end{aligned}\]</span></p><ul><li><p>原始的损失函数中需要求指数和，很容易溢出，负采样的损失函数能很好地规避这个问题。</p></li><li><p>词库从 𝑉𝑜𝑐𝑎𝑏 变成了K+1个词</p></li><li><p>在求内层导数的时候用了sigmoid函数</p></li></ul><h3 id="问题f-3分">问题(f) (3分)</h3><p>假设中心词是 <span class="math inline">\(c =w_t\)</span>，上下文窗口是<span class="math inline">\([w_{t-m}, …,w_{t-1}, w_t, w_{t+1}, …, w_{t+m}]\)</span>，<spanclass="math inline">\(m\)</span>是窗口大小，回顾skip-gram的word2vec实现，在该窗口下的总损失函数是：<span class="math display">\[\boldsymbol{J}_{\text {skip-gram }}\left(\boldsymbol{v}_{c}, w_{t-m},\ldots w_{t+m}, \boldsymbol{U}\right)=\sum_{-m \leq j \leq m \atop j\neq 0} \boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j},\boldsymbol{U}\right)\]</span> 这里，<spanclass="math inline">\(\boldsymbol{J}(\boldsymbol{v}_c, w_{t+j},\boldsymbol{U})\)</span>是外围词是外围词<spanclass="math inline">\(w_{t+j}\)</span>在中心词在中心词<spanclass="math inline">\(c=w_t\)</span>下产生的损失，损失函数可以是naive-softmax或者是neg-sample（负采样），这取决于具体实现。</p><p>计算：</p><ol type="i"><li><p>损失函数对 <span class="math inline">\(U\)</span>的偏导数</p></li><li><p>损失函数对 <span class="math inline">\(\boldsymbol{v}_c\)</span>的偏导数</p></li><li><p>损失函数对 <span class="math inline">\(\boldsymbol{v}_w\)</span>的偏导数</p></li></ol><strong>答：</strong> $$<span class="math display">\[\begin{aligned}\partial \boldsymbol{J}_{\text {skip-gram } }\left(\boldsymbol{v}_{c},w_{t-m}, \dots w_{t+m}, \boldsymbol{U}\right) / \partial \boldsymbol{U}&amp;=\sum_{-m \leq j \leq m \atop j \neq 0} \partial\boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) /\partial \boldsymbol{U} \\\partial \boldsymbol{J}_{\text {skip-gram } }\left(\boldsymbol{v}_{c},w_{t-m}, \dots w_{t+m}, \boldsymbol{U}\right) / \partial\boldsymbol{v_c}&amp;=\sum_{-m \leq j \leq m \atop j \neq 0} \partial\boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) /\partial  \boldsymbol{v_c} \\\partial \boldsymbol{J}_{\text {skip-gram } }\left(\boldsymbol{v}_{c},w_{t-m}, \dots w_{t+m}, \boldsymbol{U}\right) / \partial\boldsymbol{v_w}&amp;=\sum_{-m \leq j \leq m \atop j \neq 0} \partial\boldsymbol{J}\left(\boldsymbol{v}_{c}, w_{t+j}, \boldsymbol{U}\right) /\partial  \boldsymbol{v_w} \\\end{aligned}\]</span><p>$$</p><h2 id="代码实现word2vec20分">代码：实现word2vec（20分）</h2><p>点击 <ahref="http://web.stanford.edu/class/cs224n/assignments/a2.zip">此处</a>下载代码，python版本 &gt;=3.5，需要安装numpy，你利用conda来配置环境：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">conda env create -f env.yml<br>conda activate a2<br></code></pre></div></td></tr></table></figure><p>写完代码后，运行：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">conda deactivate<br></code></pre></div></td></tr></table></figure><h3 id="问题a-12分">问题(a) (12分)</h3><p>首先，实现 word2vec.py 里的sigmoid函数，要支持向量输入。接着实现同一个文件里的 softmax、负采样损失和导数。然后实现skip-gram的损失函数和导数。全部做完之后，运行pythonword2vec.py来检查是否正确。</p><p><strong>答：</strong></p><h4 id="sigmoid">sigmoid</h4><p>numpy具备广播特性，最终得到向量输出</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">    Compute the sigmoid function for the input here.</span><br><span class="hljs-string">    Arguments:</span><br><span class="hljs-string">    x -- A scalar or numpy array.</span><br><span class="hljs-string">    Return:</span><br><span class="hljs-string">    s -- sigmoid(x)</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment">### YOUR CODE HERE</span><br>    s = <span class="hljs-number">1</span> / (<span class="hljs-number">1</span> + np.exp(-x))<br><br>    <span class="hljs-comment">### END YOUR CODE</span><br><br>    <span class="hljs-keyword">return</span> s<br></code></pre></div></td></tr></table></figure><h4 id="naivesoftmaxlossandgradient">naiveSoftmaxLossAndGradient</h4><p>这个模型其实就是一个三层的前馈神经网络，只需要注意维度即可，注释里已经标记出了维度。</p><p>需要注意的是，单词表示是在行，而不是列。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">### YOUR CODE HERE</span><br><br><span class="hljs-comment">### Please use the provided softmax function (imported earlier in this file)</span><br><span class="hljs-comment">### This numerically stable implementation helps you avoid issues pertaining</span><br><span class="hljs-comment">### to integer overflow. </span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">    centerWordVec: 1 * d</span><br><span class="hljs-string">    outsideVectors: n * d</span><br><span class="hljs-string">    &#x27;&#x27;&#x27;</span><br><span class="hljs-comment">#1 * n</span><br>vec = centerWordVec.dot(outsideVectors.T)<br><span class="hljs-comment">#1 * n</span><br>prob = softmax(vec)<br>loss = -np.log(prob[outsideWordIdx])<br><span class="hljs-comment">#1 * d</span><br>gradCenterVec = -outsideVectors[outsideWordIdx] + prob.dot(outsideVectors)<br><span class="hljs-comment">#n * d</span><br>gradOutsideVecs = prob.reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>).dot(centerWordVec.reshape(<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>))<br><span class="hljs-comment">#n * d</span><br>gradOutsideVecs[outsideWordIdx] -= centerWordVec<br><span class="hljs-comment">### END YOUR CODE</span><br></code></pre></div></td></tr></table></figure><h4 id="negsamplinglossandgradient">negSamplingLossAndGradient</h4><p>与native-softmax不同的是：</p><ul><li>只选取K个非外围词作为负样本，加上1个正确的外围词，共K+1个输出</li><li>最后一层使用sigmoid输出，而不是softmax</li></ul><p>注意，反向传播得到的是这K+1个词的梯度，所以需要挨个更新到<em>梯度矩阵</em> 中去</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">### Please use your implementation of sigmoid in here.</span><br><br><span class="hljs-comment"># indices might have same index</span><br><span class="hljs-comment"># extract W</span><br>W = np.zeros((<span class="hljs-built_in">len</span>(indices), outsideVectors.shape[<span class="hljs-number">1</span>]))<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(indices)):<br>    W[i] = outsideVectors[indices[i]]<br><br><span class="hljs-comment"># forward</span><br>a = centerWordVec<br>a = a.reshape((a.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))<br><br>z = np.dot(W, a) <span class="hljs-comment"># (K+1, 1)</span><br>preds = sigmoid(z)<br><br><span class="hljs-comment"># backprop</span><br>y = np.zeros((preds.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>))<br>y[<span class="hljs-number">0</span>] = <span class="hljs-number">1</span> <span class="hljs-comment"># index 0 is target</span><br><br>loss = -(y*np.log(preds) + (<span class="hljs-number">1</span> - y)*np.log(<span class="hljs-number">1</span> - preds)).<span class="hljs-built_in">sum</span>()<br><br>delta = preds - y<br>gradCenterVec = np.dot(W.T, delta) <span class="hljs-comment"># (V, 1)</span><br>gradW = np.dot(delta, a.T) <span class="hljs-comment"># (K+1, V)</span><br>gradCenterVec = gradCenterVec.flatten()<br><br><span class="hljs-comment"># apply gradW into gradOutsideVecs</span><br>gradOutsideVecs = np.zeros_like(outsideVectors)<br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(indices)):<br>    oi = indices[i]<br>    gradOutsideVecs[oi] += gradW[i]<br></code></pre></div></td></tr></table></figure><h4 id="skipgram">skipgram</h4><p>遍历所有的外围词，求和损失函数</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">ci = word2Ind[currentCenterWord]<br>vc = centerWordVectors[ci]<br><br><span class="hljs-keyword">for</span> o <span class="hljs-keyword">in</span> outsideWords:<br>    oi = word2Ind[o]<br>    loss_, gradVc, gradUo = word2vecLossAndGradient(vc, oi, outsideVectors, dataset)<br>    gradCenterVecs[ci] += gradVc<br>    gradOutsideVectors += gradUo<br>    loss += loss_<br></code></pre></div></td></tr></table></figure><h3 id="问题b-4分">问题(b) (4分)</h3><p>完成sgd.py文件的SGD优化器，运行python sgd.py来检查是否正确。</p><p><strong>答</strong>：</p><h4 id="sgd">sgd</h4><p>调用函数得到损失值和梯度，更新即可</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment">### YOUR CODE HERE</span><br>loss, grad = f(x)<br>x -= step * grad<br><br><span class="hljs-comment">### END YOUR CODE</span><br></code></pre></div></td></tr></table></figure><h3 id="问题c-4分">问题(c) (4分)</h3><p>至此所有的代码都写完了，接下来是下载数据集，这里我们使用StanformSentimentTreebank(SST)数据集，它可以用在简单的语义分析任务中去。通过运行 shget_datasets.sh 可以获得该数据集，下载完成后运行 python run.py即可。</p><p>注意：训练的时间取决于代码是否高效（即便是高效的实现，也要跑接近一个小时）</p><p>经过40,000次迭代后，最终结果会保存到 word_vectors.png 里。</p><p><strong>答：</strong></p><p><img src="http://ww1.sinaimg.cn/large/0060yMmAly1gsz2rwoc9sj30hs0dc3zy.jpg" referrerpolicy="no-referrer" /></p><ul><li>在上图中可以观察到，male-&gt;famale 和 king -&gt; queen这两条向量是平行的</li><li>(women, famale)，(enjoyable,annoying) 这些含义接近的词距离很近</li></ul>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>NLP-Assignment1</title>
    <link href="/2022/05/05/NLP-Assignment1/"/>
    <url>/2022/05/05/NLP-Assignment1/</url>
    
    <content type="html"><![CDATA[<h1 id="assignment1">Assignment1</h1><p>所有使用到的头文件如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># All Import Statements Defined Here</span><br><span class="hljs-comment"># Note: Do not add to this list.</span><br><span class="hljs-comment"># ----------------</span><br><br><span class="hljs-keyword">import</span> sys<br><span class="hljs-keyword">assert</span> sys.version_info[<span class="hljs-number">0</span>]==<span class="hljs-number">3</span><br><span class="hljs-keyword">assert</span> sys.version_info[<span class="hljs-number">1</span>] &gt;= <span class="hljs-number">5</span><br><br><span class="hljs-keyword">from</span> gensim.models <span class="hljs-keyword">import</span> KeyedVectors<br><span class="hljs-keyword">from</span> gensim.test.utils <span class="hljs-keyword">import</span> datapath<br><span class="hljs-keyword">import</span> pprint<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>plt.rcParams[<span class="hljs-string">&#x27;figure.figsize&#x27;</span>] = [<span class="hljs-number">10</span>, <span class="hljs-number">5</span>]<br><span class="hljs-keyword">import</span> nltk<br>nltk.download(<span class="hljs-string">&#x27;reuters&#x27;</span>)<br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> reuters<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> scipy <span class="hljs-keyword">as</span> sp<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> TruncatedSVD<br><span class="hljs-keyword">from</span> sklearn.decomposition <span class="hljs-keyword">import</span> PCA<br><br>START_TOKEN = <span class="hljs-string">&#x27;&lt;START&gt;&#x27;</span><br>END_TOKEN = <span class="hljs-string">&#x27;&lt;END&gt;&#x27;</span><br><br>np.random.seed(<span class="hljs-number">0</span>)<br>random.seed(<span class="hljs-number">0</span>)<br><span class="hljs-comment"># ----------------</span><br></code></pre></div></td></tr></table></figure><h2 id="part-1基于计数的词向量">Part 1：基于计数的词向量</h2><p>大多数词向量模型都是基于一个观点：</p><p><strong>You shall know a word by the company it keeps (<ahref="https://en.wikipedia.org/wiki/John_Rupert_Firth">Firth, J. R.1957:11</a>)</strong></p><p>大多数词向量的实现的核心是 <em>相似词</em>，也就是同义词，因为它们有相似的上下文。这里我们介绍一种策略叫做<em>共现矩阵</em> (更多信息可以查看 <ahref="http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf">这里</a>或 <ahref="https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285">这里</a>)</p><p>这部分要实现的是，给定语料库，根据共现矩阵计算词向量，得到语料库中每个词的词向量，流程如下：</p><ul><li>计算语料库的单词集</li><li>计算共现矩阵</li><li>使用SVD降维</li><li>分析词向量</li></ul><h3 id="问题1.1实现-dicintct_words">问题1.1：实现 dicintct_words</h3><p>计算语料库的单词数量、单词集</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">distinct_words</span>(<span class="hljs-params">corpus</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Determine a list of distinct words for the corpus.</span><br><span class="hljs-string">        Params:</span><br><span class="hljs-string">            corpus (list of list of strings): corpus of documents</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python &#x27;sorted&#x27; function)</span><br><span class="hljs-string">            num_corpus_words (integer): number of distinct words across the corpus</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    corpus_words = []<br>    num_corpus_words = -<span class="hljs-number">1</span><br>    <br>    <span class="hljs-comment"># ------------------</span><br>    <span class="hljs-comment"># Write your implementation here.</span><br>    corpus_words =  <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>([word <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> corpus <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence])))<br>    num_corpus_words = <span class="hljs-built_in">len</span>(corpus_words)<br><br>    <span class="hljs-comment"># ------------------</span><br><br>    <span class="hljs-keyword">return</span> corpus_words, num_corpus_words<br></code></pre></div></td></tr></table></figure><p>测试用例：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ---------------------</span><br><span class="hljs-comment"># Run this sanity check</span><br><span class="hljs-comment"># Note that this not an exhaustive check for correctness.</span><br><span class="hljs-comment"># ---------------------</span><br><br><span class="hljs-comment"># Define toy corpus</span><br>test_corpus = [<span class="hljs-string">&quot;&#123;&#125; All that glitters isn&#x27;t gold &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(START_TOKEN, END_TOKEN).split(<span class="hljs-string">&quot; &quot;</span>), <span class="hljs-string">&quot;&#123;&#125; All&#x27;s well that ends well &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(START_TOKEN, END_TOKEN).split(<span class="hljs-string">&quot; &quot;</span>)]<br>test_corpus_words, num_corpus_words = distinct_words(test_corpus)<br><br><span class="hljs-comment"># Correct answers</span><br>ans_test_corpus_words = <span class="hljs-built_in">sorted</span>([START_TOKEN, <span class="hljs-string">&quot;All&quot;</span>, <span class="hljs-string">&quot;ends&quot;</span>, <span class="hljs-string">&quot;that&quot;</span>, <span class="hljs-string">&quot;gold&quot;</span>, <span class="hljs-string">&quot;All&#x27;s&quot;</span>, <span class="hljs-string">&quot;glitters&quot;</span>, <span class="hljs-string">&quot;isn&#x27;t&quot;</span>, <span class="hljs-string">&quot;well&quot;</span>, END_TOKEN])<br>ans_num_corpus_words = <span class="hljs-built_in">len</span>(ans_test_corpus_words)<br><br><span class="hljs-comment"># Test correct number of words</span><br><span class="hljs-keyword">assert</span>(num_corpus_words == ans_num_corpus_words), <span class="hljs-string">&quot;Incorrect number of distinct words. Correct: &#123;&#125;. Yours: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(ans_num_corpus_words, num_corpus_words)<br><br><span class="hljs-comment"># Test correct words</span><br><span class="hljs-keyword">assert</span> (test_corpus_words == ans_test_corpus_words), <span class="hljs-string">&quot;Incorrect corpus_words.\nCorrect: &#123;&#125;\nYours:   &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">str</span>(ans_test_corpus_words), <span class="hljs-built_in">str</span>(test_corpus_words))<br><br><span class="hljs-comment"># Print Success</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Passed All Tests!&quot;</span>)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br></code></pre></div></td></tr></table></figure><h3id="问题1.2实现compute_co_occurrence_matrix">问题1.2：实现compute_co_occurrence_matrix</h3><p>计算给定语料库的共现矩阵。具体来说，对于每一个词<code>w</code>，统计前、后方 <code>window_size</code> 个词的出现次数</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_co_occurrence_matrix</span>(<span class="hljs-params">corpus, window_size=<span class="hljs-number">4</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Compute co-occurrence matrix for the given corpus and window_size (default of 4).</span><br><span class="hljs-string">    </span><br><span class="hljs-string">        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller</span><br><span class="hljs-string">              number of co-occurring words.</span><br><span class="hljs-string">              </span><br><span class="hljs-string">              For example, if we take the document &quot;START All that glitters is not gold END&quot; with window size of 4,</span><br><span class="hljs-string">              &quot;All&quot; will co-occur with &quot;START&quot;, &quot;that&quot;, &quot;glitters&quot;, &quot;is&quot;, and &quot;not&quot;.</span><br><span class="hljs-string">    </span><br><span class="hljs-string">        Params:</span><br><span class="hljs-string">            corpus (list of list of strings): corpus of documents</span><br><span class="hljs-string">            window_size (int): size of context window</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): </span><br><span class="hljs-string">                Co-occurence matrix of word counts. </span><br><span class="hljs-string">                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.</span><br><span class="hljs-string">            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    words, num_words = distinct_words(corpus)<br>    M = np.zeros((num_words, num_words))<br>    word2Ind = <span class="hljs-built_in">dict</span>([(word, index) <span class="hljs-keyword">for</span> index, word <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(words)])<br>    <br>    <span class="hljs-comment"># ------------------</span><br>    <span class="hljs-comment"># Write your implementation here.</span><br>    <span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> corpus:<br>        current_index = <span class="hljs-number">0</span><br>        sentence_len = <span class="hljs-built_in">len</span>(sentence)<br>        indices = [word2Ind[i] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> sentence]<br>        <span class="hljs-keyword">while</span> current_index &lt; sentence_len:<br>            left  = <span class="hljs-built_in">max</span>(current_index - window_size, <span class="hljs-number">0</span>)<br>            right = <span class="hljs-built_in">min</span>(current_index + window_size + <span class="hljs-number">1</span>, sentence_len) <br>            current_word = sentence[current_index]<br>            current_word_index = word2Ind[current_word]<br>            words_around = indices[left:current_index] + indices[current_index+<span class="hljs-number">1</span>:right]<br>            <br>            <span class="hljs-keyword">for</span> ind <span class="hljs-keyword">in</span> words_around:<br>                M[current_word_index, ind] += <span class="hljs-number">1</span><br>            <br>            current_index += <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># ------------------</span><br><br>    <span class="hljs-keyword">return</span> M, word2Ind<br></code></pre></div></td></tr></table></figure><p>测试用例：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ---------------------</span><br><span class="hljs-comment"># Run this sanity check</span><br><span class="hljs-comment"># Note that this is not an exhaustive check for correctness.</span><br><span class="hljs-comment"># ---------------------</span><br><br><span class="hljs-comment"># Define toy corpus and get student&#x27;s co-occurrence matrix</span><br>test_corpus = [<span class="hljs-string">&quot;&#123;&#125; All that glitters isn&#x27;t gold &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(START_TOKEN, END_TOKEN).split(<span class="hljs-string">&quot; &quot;</span>), <span class="hljs-string">&quot;&#123;&#125; All&#x27;s well that ends well &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(START_TOKEN, END_TOKEN).split(<span class="hljs-string">&quot; &quot;</span>)]<br>M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=<span class="hljs-number">1</span>)<br><br><span class="hljs-comment"># Correct M and word2ind</span><br>M_test_ans = np.array( <br>    [[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>,],<br>     [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>,],<br>     [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>,],<br>     [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>,]]<br>)<br>ans_test_corpus_words = <span class="hljs-built_in">sorted</span>([START_TOKEN, <span class="hljs-string">&quot;All&quot;</span>, <span class="hljs-string">&quot;ends&quot;</span>, <span class="hljs-string">&quot;that&quot;</span>, <span class="hljs-string">&quot;gold&quot;</span>, <span class="hljs-string">&quot;All&#x27;s&quot;</span>, <span class="hljs-string">&quot;glitters&quot;</span>, <span class="hljs-string">&quot;isn&#x27;t&quot;</span>, <span class="hljs-string">&quot;well&quot;</span>, END_TOKEN])<br>word2ind_ans = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(ans_test_corpus_words, <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(ans_test_corpus_words))))<br><br><span class="hljs-comment"># Test correct word2ind</span><br><span class="hljs-keyword">assert</span> (word2ind_ans == word2ind_test), <span class="hljs-string">&quot;Your word2ind is incorrect:\nCorrect: &#123;&#125;\nYours: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(word2ind_ans, word2ind_test)<br><br><span class="hljs-comment"># Test correct M shape</span><br><span class="hljs-keyword">assert</span> (M_test.shape == M_test_ans.shape), <span class="hljs-string">&quot;M matrix has incorrect shape.\nCorrect: &#123;&#125;\nYours: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(M_test.shape, M_test_ans.shape)<br><br><span class="hljs-comment"># Test correct M values</span><br><span class="hljs-keyword">for</span> w1 <span class="hljs-keyword">in</span> word2ind_ans.keys():<br>    idx1 = word2ind_ans[w1]<br>    <span class="hljs-keyword">for</span> w2 <span class="hljs-keyword">in</span> word2ind_ans.keys():<br>        idx2 = word2ind_ans[w2]<br>        student = M_test[idx1, idx2]<br>        correct = M_test_ans[idx1, idx2]<br>        <span class="hljs-keyword">if</span> student != correct:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Correct M:&quot;</span>)<br>            <span class="hljs-built_in">print</span>(M_test_ans)<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Your M: &quot;</span>)<br>            <span class="hljs-built_in">print</span>(M_test)<br>            <span class="hljs-keyword">raise</span> AssertionError(<span class="hljs-string">&quot;Incorrect count at index (&#123;&#125;, &#123;&#125;)=(&#123;&#125;, &#123;&#125;) in matrix M. Yours has &#123;&#125; but should have &#123;&#125;.&quot;</span>.<span class="hljs-built_in">format</span>(idx1, idx2, w1, w2, student, correct))<br><br><span class="hljs-comment"># Print Success</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Passed All Tests!&quot;</span>)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br></code></pre></div></td></tr></table></figure><h3 id="问题1.3实现-reduce_to_k_dim">问题1.3：实现 reduce_to_k_dim</h3><p>这一步是降维。在问题1.2得到的是一个N xN的矩阵（N是单词集的大小），使用scikit-learn实现的SVD（奇异值分解），从这个大矩阵里分解出一个含k个特制的Nx k 小矩阵。</p><p><strong>注意</strong>：在numpy、scipy和scikit-learn都提供了一些SVD的实现，但是只有scipy、sklearn有TruncatedSVD，并且只有sklearn提供了计算大规模SVD的高效的randomized算法，详情参考<ahref="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">sklearn.decomposition.TruncatedSVD</a>。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reduce_to_k_dim</span>(<span class="hljs-params">M, k=<span class="hljs-number">2</span></span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)</span><br><span class="hljs-string">        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:</span><br><span class="hljs-string">            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html</span><br><span class="hljs-string">    </span><br><span class="hljs-string">        Params:</span><br><span class="hljs-string">            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts</span><br><span class="hljs-string">            k (int): embedding size of each word after dimension reduction</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.</span><br><span class="hljs-string">                    In terms of the SVD from math class, this actually returns U * S</span><br><span class="hljs-string">    &quot;&quot;&quot;</span>    <br>    n_iters = <span class="hljs-number">10</span>     <span class="hljs-comment"># Use this parameter in your call to `TruncatedSVD`</span><br>    M_reduced = <span class="hljs-literal">None</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Running Truncated SVD over %i words...&quot;</span> % (M.shape[<span class="hljs-number">0</span>]))<br>    <br>    <span class="hljs-comment"># ------------------</span><br>    <span class="hljs-comment"># Write your implementation here.</span><br>    TSVD = TruncatedSVD(n_components=k, n_iter=n_iters)<br>    M_reduced = TSVD.fit_transform(M)<br><br>    <span class="hljs-comment"># ------------------</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Done.&quot;</span>)<br>    <span class="hljs-keyword">return</span> M_reduced<br></code></pre></div></td></tr></table></figure><p>测试用例：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ---------------------</span><br><span class="hljs-comment"># Run this sanity check</span><br><span class="hljs-comment"># Note that this is not an exhaustive check for correctness </span><br><span class="hljs-comment"># In fact we only check that your M_reduced has the right dimensions.</span><br><span class="hljs-comment"># ---------------------</span><br><br><span class="hljs-comment"># Define toy corpus and run student code</span><br>test_corpus = [<span class="hljs-string">&quot;&#123;&#125; All that glitters isn&#x27;t gold &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(START_TOKEN, END_TOKEN).split(<span class="hljs-string">&quot; &quot;</span>), <span class="hljs-string">&quot;&#123;&#125; All&#x27;s well that ends well &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(START_TOKEN, END_TOKEN).split(<span class="hljs-string">&quot; &quot;</span>)]<br>M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=<span class="hljs-number">1</span>)<br>M_test_reduced = reduce_to_k_dim(M_test, k=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Test proper dimensions</span><br><span class="hljs-keyword">assert</span> (M_test_reduced.shape[<span class="hljs-number">0</span>] == <span class="hljs-number">10</span>), <span class="hljs-string">&quot;M_reduced has &#123;&#125; rows; should have &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(M_test_reduced.shape[<span class="hljs-number">0</span>], <span class="hljs-number">10</span>)<br><span class="hljs-keyword">assert</span> (M_test_reduced.shape[<span class="hljs-number">1</span>] == <span class="hljs-number">2</span>), <span class="hljs-string">&quot;M_reduced has &#123;&#125; columns; should have &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(M_test_reduced.shape[<span class="hljs-number">1</span>], <span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Print Success</span><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Passed All Tests!&quot;</span>)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br></code></pre></div></td></tr></table></figure><h3 id="问题1.4-实现-plot_embeddings">问题1.4 实现 plot_embeddings</h3><p>基于matplotlib，用<code>scatter</code> 画 “×”，用 <code>text</code>写字</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_embeddings</span>(<span class="hljs-params">M_reduced, word2Ind, words</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Plot in a scatterplot the embeddings of the words specified in the list &quot;words&quot;.</span><br><span class="hljs-string">        NOTE: do not plot all the words listed in M_reduced / word2Ind.</span><br><span class="hljs-string">        Include a label next to each point.</span><br><span class="hljs-string">        </span><br><span class="hljs-string">        Params:</span><br><span class="hljs-string">            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings</span><br><span class="hljs-string">            word2Ind (dict): dictionary that maps word to indices for matrix M</span><br><span class="hljs-string">            words (list of strings): words whose embeddings we want to visualize</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br><br>    <span class="hljs-comment"># ------------------</span><br>    <span class="hljs-comment"># Write your implementation here.</span><br>    <br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words:<br>        index = word2Ind[w]<br>        embedding = M_reduced[index]<br>        x, y  = embedding[<span class="hljs-number">0</span>], embedding[<span class="hljs-number">1</span>]<br>        plt.scatter(x, y, marker=<span class="hljs-string">&#x27;x&#x27;</span>, color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>        plt.text(x, y, word, fontsize=<span class="hljs-number">9</span>)<br>    plt.show()<br>    <span class="hljs-comment"># ------------------</span><br></code></pre></div></td></tr></table></figure><p>测试用例：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ---------------------</span><br><span class="hljs-comment"># Run this sanity check</span><br><span class="hljs-comment"># Note that this is not an exhaustive check for correctness.</span><br><span class="hljs-comment"># The plot produced should look like the &quot;test solution plot&quot; depicted below. </span><br><span class="hljs-comment"># ---------------------</span><br><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Outputted Plot:&quot;</span>)<br><br>M_reduced_plot_test = np.array([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>], [-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br>word2ind_plot_test = &#123;<span class="hljs-string">&#x27;test1&#x27;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;test2&#x27;</span>: <span class="hljs-number">1</span>, <span class="hljs-string">&#x27;test3&#x27;</span>: <span class="hljs-number">2</span>, <span class="hljs-string">&#x27;test4&#x27;</span>: <span class="hljs-number">3</span>, <span class="hljs-string">&#x27;test5&#x27;</span>: <span class="hljs-number">4</span>&#125;<br>words = [<span class="hljs-string">&#x27;test1&#x27;</span>, <span class="hljs-string">&#x27;test2&#x27;</span>, <span class="hljs-string">&#x27;test3&#x27;</span>, <span class="hljs-string">&#x27;test4&#x27;</span>, <span class="hljs-string">&#x27;test5&#x27;</span>]<br>plot_embeddings(M_reduced_plot_test, word2ind_plot_test, words)<br><br><span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;-&quot;</span> * <span class="hljs-number">80</span>)<br></code></pre></div></td></tr></table></figure><p>效果：</p><p><img src="/img/nlp/assignment1/1.jpg"/></p><h3 id="问题1.5共现打印分析">问题1.5：共现打印分析</h3><p>将词嵌入到2个维度上，归一化，最终词向量会落到一个单位圆内，在坐标系上寻找相近的词。</p><p>测试用例：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># -----------------------------</span><br><span class="hljs-comment"># Run This Cell to Produce Your Plot</span><br><span class="hljs-comment"># ------------------------------</span><br>reuters_corpus = read_corpus()<br>M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)<br>M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Rescale (normalize) the rows to make them each of unit-length</span><br>M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=<span class="hljs-number">1</span>)<br>M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] <span class="hljs-comment"># broadcasting</span><br><br>words = [<span class="hljs-string">&#x27;barrels&#x27;</span>, <span class="hljs-string">&#x27;bpd&#x27;</span>, <span class="hljs-string">&#x27;ecuador&#x27;</span>, <span class="hljs-string">&#x27;energy&#x27;</span>, <span class="hljs-string">&#x27;industry&#x27;</span>, <span class="hljs-string">&#x27;kuwait&#x27;</span>, <span class="hljs-string">&#x27;oil&#x27;</span>, <span class="hljs-string">&#x27;output&#x27;</span>, <span class="hljs-string">&#x27;petroleum&#x27;</span>, <span class="hljs-string">&#x27;iraq&#x27;</span>]<br><br>plot_embeddings(M_normalized, word2ind_co_occurrence, words)<br></code></pre></div></td></tr></table></figure><p><img src="/img/nlp/assignment1/2.jpg"/></p><h2 id="part-2基于预测的词向量">Part 2：基于预测的词向量</h2><p>目前，基于预测的词向量是最流行的，比如word2vec。现在我们来探索word2vec生成的词向量，如果想要深入了解，可以读一读<ahref="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">原始论文</a>。</p><p>这一部分主要是使用gensim探索词向量，不是自己实现word2vec，所使用的词向量维度是300，由google发布。</p><p>首先使用SVD降维，将300维降2维，方便打印查看。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">load_embedding_model</span>():<br>    <span class="hljs-string">&quot;&quot;&quot; Load GloVe Vectors</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            wv_from_bin: All 400000 embeddings, each lengh 200</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> gensim.downloader <span class="hljs-keyword">as</span> api<br>    wv_from_bin = api.load(<span class="hljs-string">&quot;glove-wiki-gigaword-200&quot;</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loaded vocab size %i&quot;</span> % <span class="hljs-built_in">len</span>(wv_from_bin.vocab.keys()))<br>    <span class="hljs-keyword">return</span> wv_from_bin<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># -----------------------------------</span><br><span class="hljs-comment"># Run Cell to Load Word Vectors</span><br><span class="hljs-comment"># Note: This will take a couple minutes</span><br><span class="hljs-comment"># -----------------------------------</span><br>wv_from_bin = load_embedding_model()<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">get_matrix_of_vectors</span>(<span class="hljs-params">wv_from_bin, required_words=[<span class="hljs-string">&#x27;barrels&#x27;</span>, <span class="hljs-string">&#x27;bpd&#x27;</span>, <span class="hljs-string">&#x27;ecuador&#x27;</span>, <span class="hljs-string">&#x27;energy&#x27;</span>, <span class="hljs-string">&#x27;industry&#x27;</span>, <span class="hljs-string">&#x27;kuwait&#x27;</span>, <span class="hljs-string">&#x27;oil&#x27;</span>, <span class="hljs-string">&#x27;output&#x27;</span>, <span class="hljs-string">&#x27;petroleum&#x27;</span>, <span class="hljs-string">&#x27;iraq&#x27;</span>]</span>):<br>    <span class="hljs-string">&quot;&quot;&quot; Put the GloVe vectors into a matrix M.</span><br><span class="hljs-string">        Param:</span><br><span class="hljs-string">            wv_from_bin: KeyedVectors object; the 400000 GloVe vectors loaded from file</span><br><span class="hljs-string">        Return:</span><br><span class="hljs-string">            M: numpy matrix shape (num words, 200) containing the vectors</span><br><span class="hljs-string">            word2ind: dictionary mapping each word to its row number in M</span><br><span class="hljs-string">    &quot;&quot;&quot;</span><br>    <span class="hljs-keyword">import</span> random<br>    words = <span class="hljs-built_in">list</span>(wv_from_bin.vocab.keys())<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Shuffling words ...&quot;</span>)<br>    random.seed(<span class="hljs-number">224</span>)<br>    random.shuffle(words)<br>    words = words[:<span class="hljs-number">10000</span>]<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Putting %i words into word2ind and matrix M...&quot;</span> % <span class="hljs-built_in">len</span>(words))<br>    word2ind = &#123;&#125;<br>    M = []<br>    curInd = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> words:<br>        <span class="hljs-keyword">try</span>:<br>            M.append(wv_from_bin.word_vec(w))<br>            word2ind[w] = curInd<br>            curInd += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">except</span> KeyError:<br>            <span class="hljs-keyword">continue</span><br>    <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> required_words:<br>        <span class="hljs-keyword">if</span> w <span class="hljs-keyword">in</span> words:<br>            <span class="hljs-keyword">continue</span><br>        <span class="hljs-keyword">try</span>:<br>            M.append(wv_from_bin.word_vec(w))<br>            word2ind[w] = curInd<br>            curInd += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">except</span> KeyError:<br>            <span class="hljs-keyword">continue</span><br>    M = np.stack(M)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Done.&quot;</span>)<br>    <span class="hljs-keyword">return</span> M, word2ind<br></code></pre></div></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># -----------------------------------------------------------------</span><br><span class="hljs-comment"># Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions</span><br><span class="hljs-comment"># Note: This should be quick to run</span><br><span class="hljs-comment"># -----------------------------------------------------------------</span><br>M, word2ind = get_matrix_of_vectors(wv_from_bin)<br>M_reduced = reduce_to_k_dim(M, k=<span class="hljs-number">2</span>)<br><br><span class="hljs-comment"># Rescale (normalize) the rows to make them each of unit-length</span><br>M_lengths = np.linalg.norm(M_reduced, axis=<span class="hljs-number">1</span>)<br>M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] <span class="hljs-comment"># broadcasting</span><br></code></pre></div></td></tr></table></figure><h3 id="问题2.1word2vec打印分析">问题2.1：word2vec打印分析</h3><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">words = [<span class="hljs-string">&#x27;barrels&#x27;</span>, <span class="hljs-string">&#x27;bpd&#x27;</span>, <span class="hljs-string">&#x27;ecuador&#x27;</span>, <span class="hljs-string">&#x27;energy&#x27;</span>, <span class="hljs-string">&#x27;industry&#x27;</span>, <span class="hljs-string">&#x27;kuwait&#x27;</span>, <span class="hljs-string">&#x27;oil&#x27;</span>, <span class="hljs-string">&#x27;output&#x27;</span>, <span class="hljs-string">&#x27;petroleum&#x27;</span>, <span class="hljs-string">&#x27;iraq&#x27;</span>]<br>plot_embeddings(M_reduced_normalized, word2ind, words)<br></code></pre></div></td></tr></table></figure><p>和问题1.5一样</p><h3 id="问题2.2一词多义">问题2.2：一词多义</h3><p>找到一个有多个含义的词（比如“leaves”，”scoop”），这种词的top-10相似词（根据余弦相似度）里有两个词的意思不一样。比如”leaves”（叶子，花瓣）的top-10词里有”vanishes”（消失）和”stalks”（茎秆）。</p><p><strong>Note</strong>: You should use the<code>wv_from_bin.most_similar(word)</code> function to get the top 10similar words. This function ranks all other words in the vocabularywith respect to their cosine similarity to the given word. For furtherassistance, please check the <strong><ahref="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar">GenSimdocumentation</a></strong>.</p><p>这里我找到的词是”column”（列），它的top-10里有”columnist”（专栏作家）和”article”（文章）</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ------------------</span><br><span class="hljs-comment"># Write your polysemous word exploration code here.</span><br><br>wv_from_bin.most_similar(<span class="hljs-string">&quot;column&quot;</span>)<br><br><span class="hljs-comment"># ------------------</span><br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight scheme"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs scheme">[(<span class="hljs-symbol">&#x27;columns</span>&#x27;, <span class="hljs-number">0.767943263053894</span>),<br> (<span class="hljs-symbol">&#x27;columnist</span>&#x27;, <span class="hljs-number">0.6541407108306885</span>),<br> (<span class="hljs-symbol">&#x27;article</span>&#x27;, <span class="hljs-number">0.651928186416626</span>),<br> (<span class="hljs-symbol">&#x27;columnists</span>&#x27;, <span class="hljs-number">0.617466926574707</span>),<br> (<span class="hljs-symbol">&#x27;syndicated_column</span>&#x27;, <span class="hljs-number">0.599014401435852</span>),<br> (<span class="hljs-symbol">&#x27;op_ed</span>&#x27;, <span class="hljs-number">0.588202714920044</span>),<br> (<span class="hljs-symbol">&#x27;Op_Ed</span>&#x27;, <span class="hljs-number">0.5801560282707214</span>),<br> (<span class="hljs-symbol">&#x27;op_ed_column</span>&#x27;, <span class="hljs-number">0.5779396891593933</span>),<br> (<span class="hljs-symbol">&#x27;nationally_syndicated_column</span>&#x27;, <span class="hljs-number">0.572504997253418</span>),<br> (<span class="hljs-symbol">&#x27;colum</span>&#x27;, <span class="hljs-number">0.5595961213111877</span>)]<br></code></pre></div></td></tr></table></figure><h3 id="问题2.3近义词和反义词">问题2.3：近义词和反义词</h3><p>找到三个词(w1, w2,w3)，其中w1和w2是近义词，w1和w3是反义词，但是w1和w3的距离&lt;w1和w2的距离。例如：w1=”happy”，w2=”cheerful”，w3=”sad”</p><p>You should use the the <code>wv_from_bin.distance(w1, w2)</code>function here in order to compute the cosine distance between two words.Please see the <strong><ahref="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance">GenSimdocumentation</a></strong> for further assistance.</p><p>为什么反义词的相似度反而更大呢（距离越小说明越相似）？因为他们的上下文通常非常一致</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ------------------</span><br><span class="hljs-comment"># Write your synonym &amp; antonym exploration code here.</span><br><br>w1 = <span class="hljs-string">&quot;love&quot;</span><br>w2 = <span class="hljs-string">&quot;like&quot;</span><br>w3 = <span class="hljs-string">&quot;hate&quot;</span><br>w1_w2_dist = wv_from_bin.distance(w1, w2)<br>w1_w3_dist = wv_from_bin.distance(w1, w3)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Synonyms &#123;&#125;, &#123;&#125; have cosine distance: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(w1, w2, w1_w2_dist))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Antonyms &#123;&#125;, &#123;&#125; have cosine distance: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(w1, w3, w1_w3_dist))<br><br><span class="hljs-comment"># ------------------</span><br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight apache"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs apache"><span class="hljs-attribute">Synonyms</span> love, like have cosine distance: <span class="hljs-number">0</span>.<span class="hljs-number">6328612565994263</span><br><span class="hljs-attribute">Antonyms</span> love, hate have cosine distance: <span class="hljs-number">0</span>.<span class="hljs-number">39960432052612305</span><br></code></pre></div></td></tr></table></figure><h3 id="问题2.4类比">问题2.4：类比</h3><p>man 对于king，相当于woman对于___，这样的问题也可以用word2vec来解决，关于most_similar的详细用法可以参考<ahref="https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar">GenSim文档</a>。</p><p>这里我们找另外一组类比</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ------------------</span><br><span class="hljs-comment"># Write your analogy exploration code here.</span><br><span class="hljs-comment"># man : him :: woman : her</span><br>pprint.pprint(wv_from_bin.most_similar(positive=[<span class="hljs-string">&#x27;woman&#x27;</span>, <span class="hljs-string">&#x27;him&#x27;</span>], negative=[<span class="hljs-string">&#x27;man&#x27;</span>]))<br><br><span class="hljs-comment"># ------------------</span><br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight scheme"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs scheme">[(<span class="hljs-symbol">&#x27;her</span>&#x27;, <span class="hljs-number">0.694490909576416</span>),<br> (<span class="hljs-symbol">&#x27;she</span>&#x27;, <span class="hljs-number">0.6385233402252197</span>),<br> (<span class="hljs-symbol">&#x27;me</span>&#x27;, <span class="hljs-number">0.628451406955719</span>),<br> (<span class="hljs-symbol">&#x27;herself</span>&#x27;, <span class="hljs-number">0.6239798665046692</span>),<br> (<span class="hljs-symbol">&#x27;them</span>&#x27;, <span class="hljs-number">0.5843966007232666</span>),<br> (<span class="hljs-symbol">&#x27;She</span>&#x27;, <span class="hljs-number">0.5237804651260376</span>),<br> (<span class="hljs-symbol">&#x27;myself</span>&#x27;, <span class="hljs-number">0.4885627031326294</span>),<br> (<span class="hljs-symbol">&#x27;saidshe</span>&#x27;, <span class="hljs-number">0.48337966203689575</span>),<br> (<span class="hljs-symbol">&#x27;he</span>&#x27;, <span class="hljs-number">0.48184287548065186</span>),<br> (<span class="hljs-symbol">&#x27;Gail_Quets</span>&#x27;, <span class="hljs-number">0.4784894585609436</span>)]<br></code></pre></div></td></tr></table></figure><p>可以看到正确的计算出了”her”</p><h3 id="问题2.5错误的类比">问题2.5：错误的类比</h3><p>找到一个错误的类比，树：树叶 ：：花：花瓣</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ------------------</span><br><span class="hljs-comment"># Write your incorrect analogy exploration code here.</span><br><span class="hljs-comment"># tree : leaf :: flower : petal</span><br>pprint.pprint(wv_from_bin.most_similar(positive=[<span class="hljs-string">&#x27;leaf&#x27;</span>, <span class="hljs-string">&#x27;flower&#x27;</span>], negative=[<span class="hljs-string">&#x27;tree&#x27;</span>]))<br><br><span class="hljs-comment"># ------------------</span><br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight scheme"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs scheme">[(<span class="hljs-symbol">&#x27;floral</span>&#x27;, <span class="hljs-number">0.5532568693161011</span>),<br> (<span class="hljs-symbol">&#x27;marigold</span>&#x27;, <span class="hljs-number">0.5291938185691833</span>),<br> (<span class="hljs-symbol">&#x27;tulip</span>&#x27;, <span class="hljs-number">0.521312952041626</span>),<br> (<span class="hljs-symbol">&#x27;rooted_cuttings</span>&#x27;, <span class="hljs-number">0.5189826488494873</span>),<br> (<span class="hljs-symbol">&#x27;variegation</span>&#x27;, <span class="hljs-number">0.5136324763298035</span>),<br> (<span class="hljs-symbol">&#x27;Asiatic_lilies</span>&#x27;, <span class="hljs-number">0.5132641792297363</span>),<br> (<span class="hljs-symbol">&#x27;gerberas</span>&#x27;, <span class="hljs-number">0.5106234550476074</span>),<br> (<span class="hljs-symbol">&#x27;gerbera_daisies</span>&#x27;, <span class="hljs-number">0.5101010203361511</span>),<br> (<span class="hljs-symbol">&#x27;Verbena_bonariensis</span>&#x27;, <span class="hljs-number">0.5070016980171204</span>),<br> (<span class="hljs-symbol">&#x27;violet</span>&#x27;, <span class="hljs-number">0.5058108568191528</span>)]<br></code></pre></div></td></tr></table></figure><p>结果输出的里面没有“花瓣”</p><h3 id="问题2.6偏见分析">问题2.6：偏见分析</h3><p>注意偏见是很重要的比如性别歧视、种族歧视等，执行下面代码，分析两个问题：</p><ol type="a"><li><p>哪个词与“woman”和“boss”最相似，和“man”最不相似?</p></li><li><p>哪个词与“man”和“boss”最相似，和“woman”最不相似?</p></li></ol><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Run this cell</span><br><span class="hljs-comment"># Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be</span><br><span class="hljs-comment"># most dissimilar from.</span><br>pprint.pprint(wv_from_bin.most_similar(positive=[<span class="hljs-string">&#x27;woman&#x27;</span>, <span class="hljs-string">&#x27;boss&#x27;</span>], negative=[<span class="hljs-string">&#x27;man&#x27;</span>]))<br><span class="hljs-built_in">print</span>()<br>pprint.pprint(wv_from_bin.most_similar(positive=[<span class="hljs-string">&#x27;man&#x27;</span>, <span class="hljs-string">&#x27;boss&#x27;</span>], negative=[<span class="hljs-string">&#x27;woman&#x27;</span>]))<br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight scheme"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs scheme">[(<span class="hljs-symbol">&#x27;bosses</span>&#x27;, <span class="hljs-number">0.5522644519805908</span>),<br> (<span class="hljs-symbol">&#x27;manageress</span>&#x27;, <span class="hljs-number">0.49151360988616943</span>),<br> (<span class="hljs-symbol">&#x27;exec</span>&#x27;, <span class="hljs-number">0.45940813422203064</span>),<br> (<span class="hljs-symbol">&#x27;Manageress</span>&#x27;, <span class="hljs-number">0.45598435401916504</span>),<br> (<span class="hljs-symbol">&#x27;receptionist</span>&#x27;, <span class="hljs-number">0.4474116563796997</span>),<br> (<span class="hljs-symbol">&#x27;Jane_Danson</span>&#x27;, <span class="hljs-number">0.44480544328689575</span>),<br> (<span class="hljs-symbol">&#x27;Fiz_Jennie_McAlpine</span>&#x27;, <span class="hljs-number">0.44275766611099243</span>),<br> (<span class="hljs-symbol">&#x27;Coronation_Street_actress</span>&#x27;, <span class="hljs-number">0.44275566935539246</span>),<br> (<span class="hljs-symbol">&#x27;supremo</span>&#x27;, <span class="hljs-number">0.4409853219985962</span>),<br> (<span class="hljs-symbol">&#x27;coworker</span>&#x27;, <span class="hljs-number">0.43986251950263977</span>)]<br><br>[(<span class="hljs-symbol">&#x27;supremo</span>&#x27;, <span class="hljs-number">0.6097398400306702</span>),<br> (<span class="hljs-symbol">&#x27;MOTHERWELL_boss</span>&#x27;, <span class="hljs-number">0.5489562153816223</span>),<br> (<span class="hljs-symbol">&#x27;CARETAKER_boss</span>&#x27;, <span class="hljs-number">0.5375303626060486</span>),<br> (<span class="hljs-symbol">&#x27;Bully_Wee_boss</span>&#x27;, <span class="hljs-number">0.5333974361419678</span>),<br> (<span class="hljs-symbol">&#x27;YEOVIL_Town_boss</span>&#x27;, <span class="hljs-number">0.5321705341339111</span>),<br> (<span class="hljs-symbol">&#x27;head_honcho</span>&#x27;, <span class="hljs-number">0.5281980037689209</span>),<br> (<span class="hljs-symbol">&#x27;manager_Stan_Ternent</span>&#x27;, <span class="hljs-number">0.525971531867981</span>),<br> (<span class="hljs-symbol">&#x27;Viv_Busby</span>&#x27;, <span class="hljs-number">0.5256162881851196</span>),<br> (<span class="hljs-symbol">&#x27;striker_Gabby_Agbonlahor</span>&#x27;, <span class="hljs-number">0.5250812768936157</span>),<br> (<span class="hljs-symbol">&#x27;BARNSLEY_boss</span>&#x27;, <span class="hljs-number">0.5238943099975586</span>)]<br></code></pre></div></td></tr></table></figure><p>第一个类比 男人:女人 ::老板:___，最合适的词应该是”landlady”（老板娘）之类的，但是top-10里只有”manageress”（女经理），”receptionist”（接待员）之类的词。</p><p>第二个类比 女人:男人 :: 老板:___，输出的不知道是些什么东西/捂脸</p><h3 id="问题2.7自行分析偏见">问题2.7：自行分析偏见</h3><p>这里我找的例子是：</p><ul><li>男人:女人 :: 医生:___</li><li>女人:男人 :: 医生:___</li></ul><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># ------------------</span><br><span class="hljs-comment"># Write your bias exploration code here.</span><br><br>pprint.pprint(wv_from_bin.most_similar(positive=[<span class="hljs-string">&#x27;woman&#x27;</span>, <span class="hljs-string">&#x27;doctor&#x27;</span>], negative=[<span class="hljs-string">&#x27;man&#x27;</span>]))<br><span class="hljs-built_in">print</span>()<br>pprint.pprint(wv_from_bin.most_similar(positive=[<span class="hljs-string">&#x27;man&#x27;</span>, <span class="hljs-string">&#x27;doctor&#x27;</span>], negative=[<span class="hljs-string">&#x27;woman&#x27;</span>]))<br><br><span class="hljs-comment"># ------------------</span><br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight scheme"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs scheme">[(<span class="hljs-symbol">&#x27;gynecologist</span>&#x27;, <span class="hljs-number">0.7093892097473145</span>),<br> (<span class="hljs-symbol">&#x27;nurse</span>&#x27;, <span class="hljs-number">0.647728681564331</span>),<br> (<span class="hljs-symbol">&#x27;doctors</span>&#x27;, <span class="hljs-number">0.6471461057662964</span>),<br> (<span class="hljs-symbol">&#x27;physician</span>&#x27;, <span class="hljs-number">0.64389967918396</span>),<br> (<span class="hljs-symbol">&#x27;pediatrician</span>&#x27;, <span class="hljs-number">0.6249487996101379</span>),<br> (<span class="hljs-symbol">&#x27;nurse_practitioner</span>&#x27;, <span class="hljs-number">0.6218312978744507</span>),<br> (<span class="hljs-symbol">&#x27;obstetrician</span>&#x27;, <span class="hljs-number">0.6072014570236206</span>),<br> (<span class="hljs-symbol">&#x27;ob_gyn</span>&#x27;, <span class="hljs-number">0.5986712574958801</span>),<br> (<span class="hljs-symbol">&#x27;midwife</span>&#x27;, <span class="hljs-number">0.5927063226699829</span>),<br> (<span class="hljs-symbol">&#x27;dermatologist</span>&#x27;, <span class="hljs-number">0.5739566683769226</span>)]<br><br>[(<span class="hljs-symbol">&#x27;physician</span>&#x27;, <span class="hljs-number">0.6463665962219238</span>),<br> (<span class="hljs-symbol">&#x27;doctors</span>&#x27;, <span class="hljs-number">0.5858404040336609</span>),<br> (<span class="hljs-symbol">&#x27;surgeon</span>&#x27;, <span class="hljs-number">0.5723941326141357</span>),<br> (<span class="hljs-symbol">&#x27;dentist</span>&#x27;, <span class="hljs-number">0.552364706993103</span>),<br> (<span class="hljs-symbol">&#x27;cardiologist</span>&#x27;, <span class="hljs-number">0.5413815975189209</span>),<br> (<span class="hljs-symbol">&#x27;neurologist</span>&#x27;, <span class="hljs-number">0.5271126627922058</span>),<br> (<span class="hljs-symbol">&#x27;neurosurgeon</span>&#x27;, <span class="hljs-number">0.5249835848808289</span>),<br> (<span class="hljs-symbol">&#x27;urologist</span>&#x27;, <span class="hljs-number">0.5247740149497986</span>),<br> (<span class="hljs-symbol">&#x27;Doctor</span>&#x27;, <span class="hljs-number">0.5240625143051147</span>),<br> (<span class="hljs-symbol">&#x27;internist</span>&#x27;, <span class="hljs-number">0.5183224081993103</span>)]<br></code></pre></div></td></tr></table></figure><p>第一个类比中，我们看到了”nurse”（护士），这是一个有偏见的类比</p><h3 id="问题2.8思考偏见问题">问题2.8：思考偏见问题</h3><p>什么会导致词向量里的偏见？</p><p>因为数据集中有偏见</p><h2 id="参考">参考</h2><p>[1] CS224n: Natural Language Processing with Deep Learning,2019-03-12. http://web.stanford.edu/class/cs224n.</p><p>[2]https://github.com/ShowMeAI-Hub/awesome-AI-courses-notes-cheatsheets/tree/main/CS224n-Natural-Language-Processing-with-Deep-Learning/assignment-solutions/Assignment1</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第二讲-词向量进阶</title>
    <link href="/2022/05/05/%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%9B%E9%98%B6/"/>
    <url>/2022/05/05/%E7%AC%AC%E4%BA%8C%E8%AE%B2-%E8%AF%8D%E5%90%91%E9%87%8F%E8%BF%9B%E9%98%B6/</url>
    
    <content type="html"><![CDATA[<h1 id="算法优化基础">1 算法优化基础</h1><h2 id="词向量梯度下降算法">1.1 词向量梯度下降算法</h2><p>问题：梯度下降会一次性使用所有数据样本进行参数更新，对应到我们当前的词向量建模问题，就是<spanclass="math inline">\(J(\theta)\)</span>的计算需要基于语料库所有的样本(窗口)，数据规模非常大：</p><ul><li>计算非常耗资源</li><li>计算时间长</li></ul><p>解决方案：随机梯度下降算法 Stochastic Gradient Descent（SGD）</p><ul><li>在单个样本中计算和更新参数，并遍历所有样本</li></ul><p>但基于单个样本更新会表现为参数震荡很厉害，收敛过程并不平稳，所以很多时候我们会改为使用<strong>mini-batchgradient descent</strong></p><h2 id="词向量建模中的随机梯度下降">1.2 词向量建模中的随机梯度下降</h2><ul><li>应用随机梯度下降，在每个窗口计算和更新参数，遍历所有样本</li><li>在每个窗口内，我们最多只有<spanclass="math inline">\(2m+1\)</span>个词，因此<spanclass="math inline">\(\nabla_{\theta}J_{t}(\theta)\)</span>是非常稀疏的</li></ul><p><span class="math display">\[\nabla_{\theta} J_{t}(\theta)=\left[\begin{array}{l}0 \\\vdots \\\nabla_{v_{\text {like }}} \\\vdots \\0 \\\nabla_{u_{I}} \\\vdots \\\nabla_{u_{\text {learning }}} \\\vdots\end{array}\right] \in \mathbb{R}^{2 d V}\]</span></p><p>上面提到的稀疏性问题，一种解决方式是我们<strong>只更新实际出现的向量</strong></p><ul><li>需要稀疏矩阵更新操作来只更新矩阵<spanclass="math inline">\(U\)</span>和<spanclass="math inline">\(V\)</span>中的特定行</li><li>需要保留单词向量的哈希/散列</li></ul><p>如果有数百万个单词向量，并且进行分布式计算，我们无需再传输巨大的更新信息（数据传输有成本）</p><h2 id="word2vec的更多细节">1.3 Word2vec的更多细节</h2><p>word2vec有两个模型变体：</p><ul><li>1.Skip-grams (SG)：输入中心词并预测上下文中的单词</li><li>2.Continuous Bag of Words(CBOW)：输入上下文中的单词并预测中心词</li></ul><p>之前一直使用naive的softmax(简单但代价很高的训练方法)，其实可以使用负采样方法加快训练速率</p><h2 id="负例采样的skip-gram模型作业2">1.4负例采样的skip-gram模型（作业2）</h2><p>softmax中用于归一化的分母的计算代价太高</p><ul><li>我们将在作业2中实现使用 negative sampling/负例采样方法的 skip-gram模型。</li><li>使用一个 true pair (中心词及其上下文窗口中的词)与几个 noise pair(中心词与随机词搭配) 形成的样本，训练二元逻辑回归。</li></ul><p>原文中的(最大化)目标函数是：<spanclass="math inline">\(J(\theta)=\frac{1}{T} \sum_{t=1}^{T}J_{t}(\theta)\)</span> <span class="math display">\[J_{t}(\theta)=\log \sigma\left(u_{o}^{T} v_{c}\right)+\sum_{i=1}^{k}\mathbb{E}_{j \sim P(w)}\left[\log \sigma\left(-u_{j}^{T}v_{c}\right)\right]\]</span></p><ul><li>左侧为sigmoid函数(大家会在后续的内容里经常见到它)</li><li>我们要最大化2个词共现的概率</li></ul><p>本课以及作业中的目标函数是: <span class="math display">\[J_{n e g-s a m p l e}\left(\boldsymbol{o}, \boldsymbol{v}_{c},\boldsymbol{U}\right)=-\log \left(\sigma\left(\boldsymbol{u}_{o}^{\top}\boldsymbol{v}_{c}\right)\right)-\sum_{k=1}^{K} \log\left(\sigma\left(-\boldsymbol{u}_{k}^{\top}\boldsymbol{v}_{c}\right)\right)\]</span></p><ul><li>我们取<span class="math inline">\(K\)</span>个负例采样</li><li>最大化窗口中包围「中心词」的这些词语出现的概率，而最小化其他没有出现的随机词的概率</li><li><span class="math inline">\(P(w)=U(w)^{3 / 4} /Z\)</span>我们用左侧的公式进行抽样，其中<spanclass="math inline">\(U(w)\)</span>​是 unigram 分布</li><li>通过 3/4 次方，相对减少常见单词的频率，增大稀有词的概率</li><li><span class="math inline">\(Z\)</span>用于生成概率分布</li></ul><h1 id="计数与共线矩阵">2 计数与共线矩阵</h1><h2 id="共现矩阵与词向量构建">2.1 共现矩阵与词向量构建</h2><p>在自然语言处理里另外一个构建词向量的思路是借助于<strong>共现矩阵</strong>（我们设其为<spanclass="math inline">\(X\)</span>），我们有两种方式，可以基于窗口（window）或者全文档（fulldocument)统计：</p><ul><li><strong>Window</strong>：与word2vec类似，在每个单词周围都使用Window，包括语法(POS)和语义信息</li><li><strong>Word-document</strong>共现矩阵的基本假设是在同一篇文章中出现的单词更有可能相互关联。假设单词<spanclass="math inline">\(i\)</span>出现在文章<spanclass="math inline">\(j\)</span>中，则矩阵元素<spanclass="math inline">\(X_{ij}\)</span>加一，当我们处理完数据库中的所有文章后，就得到了矩阵<spanclass="math inline">\(X\)</span>，其大小为<spanclass="math inline">\(|V| \times M\)</span>，其中<spanclass="math inline">\(|V|\)</span>为词汇量，而<spanclass="math inline">\(M\)</span>为文章数。这一构建单词文章co-occurrencematrix的方法也是经典的Latent SemanticAnalysis所采用的【语义分析】。</li></ul><h2 id="基于窗口的共现矩阵示例">2.2 基于窗口的共现矩阵示例</h2><p>利用某个定长窗口(通常取5-10)中单词与单词同时出现的次数，来产生基于窗口的共现矩阵。</p><p>下面以窗口长度为1来举例，假设我们的数据包含以下几个句子：</p><ul><li>I like deep learning.</li><li>I like NLP.</li><li>I enjoy flying.</li></ul><p>我们可以得到如下的词词共现矩阵（word-word co-occurrence matrix）</p><p><img src="/img/nlp/第二讲/1.png" /></p><h2 id="基于直接的共现矩阵构建词向量的问题">2.3基于直接的共现矩阵构建词向量的问题</h2><p>直接基于共现矩阵构建词向量，会有一些明显的问题，如下：</p><ul><li>使用共现次数衡量单词的相似性，但是会随着词汇量的增加而增大矩阵的大小。</li><li>需要很多空间来存储这一高维矩阵。</li><li>后续的分类模型也会由于矩阵的稀疏性而存在稀疏性问题，使得效果不佳。</li></ul><h2 id="解决方案低维向量">2.4 解决方案：低维向量</h2><p>针对上述问题，我们的一个处理方式是降维，获得低维稠密向量。</p><ul><li>通常降维到(25-1000)维，和word2vec类似</li></ul><p>如何降维呢？</p><h2 id="方法1对x进行降维作业1">2.5 方法1：对X进行降维（作业1）</h2><p>可以使用奇异值分解（SVD）方法将共现矩阵<spanclass="math inline">\(X\)</span>分解为<span class="math inline">\(U\Sigma V^{T}\)</span>，其中：</p><ul><li><spanclass="math inline">\(\Sigma\)</span>是对角线矩阵，对角线上的值是矩阵的奇异值</li><li><span class="math inline">\(U\)</span>，<spanclass="math inline">\(V\)</span>是对应于行和列的正交基</li></ul><p>为了减少尺度同时尽量保存有效信息，可保留对角矩阵的最大的<spanclass="math inline">\(k\)</span>个值，并将矩阵<spanclass="math inline">\(U\)</span>，<spanclass="math inline">\(V\)</span>的相应的行列保留。</p><ul><li>这是经典的线性代数算法，对于大型矩阵而言，计算代价昂贵。</li></ul><h2 id="词向量svd分解的python代码示例">2.6词向量SVD分解的python代码示例</h2><p><img src="/img/nlp/第二讲/2.png" /></p><p>将向量进行可视化</p><p><img src="/img/nlp/第二讲/3.png" /></p><h2 id="论文讲解">2.7 论文讲解</h2><h3 id="hacks-to-x-several-used-in-rohde-et-al.-2005">Hacks to X(several used in Rohde et al. 2005)</h3><p>按比例调整 counts 会很有效</p><ul><li>对高频词进行缩放(语法有太多的影响)<ul><li>使用log进行缩放</li><li><span class="math inline">\(\min (X, t), t \approx 100\)</span></li><li>直接全部忽视</li></ul></li><li>在基于window的计数中，提高更加接近的单词的计数</li><li>使用Person相关系数</li></ul><h2 id="词向量分布探究">2.8 词向量分布探究</h2><p>如果对词向量进行空间分布，会发现同一个词汇的附近分布着它不同时态语态的单词：</p><ul><li><span class="math inline">\(dirve \to deriver\)</span></li><li><span class="math inline">\(swim \to swimmer\)</span></li><li><span class="math inline">\(teach \to teacher\)</span></li></ul><p>在向量中出现的有趣的句法模式：语义向量基本上是线性组件，虽然有一些摆动，但是基本是存在动词和动词实施者的方向。</p><h2 id="基于计数-vs.-基于预估">2.9 基于计数 VS. 基于预估</h2><p>我们来总结一下基于共现矩阵计数和基于预估模型两种得到词向量的方式</p><p><strong>基于计数</strong>：使用整个矩阵的全局统计数据来直接估计</p><ul><li><strong>优点</strong>：训练快速；统计数据高效利用</li><li><strong>缺点</strong>：主要用于捕捉单词相似性；对大量数据给予比例失调的重视</li></ul><p><strong>基于预估模型</strong>：定义概率分布并试图预测单词</p><ul><li><strong>优点</strong>：提高其他任务的性能；能捕获除了单词相似性以外的复杂的模式</li><li><strong>缺点</strong>：随语料库增大会增大规模；统计数据的低效使用（采样是对统计数据的低效使用）</li></ul><h1 id="glove模型">3 GloVe模型</h1><h2 id="论文讲解-1">3.1 论文讲解</h2><h3 id="encoding-meaning-in-vector-differences">3.1.1 Encoding meaningin vector differences</h3><p>GloVe模型关键思想：共现概率的比值可以对meaningcomponent进行编码。将两个流派的想法结合起来，在神经网络中使用计数矩阵。</p><p><img src="/img/nlp/第二讲/4.png" /></p><p>例如我们想区分热力学上两种不同状态ice冰与蒸汽steam，如果只是看概率则数值很小，不能透露有效的信息，但是他们的比值比较大，所以使用比值更能体现信息。图中可以看出solid更常来描述ice的状态而不是steam，所以solid在ice的上下文中出现的几率更大。对于gas则恰恰相反，而对于water这种描述ice与steam均可或者fashion这种与两者都没什么联系的单词，则比值接近于1。所以相较于单纯的共现概率，实际上共现概率的相对比值更有意义。</p><h3id="combining-the-best-of-both-worlds-glove-pennington-et-al.-emnlp-2014">3.1.2Combining the best of both worlds GloVe [Pennington et al., EMNLP2014]</h3><p><span class="math display">\[w_i \cdot w_j = \log P(i|j)\]</span></p><p><span class="math display">\[J=\sum_{i, j=1}^{V} f\left(X_{i j}\right)\left(w_{i}^{T}\tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log X_{i j}\right)^{2}\]</span></p><ul><li>训练快速</li><li>可以扩展到大型语料库</li><li>即使是小语料库和小向量，性能也很好</li></ul><h2 id="glove的一些结果展示">3.2 GloVe的一些结果展示</h2><p>下图是一个GloVe词向量示例，我们通过GloVe得到的词向量，我们可以找到frog（青蛙）最接近的一些词汇，可以看出它们本身是很类似的动物。</p><p><img src="/img/nlp/第二讲/5.png" /></p><h1 id="词向量估计">4 词向量估计</h1><h2 id="如何评估词向量">4.1 如何评估词向量？</h2><p>我们如何评估词向量呢，有内在和外在两种方式：</p><ul><li><strong>内在评估方式</strong><ul><li>对特定/中间子任务进行评估</li><li>计算速度快</li><li>有助于理解这个系统</li><li>不清楚是否真的有用，除非与实际任务建立了相关性</li></ul></li><li><strong>外部任务方式</strong><ul><li>对真实任务（如下游NLP任务）的评估</li><li>计算精确度可能需要很长时间</li><li>不清楚子系统问题所在，是交互还是其他子系统问题</li><li>如果用另一个子系统替换一个子系统可以提高精确度</li></ul></li></ul><h2 id="内在词向量评估">4.2 内在词向量评估</h2><p>一种内在词向量评估方式是「<strong>词向量类比</strong>」：对于具备某种关系的词对a,b，在给定词c的情况下，找到具备类似关系的词d<span class="math display">\[a: b:: c: ? \rightarrow d=\arg \max _{i}\frac{\left(x_{b}-x_{a}+x_{c}\right)^{T}x_{i}}{\left\|x_{b}-x_{a}+x_{c}\right\|}\]</span></p><ul><li>通过加法后的余弦距离是否能很好地捕捉到直观的语义和句法类比问题来评估单词向量</li><li>从搜索中丢弃输入的单词</li><li>问题:如果有信息但不是线性的怎么办？</li></ul><h2 id="glove可视化效果">4.3 Glove可视化效果</h2><p>下图为GloVe得到的词向量空间分布，我们对词向量进行减法计算，可以发现类比的词对有相似的距离。</p><p>brother – sister, man – woman, king - queen</p><p><img src="/img/nlp/第二讲/6.png" /></p><h2 id="类比任务评估与超参数">4.4 类比任务评估与超参数</h2><p><img src="/img/nlp/第二讲/7.png" /></p><p><img src="/img/nlp/第二讲/8.png" /></p><ul><li>数据集越大越好，并且维基百科数据集比新闻文本数据集要好</li><li>300是一个很好的词向量维度</li></ul><h2 id="另一个内在的词向量评估">4.5 另一个内在的词向量评估</h2><p><img src="/img/nlp/第二讲/9.png" /></p><h2 id="相关性评估">4.6 相关性评估</h2><h3 id="section"><img src="/img/nlp/第二讲/10.png" /></h3><h1 id="word-senses">5 word senses</h1><h2 id="词义与词义歧义">5.1 词义与词义歧义</h2><p>大多数单词都是多义的</p><ul><li>特别是常见单词</li><li>特别是存在已久的单词</li></ul><p>例如：pike</p><p>那么，词向量是总体捕捉了所有这些信息，还是杂乱在一起了呢？</p><h2 id="pike的不同含义示例">5.2 pike的不同含义示例</h2><p><img src="/img/nlp/第二讲/11.png" /></p><h2 id="论文讲解-2">5.3 论文讲解</h2><h3id="improving-word-representations-via-global-context-and-multiple-word-prototypes-huang-et-al.-2012">5.3.1Improving Word Representations Via Global Context And Multiple WordPrototypes (Huang et al. 2012)</h3><p><img src="/img/nlp/第二讲/12.png" /></p><p>将常用词的所有上下文进行聚类，通过该词得到一些清晰的簇，从而将这个常用词分解为多个单词，例如bank1、bank2</p><h3id="linear-algebraic-structure-of-word-senses-with-applications-to-polysemy">5.3.2Linear Algebraic Structure of Word Senses, with Applications toPolysemy</h3><ul><li>单词在标准单词嵌入(如word2vec)中的不同含义以线性叠加(加权和)的形式存在</li></ul><p><span class="math display">\[v_{pike} = \alpha_1 v_{pike_1} + \alpha_2 v_{pike_2} + \alpha_3v_{pike_3}\]</span></p><ul><li>其中，<span class="math inline">\(\alpha =\frac{f_{1}}{f_{1}+f_{2}+f_{3}}\)</span></li></ul><p>令人惊讶的结果：</p><ul><li>只是加权平均值就已经可以获得很好的效果</li><li>由于从稀疏编码中得到的概念，你实际上可以将感官分离出来(前提是它们相对比较常见)</li></ul><h2 id="外向词向量评估">5.4 外向词向量评估</h2><ul><li>单词向量的外部评估：词向量可以应用于NLP的很多下游任务</li><li>一个例子是在命名实体识别任务中，寻找人名、机构名、地理位置名，词向量非常有帮助</li></ul><p><img src="/img/nlp/第二讲/13.png" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>回溯</title>
    <link href="/2022/05/03/%E5%9B%9E%E6%BA%AF/"/>
    <url>/2022/05/03/%E5%9B%9E%E6%BA%AF/</url>
    
    <content type="html"><![CDATA[<h2 id="电话号码的字母组合">17、电话号码的字母组合</h2><p><img src="/img/LeetCode/字符串/17.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">letterCombinations</span>(<span class="hljs-params">self, digits: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        <span class="hljs-keyword">if</span> digits == <span class="hljs-string">&#x27;&#x27;</span>:<br>            <span class="hljs-keyword">return</span> []<br><br>        phoneMap = &#123;<br>        <span class="hljs-string">&#x27;2&#x27;</span>: <span class="hljs-string">&#x27;abc&#x27;</span>,<br>        <span class="hljs-string">&#x27;3&#x27;</span>: <span class="hljs-string">&#x27;def&#x27;</span>,<br>        <span class="hljs-string">&#x27;4&#x27;</span>: <span class="hljs-string">&#x27;ghi&#x27;</span>,<br>        <span class="hljs-string">&#x27;5&#x27;</span>: <span class="hljs-string">&#x27;jkl&#x27;</span>,<br>        <span class="hljs-string">&#x27;6&#x27;</span>: <span class="hljs-string">&#x27;mno&#x27;</span>,<br>        <span class="hljs-string">&#x27;7&#x27;</span>: <span class="hljs-string">&#x27;pqrs&#x27;</span>,<br>        <span class="hljs-string">&#x27;8&#x27;</span>: <span class="hljs-string">&#x27;tuv&#x27;</span>,<br>        <span class="hljs-string">&#x27;9&#x27;</span>: <span class="hljs-string">&#x27;wxyz&#x27;</span>,   <br>        &#125;<br>        <br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">index: <span class="hljs-built_in">int</span></span>):<span class="hljs-comment"># 回溯</span><br>            <span class="hljs-keyword">if</span> index == <span class="hljs-built_in">len</span>(digits):<br>                combinations.append(<span class="hljs-string">&#x27;&#x27;</span>.join(combination))<br>            <span class="hljs-keyword">else</span>:<br>                digit = digits[index]<br>                <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> phoneMap[digit]:<br>                    combination.append(ch)<br>                    backtrack(index + <span class="hljs-number">1</span>)<br>                    combination.pop()<br><br>        combination = <span class="hljs-built_in">list</span>()<br>        combinations = <span class="hljs-built_in">list</span>()<br>        backtrack(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">return</span> combinations<br></code></pre></div></td></tr></table></figure><h2 id="括号生成">22、括号生成</h2><p><img src="/img/LeetCode/字符串/22.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generateParenthesis</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        ans = []<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">S, left, right</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(S) == <span class="hljs-number">2</span> * n:<br>                ans.append(<span class="hljs-string">&#x27;&#x27;</span>.join(S))<br>                <span class="hljs-keyword">return</span><br>            <span class="hljs-keyword">if</span> left &lt; n:<br>                S.append(<span class="hljs-string">&#x27;(&#x27;</span>)<br>                backtrack(S, left + <span class="hljs-number">1</span>, right)<br>                S.pop()<br>            <span class="hljs-keyword">if</span> right &lt; left:<br>                S.append(<span class="hljs-string">&#x27;)&#x27;</span>)<br>                backtrack(S, left, right + <span class="hljs-number">1</span>)<br>                S.pop()<br>        backtrack([], <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="组合总和">39、组合总和</h2><p><img src="/img/LeetCode/数组及数学/39.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">combinationSum</span>(<span class="hljs-params">self, candidates: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        candidates.sort()<br>        n = <span class="hljs-built_in">len</span>(candidates)<br>        res = []<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">i, tmp_sum, tmp</span>):<br>            <span class="hljs-keyword">if</span>  tmp_sum &gt; target <span class="hljs-keyword">or</span> i == n:<br>                <span class="hljs-keyword">return</span> <br>            <span class="hljs-keyword">if</span> tmp_sum == target:<br>                res.append(tmp)<br>                <span class="hljs-keyword">return</span> <br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i, n):<br>                <span class="hljs-keyword">if</span> tmp_sum + candidates[j] &gt; target:<br>                    <span class="hljs-keyword">break</span><br>                backtrack(j, tmp_sum+candidates[j], tmp+[candidates[j]])<br>        backtrack(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, [])<br>        <span class="hljs-keyword">return</span> res<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>第一讲-NLP介绍与词向量初步</title>
    <link href="/2022/05/01/%E7%AC%AC%E4%B8%80%E8%AE%B2-NLP%E4%BB%8B%E7%BB%8D%E4%B8%8E%E8%AF%8D%E5%90%91%E9%87%8F%E5%88%9D%E6%AD%A5/"/>
    <url>/2022/05/01/%E7%AC%AC%E4%B8%80%E8%AE%B2-NLP%E4%BB%8B%E7%BB%8D%E4%B8%8E%E8%AF%8D%E5%90%91%E9%87%8F%E5%88%9D%E6%AD%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="自然语言与词汇含义">1 自然语言与词汇含义</h1><h2 id="如何在计算机里表达词的意义">1.1 如何在计算机里表达词的意义</h2><p>要使用计算机处理文本词汇，一种处理方式是<strong>WordNet</strong>：即构建一个包含同义词集和上位词(“isa”关系)的列表的辞典。英文当中确实有这样一个wordnet，我们在安装完NLTK工具库和下载数据包后可以使用，对应的 python代码如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet <span class="hljs-keyword">as</span> wn<br>poses = &#123; <span class="hljs-string">&#x27;n&#x27;</span>:<span class="hljs-string">&#x27;noun&#x27;</span>, <span class="hljs-string">&#x27;v&#x27;</span>:<span class="hljs-string">&#x27;verb&#x27;</span>, <span class="hljs-string">&#x27;s&#x27;</span>:<span class="hljs-string">&#x27;adj (s)&#x27;</span>, <span class="hljs-string">&#x27;a&#x27;</span>:<span class="hljs-string">&#x27;adj&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>:<span class="hljs-string">&#x27;adv&#x27;</span>&#125;<br><span class="hljs-keyword">for</span> synset <span class="hljs-keyword">in</span> wn.synsets(<span class="hljs-string">&quot;good&quot;</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(poses[synset.pos()], <span class="hljs-string">&quot;, &quot;</span>.join([l.name() <span class="hljs-keyword">for</span> l <span class="hljs-keyword">in</span> synset.lemmas()])))<br>        <br><span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> wordnet <span class="hljs-keyword">as</span> wn<br>panda = wn.synset(<span class="hljs-string">&quot;panda.n.01&quot;</span>)<br>hyper = <span class="hljs-keyword">lambda</span> s: s.hypernyms()<br><span class="hljs-built_in">list</span>(panda.closure(hyper))<br></code></pre></div></td></tr></table></figure><p>运行结果如下：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">noun: good<br>noun: good, goodness<br>noun: good, goodness<br>noun: commodity, trade_good, good<br>adj: good<br>adj (sat): full, good<br>adj: good<br>adj (sat): estimable, good, honorable, respectable<br>adj (sat): beneficial, good<br>adj (sat): good<br>adj (sat): good, just, upright<br>…<br>adverb: well, good<br>adverb: thoroughly, soundly, good<br></code></pre></div></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">[Synset(&#x27;procyonid.n.01&#x27;),<br>Synset(&#x27;carnivore.n.01&#x27;),<br>Synset(&#x27;placental.n.01&#x27;),<br>Synset(&#x27;mammal.n.01&#x27;),<br>Synset(&#x27;vertebrate.n.01&#x27;),<br>Synset(&#x27;chordate.n.01&#x27;),<br>Synset(&#x27;animal.n.01&#x27;),<br>Synset(&#x27;organism.n.01&#x27;),<br>Synset(&#x27;living_thing.n.01&#x27;),<br>Synset(&#x27;whole.n.02&#x27;),<br>Synset(&#x27;object.n.01&#x27;),<br>Synset(&#x27;physical_entity.n.01&#x27;),<br>Synset(&#x27;entity.n.01&#x27;)]<br></code></pre></div></td></tr></table></figure><h2 id="wordnet的问题">1.2 WordNet的问题</h2><ul><li><p>作为一个资源很好，但忽略了细微差别。例如“proficient”被列为“good”的同义词，这只是在一些上下文中是正确的。</p></li><li><p>缺少单词的新含义，难以持续更新。</p></li><li><p>是主观的，需要人们来创造和调整，无法计算单词相似度。</p></li></ul><h2 id="文本词汇的离散表征">1.3 文本(词汇)的离散表征</h2><p>在传统的自然语言处理中，我们会对文本做离散表征，把词语看作离散的符号：例如hotel、conference、motel等。</p><p>one-hot vector是只有一个1，其余全为零的稀疏向量，单词可以通过one-hotvector来表示： <span class="math display">\[\text { motel }=\left[\begin{array}{lllllllllllllll}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right]\]</span></p><p><span class="math display">\[\text { hotel }=\left[\begin{array}{lllllllllllllll}0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp;0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\end{array}\right]\]</span></p><p>在one-hot vector中向量维度 = 词汇量（如500，000）</p><h2 id="离散表征的问题">1.4 离散表征的问题</h2><p>在上述的独热向量离散表征里，所有词向量是正交的，这是一个很大的问题。对于独热向量，没有关于相似性概念，并且向量维度过大。</p><p>对于上述问题有一些解决思路：</p><ul><li>①使用类似WordNet的工具中的列表，获得相似度，但会因不够完整而失败</li><li>② 通过大量数据学习词向量本身相似性，获得更精确的稠密词向量编码</li></ul><h2 id="基于上下文的词汇表征">1.5 基于上下文的词汇表征</h2><p><strong>近年来在深度学习中比较有效的方式是基于上下文的词汇表征</strong>。它的<strong>核心想法</strong>是：一个单词的意思是由经常出现在它附近的单词给出的</p><p>这是现代统计NLP最成功的理念之一，总体思路有点物以类聚，人以群分的感觉。</p><ul><li>当一个单词<spanclass="math inline">\(w\)</span>出现在文本中时，它的上下文是出现在其附近的一组单词(在一个固定大小的窗口中)</li><li>基于海量数据，使用<spanclass="math inline">\(w\)</span>的许多上下文来构建<spanclass="math inline">\(w\)</span>的表示</li></ul><h1 id="word-vectors">2 Word Vectors</h1><h2 id="词向量表示">2.1 词向量表示</h2><p><strong>Word2vec</strong> (Mikolov et al.2013)是一个学习词向量表征的框架。</p><ul><li><strong>核心思路</strong>如下：<ul><li>基于海量文本语料库构建</li><li>词汇表中的每个单词都由一个向量表示（学习完成后会固定）</li><li>对应语料库文本中的每个位置<spanclass="math inline">\(t\)</span>，有一个中心词<spanclass="math inline">\(c\)</span>和一些上下文(“外部”)单词<spanclass="math inline">\(o\)</span></li><li>使用<span class="math inline">\(c\)</span>和<spanclass="math inline">\(o\)</span>的词向量来计算概率<spanclass="math inline">\(P(o|c)\)</span>，即给定中心词推断上下文词汇的概率（反之亦然）</li><li>不断调整词向量来最大化这个概率</li></ul></li></ul><p><img src="/img/nlp/3.png" /></p><h1 id="word2vec-目标函数">3 Word2vec 目标函数</h1><h2 id="似然函数">3.1 似然函数</h2><p>对于每个位置t = 1, … , T, 在大小为<spanclass="math inline">\(m\)</span>的固定窗口内预测上下文单词，给定中心词<spanclass="math inline">\(w_j\)</span>，似然函数可以表示为： <spanclass="math display">\[Likelihood = L(\theta) = \prod_{t=1}^{T} \prod_{\substack{-m \leq j \leqm \\ j \neq 0}}P(w_{t+j}|w_t;\theta)\]</span></p><p>上述公式中，<spanclass="math inline">\(\theta\)</span>为模型包含的所有待优化权重变量</p><h2 id="目标函数">3.2 目标函数</h2><p>对应上述似然函数的目标函<spanclass="math inline">\(J(\theta)\)</span>可以取作(平均)负对数似然： <spanclass="math display">\[Likelihood=-\frac1T \log L(\theta)=-\frac1T \sum_{t=1}^{T}\sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \logP(w_{t+j}|w_t;\theta)\]</span></p><p>注意：</p><ul><li>目标函数<spanclass="math inline">\(J(\theta)\)</span>有时也被称为“<strong>代价函数</strong>”或“<strong>损失函数</strong>”</li><li>最小化目标函数与最大化似然函数（预测概率/精度）<strong>两者等价</strong></li></ul><p>得到目标函数后，我们希望最小化目标函数，那我们如何计算<spanclass="math inline">\(P(w_{t+j}|w_t;\theta)\)</span>?</p><p>对于每个词<span class="math inline">\(w\)</span>都会用两个向量：</p><ul><li>当<spanclass="math inline">\(w\)</span>是中心词时，我们标记词向量为<spanclass="math inline">\(v_w\)</span></li><li>当<spanclass="math inline">\(w\)</span>是上下文词时，我们标记词向量为<spanclass="math inline">\(u_w\)</span></li></ul><p>则对于一个中心词<spanclass="math inline">\(c\)</span>和一个上下文词<spanclass="math inline">\(o\)</span>，我们有如下概率计算公式 <spanclass="math display">\[P(o \mid c)=\frac{\exp \left(u_{o}^{T} v_{c}\right)}{\sum_{w \in V} \exp\left(u_{w}^{T} v_{c}\right)}\]</span></p><ul><li>对<spanclass="math inline">\(u_o^Tv_c\)</span>是将两个向量进行点积，为了计算两个词向量的相似度，点积的结果越大，那么相似度越大。</li><li>模型的训练正是为了使得具有相似上下文的单词，具有相似的向量</li><li>点积是计算相似性的一种简单方法，在注意力机制中常使用点积计算Score</li></ul><h1 id="word2vec预测函数">4 Word2vec预测函数</h1><p>这也就是softmax function(<spanclass="math inline">\(R^n\Rightarrow(0,1)^n\)</span>​)的一个例子，将<spanclass="math inline">\(x_i\)</span>映射到<spanclass="math inline">\(p_i\)</span>： <span class="math display">\[\operatorname{softmax}\left(x_{i}\right)=\frac{\exp\left(x_{i}\right)}{\sum_{j=1}^{n} \exp \left(x_{j}\right)}=p_{i}\]</span> 为了训练模型，我们就要找出参数最小化loss函数</p><ul><li><p>Recall：<spanclass="math inline">\(\theta\)</span>通过一个长向量来代表所有的模型参数</p></li><li><p>在我们的例子中，对于d维向量和V个单词，我们得出：</p></li><li><p><span class="math display">\[\theta=\left[\begin{array}{l}v_{\text {aardvark }} \\v_{a} \\\vdots \\v_{z e b r a} \\u_{a a r d v a r k} \\u_{a} \\\vdots \\u_{z e b r a}\end{array}\right] \in \mathbb{R}^{2 d V}\]</span></p></li><li><p>所以我们每个单词有两个向量</p></li></ul><p>我们就需要计算所有向量的梯度进行梯度下降算法来最小化loss函数</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
      <category>NLP</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>双指针</title>
    <link href="/2022/05/01/%E5%8F%8C%E6%8C%87%E9%92%88/"/>
    <url>/2022/05/01/%E5%8F%8C%E6%8C%87%E9%92%88/</url>
    
    <content type="html"><![CDATA[<h2 id="盛水最多的容器">11、盛水最多的容器</h2><p><img src="/img/LeetCode/数组及数学/3.png" /></p><p>我们使用双指针的思想，两个指针分别为数组的头尾结点，每轮都进行面积的计算，并且每轮只移动较小结点的指针。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maxArea</span>(<span class="hljs-params">self, height: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(height)<br>        left = <span class="hljs-number">0</span>    <span class="hljs-comment"># 左指针</span><br>        right = n - <span class="hljs-number">1</span>   <span class="hljs-comment"># 右指针</span><br>        max_s = <span class="hljs-number">0</span>   <span class="hljs-comment"># 面积最大值</span><br>        s = <span class="hljs-number">0</span>   <span class="hljs-comment"># 面积</span><br>        l = <span class="hljs-number">0</span>   <span class="hljs-comment"># 边长</span><br>        <span class="hljs-keyword">while</span> right &gt; left:<br>            <span class="hljs-keyword">if</span> height[left] &gt; height[right]:<br>                l = height[right]<br>            <span class="hljs-keyword">else</span>:<br>                l = height[left]<br>            s = (right - left) * l<br>            <span class="hljs-keyword">if</span> s &gt; max_s:<br>                max_s = s<br>            <span class="hljs-keyword">if</span> height[left] &gt; height[right]:<br>                right = right - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                left = left + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> max_s<br></code></pre></div></td></tr></table></figure><h2 id="三数之和">15、三数之和</h2><p><img src="/img/LeetCode/数组及数学/15.png" /></p><p>采用双指针的思想，为了不使三元组重复，先对数组进行排序。当确定了第一个元素，只需要对剩下元素进行设置左指针和右指针，如果三元组大与0，右指针就左移一位，反之亦然。如果三元组之和刚好等于0，就需要左指针和右指针都移动一位。这时候还会出现重复问题，所以需要判断此时遍历的元素和之前的是否相同，如果相同就要跳过。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">threeSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        nums.sort()<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        ans = <span class="hljs-built_in">list</span>()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<span class="hljs-comment"># 固定其中一个元素</span><br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> nums[i] != nums[i - <span class="hljs-number">1</span>]:<br>                k = n - <span class="hljs-number">1</span><br>                j = i + <span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> k &gt; j:<span class="hljs-comment"># 进行双指针遍历</span><br>                    <span class="hljs-keyword">if</span> k != n - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[k] == nums[k + <span class="hljs-number">1</span>]:<span class="hljs-comment"># 如果右指针之前出现过就跳过</span><br>                        k = k - <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">continue</span><br>                    <span class="hljs-keyword">if</span> j != i + <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[j] == nums[j - <span class="hljs-number">1</span>]:<span class="hljs-comment"># 如果左指针之前出现过就跳过</span><br>                        j = j + <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">continue</span><br>                    s = nums[i] + nums[j] + nums[k]<br>                    <span class="hljs-keyword">if</span> s == <span class="hljs-number">0</span>:<br>                        ans.append([nums[i], nums[j], nums[k]])<br>                        k = k - <span class="hljs-number">1</span><br>                        j = j + <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> s &gt; <span class="hljs-number">0</span>:<br>                        k = k - <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> s &lt; <span class="hljs-number">0</span>:<br>                        j = j + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="最接近的三数之和">16、最接近的三数之和</h2><p><img src="/img/LeetCode/数组及数学/16.png" /></p><p>采用双指针的思想，先对数组进行排序。当确定了第一个元素，只需要对剩下元素进行设置左指针和右指针，如果三元组之和大于target，我们只需要将右节点左移一位，相反亦然。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">threeSumClosest</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        nums.sort()<br>        min_num = <span class="hljs-number">10</span>**<span class="hljs-number">9</span><br>        ans = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] == nums[i - <span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">continue</span><br>            k = n - <span class="hljs-number">1</span><br>            j = i + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> k &gt; j:<br>                s = nums[i] + nums[j] + nums[k]<br>                <span class="hljs-keyword">if</span> s == target:<br>                    <span class="hljs-keyword">return</span> s<br>                <span class="hljs-keyword">if</span> s &gt; target:<br>                    <span class="hljs-keyword">if</span> s - target &lt; min_num:<br>                        min_num = s - target<br>                        ans = s<br>                    k = k - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">elif</span> s &lt; target:<br>                    <span class="hljs-keyword">if</span> target - s &lt; min_num:<br>                        min_num = target - s<br>                        ans = s<br>                    j = j + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="四数之和">18、四数之和</h2><p><img src="/img/LeetCode/数组及数学/18.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fourSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">if</span> nums == [] <span class="hljs-keyword">or</span> n &lt; <span class="hljs-number">4</span>:<br>            <span class="hljs-keyword">return</span> []<br>        nums.sort()<br>        ans = <span class="hljs-built_in">list</span>()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n - <span class="hljs-number">3</span>):<br>            <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] == nums[i - <span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> nums[i] + nums[i + <span class="hljs-number">1</span>] + nums[i + <span class="hljs-number">2</span>] + nums[i + <span class="hljs-number">3</span>] &gt; target:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> nums[i] + nums[n - <span class="hljs-number">1</span>] + nums[n - <span class="hljs-number">2</span>] + nums[n - <span class="hljs-number">3</span>] &lt; target:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, n - <span class="hljs-number">2</span>):<br>                <span class="hljs-keyword">if</span> j != i + <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[j] == nums[j - <span class="hljs-number">1</span>]:<br>                    <span class="hljs-keyword">continue</span><br>                <span class="hljs-keyword">if</span> nums[i] + nums[j] + nums[j + <span class="hljs-number">1</span>] + nums[j + <span class="hljs-number">2</span>] &gt; target:<br>                    <span class="hljs-keyword">break</span><br>                <span class="hljs-keyword">if</span> nums[i] + nums[n - <span class="hljs-number">1</span>] + nums[n - <span class="hljs-number">2</span>] + nums[n - <span class="hljs-number">3</span>] &lt; target:<br>                    <span class="hljs-keyword">continue</span><br>                k = j + <span class="hljs-number">1</span><br>                l = n - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> k &lt; l:<br>                    <span class="hljs-built_in">sum</span> = nums[i] + nums[j] + nums[k] + nums[l]<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span> &lt; target:<br>                        k = k + <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">sum</span> &gt; target:<br>                        l = l - <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">sum</span> == target:<br>                        ans.append([nums[i], nums[j], nums[k], nums[l]])<br>                        <span class="hljs-keyword">while</span> k &lt; l <span class="hljs-keyword">and</span> nums[k] == nums[k + <span class="hljs-number">1</span>]:<br>                            k = k + <span class="hljs-number">1</span><br>                        k = k + <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">while</span> k &lt; l <span class="hljs-keyword">and</span> nums[l] == nums[l - <span class="hljs-number">1</span>]:<br>                            l = l - <span class="hljs-number">1</span><br>                        l = l - <span class="hljs-number">1</span> <br><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="删除链表的倒数第n个结点">19、删除链表的倒数第N个结点</h2><p><img src="/img/LeetCode/链表/19.png" /></p><p>采用双指针的思想，左右指针都为指向头节点，首先右指针向后遍历n个结点，最后让左右一起向后遍历直至右指针指向的为最后一个结点，这时候左指针指向的就是倒数第n个结点的前一个结点，就可以进行删除的操作了。</p><p>但是有一个特殊情况，如果删除的是头结点，上述不成立，所以要单独进行一个if判断，如果右指针向后遍历n个结点后为空，那么删除的就是头结点。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeNthFromEnd</span>(<span class="hljs-params">self, head: ListNode, n: <span class="hljs-built_in">int</span></span>) -&gt; ListNode:<br>        right = head<br>        left = head<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            right = right.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">if</span> right == <span class="hljs-literal">None</span>:<span class="hljs-comment"># 如果要删除的是头结点</span><br>            left = left.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">return</span> left<br>        <span class="hljs-keyword">while</span> right.<span class="hljs-built_in">next</span> != <span class="hljs-literal">None</span>:<br>            left = left.<span class="hljs-built_in">next</span><br>            right = right.<span class="hljs-built_in">next</span><br>        left.<span class="hljs-built_in">next</span> = left.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">return</span> head<br></code></pre></div></td></tr></table></figure><h2 id="删除有序数组中的重复项">26、删除有序数组中的重复项</h2><p><img src="/img/LeetCode/数组及数学/26.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeDuplicates</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> nums == []:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        n = <span class="hljs-built_in">len</span>(nums)<br>        left, right = <span class="hljs-number">1</span>, <span class="hljs-number">1</span><br>        <span class="hljs-keyword">while</span> right &lt; n:<br>            <span class="hljs-keyword">if</span> nums[right] != nums[right - <span class="hljs-number">1</span>]:<br>                nums[left] = nums[right]<br>                left = left + <span class="hljs-number">1</span><br>            right = right + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> left<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>动态规划</title>
    <link href="/2022/04/29/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    <url>/2022/04/29/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/</url>
    
    <content type="html"><![CDATA[<h2 id="最长回文子串">4、最长回文子串</h2><p><img src="/img/LeetCode/字符串/2.png" /></p><p>本题使用的动态规划的思想，也就是说只有s[i+1:j-1]<em>s</em>[<em>i</em>+1:<em>j</em>−1] 是回文串，并且s<em>s</em> 的第 i<em>i</em> 和 j<em>j</em>个字母相同时，s[i:j]<em>s</em>[<em>i</em>:<em>j</em>] 才会是回文串。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestPalindrome</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-keyword">if</span> n &lt; <span class="hljs-number">2</span>:  <span class="hljs-comment"># 如果字符串长为1</span><br>            <span class="hljs-keyword">return</span> s<br>        <br>        <span class="hljs-built_in">max</span> = <span class="hljs-number">1</span><br>        left = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># dp[i][j]代表是s[i...j]是否为回文串</span><br>        dp = [[<span class="hljs-literal">False</span>] * n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            dp[i][i] = <span class="hljs-literal">True</span><br><br>        <span class="hljs-keyword">for</span> L <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, n + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                right = L + i - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> right &gt;= n:<br>                    <span class="hljs-keyword">break</span><br>                <br>                <span class="hljs-keyword">if</span> s[i] != s[right]:<br>                    dp[i][right] = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">if</span> right - i &lt; <span class="hljs-number">3</span>:<br>                        dp[i][right] = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">else</span>:<br>                        dp[i][right] = dp[i + <span class="hljs-number">1</span>][right - <span class="hljs-number">1</span>]<br><br>                <span class="hljs-keyword">if</span> dp[i][right] <span class="hljs-keyword">and</span> L &gt; <span class="hljs-built_in">max</span>:<br>                    <span class="hljs-built_in">max</span> = L<br>                    left = i<br>        <span class="hljs-keyword">return</span> s[left:left+<span class="hljs-built_in">max</span>]<br></code></pre></div></td></tr></table></figure><h2 id="最大子数组和">53、最大子数组和</h2><p><img src="/img/LeetCode/数组及数学/53.png" /></p><p>看到这种区间的，可以想到动态规划</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maxSubArray</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(nums)):<br>            nums[i] += <span class="hljs-built_in">max</span>(nums[i - <span class="hljs-number">1</span>], <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(nums)<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>字符串</title>
    <link href="/2022/04/29/%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
    <url>/2022/04/29/%E5%AD%97%E7%AC%A6%E4%B8%B2/</url>
    
    <content type="html"><![CDATA[<h2 id="无重复字符的最长子串">3、无重复字符的最长子串</h2><p><img src="/img/LeetCode/字符串/1.png" /></p><p>方法一：暴力解</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lengthOfLongestSubstring</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        occ = <span class="hljs-built_in">set</span>()<br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-comment"># rk为左指针，ans为无重复子串长度</span><br>        rk, ans = -<span class="hljs-number">1</span>, <span class="hljs-number">0</span><br>        flag = <span class="hljs-number">0</span>    <span class="hljs-comment"># 循环中无重复子串长度</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            rk = i<br>            flag = <span class="hljs-number">0</span><br>            <span class="hljs-keyword">while</span> rk &lt; n <span class="hljs-keyword">and</span> s[rk] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> occ:<br>                occ.add(s[rk])<br>                flag = flag + <span class="hljs-number">1</span><br>                rk = rk + <span class="hljs-number">1</span><br>            ans = flag <span class="hljs-keyword">if</span> flag &gt; ans <span class="hljs-keyword">else</span> ans<br>            occ.clear()<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><p>方法二：</p><p>每次循环没必要把集合里的元素全部删除，可以每次只删除上一轮循环的最左边结点</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">lengthOfLongestSubstring</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        occ = <span class="hljs-built_in">set</span>()<br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-comment"># rk为左指针，ans为无重复子串长度</span><br>        rk, ans = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        flag = <span class="hljs-number">0</span>    <span class="hljs-comment"># 循环中无重复子串长度</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> i &gt; <span class="hljs-number">0</span>:<br>                occ.remove(s[i - <span class="hljs-number">1</span>])<br>                flag = flag - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> rk &lt; n <span class="hljs-keyword">and</span> s[rk] <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> occ:<br>                occ.add(s[rk])<br>                rk = rk + <span class="hljs-number">1</span><br>                flag = flag + <span class="hljs-number">1</span><br>            ans = flag <span class="hljs-keyword">if</span> flag &gt; ans <span class="hljs-keyword">else</span> ans<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="最长回文子串">4、最长回文子串</h2><p><img src="/img/LeetCode/字符串/2.png" /></p><p>本题使用的动态规划的思想，也就是说只有s[i+1:j-1]<em>s</em>[<em>i</em>+1:<em>j</em>−1] 是回文串，并且s<em>s</em> 的第 i<em>i</em> 和 j<em>j</em>个字母相同时，s[i:j]<em>s</em>[<em>i</em>:<em>j</em>] 才会是回文串。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestPalindrome</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-keyword">if</span> n &lt; <span class="hljs-number">2</span>:  <span class="hljs-comment"># 如果字符串长为1</span><br>            <span class="hljs-keyword">return</span> s<br>        <br>        <span class="hljs-built_in">max</span> = <span class="hljs-number">1</span><br>        left = <span class="hljs-number">0</span><br>        <span class="hljs-comment"># dp[i][j]代表是s[i...j]是否为回文串</span><br>        dp = [[<span class="hljs-literal">False</span>] * n <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n)]<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            dp[i][i] = <span class="hljs-literal">True</span><br><br>        <span class="hljs-keyword">for</span> L <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">2</span>, n + <span class="hljs-number">1</span>):<br>            <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>                right = L + i - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">if</span> right &gt;= n:<br>                    <span class="hljs-keyword">break</span><br>                <br>                <span class="hljs-keyword">if</span> s[i] != s[right]:<br>                    dp[i][right] = <span class="hljs-literal">False</span><br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">if</span> right - i &lt; <span class="hljs-number">3</span>:<br>                        dp[i][right] = <span class="hljs-literal">True</span><br>                    <span class="hljs-keyword">else</span>:<br>                        dp[i][right] = dp[i + <span class="hljs-number">1</span>][right - <span class="hljs-number">1</span>]<br><br>                <span class="hljs-keyword">if</span> dp[i][right] <span class="hljs-keyword">and</span> L &gt; <span class="hljs-built_in">max</span>:<br>                    <span class="hljs-built_in">max</span> = L<br>                    left = i<br>        <span class="hljs-keyword">return</span> s[left:left+<span class="hljs-built_in">max</span>]<br></code></pre></div></td></tr></table></figure><h2 id="z字形变换">6、Z字形变换</h2><p><img src="/img/LeetCode/字符串/3.png" /></p><p>使用二维矩阵进行模拟：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">convert</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span>, numRows: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-keyword">if</span> numRows == <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> numRows &gt;= n:<br>            <span class="hljs-keyword">return</span> s<br>        t = <span class="hljs-number">2</span> * numRows - <span class="hljs-number">2</span><br>        col = (n + t - <span class="hljs-number">1</span>) // t * (numRows - <span class="hljs-number">1</span>)<br>        matrix = [[<span class="hljs-string">&#x27;&#x27;</span>] * col <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(numRows)]<br>        x, y = <span class="hljs-number">0</span>, <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(s):<br>            matrix[x][y] = ch<br>            <span class="hljs-keyword">if</span> i % t &lt; numRows - <span class="hljs-number">1</span>:<br>                x = x + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                x = x - <span class="hljs-number">1</span><br>                y = y + <span class="hljs-number">1</span><br>        ans = <span class="hljs-string">&#x27;&#x27;</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(numRows):<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(col):<br>                <span class="hljs-keyword">if</span> matrix[i][j]:<br>                    ans = ans + matrix[i][j]<br><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="字符串转换整数">7、字符串转换整数</h2><p><img src="/img/LeetCode/字符串/4.png" /></p><p><img src="/img/LeetCode/字符串/5.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">myAtoi</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        flag = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> s == <span class="hljs-string">&#x27;&#x27;</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(s):<br>            <span class="hljs-keyword">if</span> ch != <span class="hljs-string">&#x27; &#x27;</span>:<br>                s = s[i:]<br>                <span class="hljs-keyword">break</span><br><br>        <span class="hljs-keyword">if</span> s[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;-&#x27;</span>:<br>            flag = <span class="hljs-literal">True</span><br>            s = s[<span class="hljs-number">1</span>:]<br>        <span class="hljs-keyword">elif</span> s[<span class="hljs-number">0</span>] == <span class="hljs-string">&#x27;+&#x27;</span>:<br>            s = s[<span class="hljs-number">1</span>:]<br>        a = <span class="hljs-number">0</span><br>        ans = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(s):<br>            <span class="hljs-keyword">if</span> ch.isdigit() == <span class="hljs-literal">False</span>:<br>                <span class="hljs-keyword">break</span><br>            a = <span class="hljs-built_in">int</span>(ch)<br>            ans = ans * <span class="hljs-number">10</span> + a<br>        <span class="hljs-keyword">if</span> flag:<br>            ans = -<span class="hljs-number">1</span> * ans<br>        <span class="hljs-keyword">if</span> ans &gt; <span class="hljs-number">2</span> ** <span class="hljs-number">31</span> - <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">2</span> ** <span class="hljs-number">31</span> - <span class="hljs-number">1</span><br>        <span class="hljs-keyword">elif</span> ans &lt; -(<span class="hljs-number">2</span> ** <span class="hljs-number">31</span>):<br>            <span class="hljs-keyword">return</span> -(<span class="hljs-number">2</span> ** <span class="hljs-number">31</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="最长公共前缀">14、最长公共前缀</h2><p><img src="/img/LeetCode/字符串/6.png" /></p><p>想法为横向对比，按照列表顺序开始逐步算法前x个字符串的最长前缀，直至得到最终的最长前缀</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">longestCommonPrefix</span>(<span class="hljs-params">self, strs: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>) -&gt; <span class="hljs-built_in">str</span>:<br>        flag = strs[<span class="hljs-number">0</span>]<span class="hljs-comment"># 前几个字符串的最长前缀</span><br>        a = <span class="hljs-string">&quot;&quot;</span><span class="hljs-comment"># 本字符串与flag的最长前缀</span><br>        ans = <span class="hljs-string">&quot;&quot;</span><br>        n = <span class="hljs-built_in">len</span>(strs)<br>        <span class="hljs-keyword">if</span> n == <span class="hljs-number">1</span>:<span class="hljs-comment"># 如果只有一个字符串，那就直接输出</span><br>            <span class="hljs-keyword">return</span> strs[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">for</span> i, s <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(strs):<br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span>:<span class="hljs-comment"># 如果是第一个字符串那就跳过</span><br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">for</span> j, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(s):<span class="hljs-comment"># 本字符串与flag进行比对</span><br>                <span class="hljs-keyword">if</span> j &lt; <span class="hljs-built_in">len</span>(flag) <span class="hljs-keyword">and</span> ch == flag[j]:<br>                    a = a + ch<br>                <span class="hljs-keyword">else</span>:<br>                    <span class="hljs-keyword">break</span><br>            flag = a<br>            a = <span class="hljs-string">&quot;&quot;</span><br>            <span class="hljs-keyword">if</span> i == n - <span class="hljs-number">1</span>:<span class="hljs-comment"># 如果是最后一个结点，那就是最终最长前缀了</span><br>                ans = flag<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="电话号码的字母组合">17、电话号码的字母组合</h2><p><img src="/img/LeetCode/字符串/17.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">letterCombinations</span>(<span class="hljs-params">self, digits: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        <span class="hljs-keyword">if</span> digits == <span class="hljs-string">&#x27;&#x27;</span>:<br>            <span class="hljs-keyword">return</span> []<br><br>        phoneMap = &#123;<br>        <span class="hljs-string">&#x27;2&#x27;</span>: <span class="hljs-string">&#x27;abc&#x27;</span>,<br>        <span class="hljs-string">&#x27;3&#x27;</span>: <span class="hljs-string">&#x27;def&#x27;</span>,<br>        <span class="hljs-string">&#x27;4&#x27;</span>: <span class="hljs-string">&#x27;ghi&#x27;</span>,<br>        <span class="hljs-string">&#x27;5&#x27;</span>: <span class="hljs-string">&#x27;jkl&#x27;</span>,<br>        <span class="hljs-string">&#x27;6&#x27;</span>: <span class="hljs-string">&#x27;mno&#x27;</span>,<br>        <span class="hljs-string">&#x27;7&#x27;</span>: <span class="hljs-string">&#x27;pqrs&#x27;</span>,<br>        <span class="hljs-string">&#x27;8&#x27;</span>: <span class="hljs-string">&#x27;tuv&#x27;</span>,<br>        <span class="hljs-string">&#x27;9&#x27;</span>: <span class="hljs-string">&#x27;wxyz&#x27;</span>,   <br>        &#125;<br>        <br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">index: <span class="hljs-built_in">int</span></span>):<span class="hljs-comment"># 回溯</span><br>            <span class="hljs-keyword">if</span> index == <span class="hljs-built_in">len</span>(digits):<br>                combinations.append(<span class="hljs-string">&#x27;&#x27;</span>.join(combination))<br>            <span class="hljs-keyword">else</span>:<br>                digit = digits[index]<br>                <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> phoneMap[digit]:<br>                    combination.append(ch)<br>                    backtrack(index + <span class="hljs-number">1</span>)<br>                    combination.pop()<br><br>        combination = <span class="hljs-built_in">list</span>()<br>        combinations = <span class="hljs-built_in">list</span>()<br>        backtrack(<span class="hljs-number">0</span>)<br><br>        <span class="hljs-keyword">return</span> combinations<br></code></pre></div></td></tr></table></figure><h2 id="有效的括号">20、有效的括号</h2><p><img src="/img/LeetCode/字符串/20.png" /></p><p>采用了栈的思想，将符号依次入栈进行判断</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isValid</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        stack = <span class="hljs-built_in">list</span>()<br>        <span class="hljs-keyword">for</span> ch <span class="hljs-keyword">in</span> s:<br>            <span class="hljs-keyword">if</span> ch == <span class="hljs-string">&#x27;)&#x27;</span>:<br>                <span class="hljs-keyword">if</span> stack == []:<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>                temp = stack.pop()<br>                <span class="hljs-keyword">if</span> temp != <span class="hljs-string">&#x27;(&#x27;</span>:<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">elif</span> ch == <span class="hljs-string">&#x27;&#125;&#x27;</span>:<br>                <span class="hljs-keyword">if</span> stack == []:<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>                temp = stack.pop()<br>                <span class="hljs-keyword">if</span> temp != <span class="hljs-string">&#x27;&#123;&#x27;</span>:<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">elif</span> ch == <span class="hljs-string">&#x27;]&#x27;</span>:<br>                <span class="hljs-keyword">if</span> stack == []:<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>                temp = stack.pop()<br>                <span class="hljs-keyword">if</span> temp != <span class="hljs-string">&#x27;[&#x27;</span>:<br>                    <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>            <span class="hljs-keyword">else</span>:<br>                stack.append(ch)<br>        <span class="hljs-keyword">if</span> stack:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br></code></pre></div></td></tr></table></figure><h2 id="合并两个有序链表">21、合并两个有序链表</h2><p><img src="/img/LeetCode/字符串/21.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">mergeTwoLists</span>(<span class="hljs-params">self, list1: <span class="hljs-type">Optional</span>[ListNode], list2: <span class="hljs-type">Optional</span>[ListNode]</span>) -&gt; <span class="hljs-type">Optional</span>[ListNode]:<br>        head = ListNode(<span class="hljs-number">0</span>, <span class="hljs-literal">None</span>)<br>        temp = head<br>        first = list1<br>        second = list2<br>        <span class="hljs-keyword">while</span> first <span class="hljs-keyword">and</span> second:<br>            <span class="hljs-keyword">if</span> first.val &lt;= second.val:<br>                temp.<span class="hljs-built_in">next</span> = first<br>                temp = temp.<span class="hljs-built_in">next</span><br>                first = first.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">elif</span> first.val &gt; second.val:<br>                temp.<span class="hljs-built_in">next</span> = second<br>                temp = temp.<span class="hljs-built_in">next</span><br>                second = second.<span class="hljs-built_in">next</span><br>        <br>        temp.<span class="hljs-built_in">next</span> = first <span class="hljs-keyword">if</span> first <span class="hljs-keyword">else</span> second<br>        <br>        <span class="hljs-keyword">return</span> head.<span class="hljs-built_in">next</span><br></code></pre></div></td></tr></table></figure><h2 id="括号生成">22、括号生成</h2><p><img src="/img/LeetCode/字符串/22.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">generateParenthesis</span>(<span class="hljs-params">self, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]:<br>        ans = []<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">backtrack</span>(<span class="hljs-params">S, left, right</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(S) == <span class="hljs-number">2</span> * n:<br>                ans.append(<span class="hljs-string">&#x27;&#x27;</span>.join(S))<br>                <span class="hljs-keyword">return</span><br>            <span class="hljs-keyword">if</span> left &lt; n:<br>                S.append(<span class="hljs-string">&#x27;(&#x27;</span>)<br>                backtrack(S, left + <span class="hljs-number">1</span>, right)<br>                S.pop()<br>            <span class="hljs-keyword">if</span> right &lt; left:<br>                S.append(<span class="hljs-string">&#x27;)&#x27;</span>)<br>                backtrack(S, left, right + <span class="hljs-number">1</span>)<br>                S.pop()<br>        backtrack([], <span class="hljs-number">0</span>, <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>哈希表</title>
    <link href="/2022/04/28/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    <url>/2022/04/28/%E5%93%88%E5%B8%8C%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="两数相加">1、两数相加</h2><p><img src="/img/LeetCode/哈希表/1.png" /></p><p>这样我们创建一个哈希表，对于每一个<code>x</code>，我们首先查询哈希表中是否存在<code>target - x</code>，然后将 <code>x</code>插入到哈希表中，即可保证不会让 <code>x</code> 和自己匹配。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">twoSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        hashtable = <span class="hljs-built_in">dict</span>()<br>        <span class="hljs-keyword">for</span> i, num <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(nums):<br>            <span class="hljs-keyword">if</span> target - num <span class="hljs-keyword">in</span> hashtable:<br>                <span class="hljs-keyword">return</span> [hashtable[target - num], i]<br>            <span class="hljs-keyword">else</span>:<br>                hashtable[num] = i<br>        <span class="hljs-keyword">return</span> []<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>链表</title>
    <link href="/2022/04/28/%E9%93%BE%E8%A1%A8/"/>
    <url>/2022/04/28/%E9%93%BE%E8%A1%A8/</url>
    
    <content type="html"><![CDATA[<h2 id="两数相加">2、两数相加</h2><p><img src="/img/LeetCode/链表/2.png" /></p><p>采用递归的思想，当某个链表当前节点存在下一个节点或进位符为1时，往下递归。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">addTwoNumbers</span>(<span class="hljs-params">self, l1: <span class="hljs-type">Optional</span>[ListNode], l2: <span class="hljs-type">Optional</span>[ListNode], carryflag = <span class="hljs-number">0</span></span>) -&gt; <span class="hljs-type">Optional</span>[ListNode]:<br>        n1, n2 = l1.val <span class="hljs-keyword">if</span> l1 <span class="hljs-keyword">else</span> <span class="hljs-number">0</span>, l2.val <span class="hljs-keyword">if</span> l2 <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        s = n1 + n2 + carryflag<br>        val, carry_flag = s % <span class="hljs-number">10</span>, <span class="hljs-number">1</span> <span class="hljs-keyword">if</span> s &gt; <span class="hljs-number">9</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        next1, next2 = l1.<span class="hljs-built_in">next</span> <span class="hljs-keyword">if</span> l1 <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>, l2.<span class="hljs-built_in">next</span> <span class="hljs-keyword">if</span> l2 <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> next1 <span class="hljs-keyword">or</span> next2 <span class="hljs-keyword">or</span> carry_flag:<br>            <span class="hljs-keyword">return</span> ListNode(val, self.addTwoNumbers(next1, next2, carry_flag))<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> ListNode(val)<br></code></pre></div></td></tr></table></figure><h2 id="删除链表的倒数第n个结点">19、删除链表的倒数第N个结点</h2><p><img src="/img/LeetCode/链表/19.png" /></p><p>采用双指针的思想，左右指针都为指向头节点，首先右指针向后遍历n个结点，最后让左右一起向后遍历直至右指针指向的为最后一个结点，这时候左指针指向的就是倒数第n个结点的前一个结点，就可以进行删除的操作了。</p><p>但是有一个特殊情况，如果删除的是头结点，上述不成立，所以要单独进行一个if判断，如果右指针向后遍历n个结点后为空，那么删除的就是头结点。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeNthFromEnd</span>(<span class="hljs-params">self, head: ListNode, n: <span class="hljs-built_in">int</span></span>) -&gt; ListNode:<br>        right = head<br>        left = head<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            right = right.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">if</span> right == <span class="hljs-literal">None</span>:<span class="hljs-comment"># 如果要删除的是头结点</span><br>            left = left.<span class="hljs-built_in">next</span><br>            <span class="hljs-keyword">return</span> left<br>        <span class="hljs-keyword">while</span> right.<span class="hljs-built_in">next</span> != <span class="hljs-literal">None</span>:<br>            left = left.<span class="hljs-built_in">next</span><br>            right = right.<span class="hljs-built_in">next</span><br>        left.<span class="hljs-built_in">next</span> = left.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">return</span> head<br></code></pre></div></td></tr></table></figure><h2 id="亮亮交换链表中的结点">24、亮亮交换链表中的结点</h2><p><img src="/img/LeetCode/链表/24.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># Definition for singly-linked list.</span><br><span class="hljs-comment"># class ListNode:</span><br><span class="hljs-comment">#     def __init__(self, val=0, next=None):</span><br><span class="hljs-comment">#         self.val = val</span><br><span class="hljs-comment">#         self.next = next</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">swapPairs</span>(<span class="hljs-params">self, head: ListNode</span>) -&gt; ListNode:<br>        <span class="hljs-keyword">if</span> head == <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">return</span> head<br>        temp = ListNode(<span class="hljs-number">0</span>, head)<br>        first = temp<br>        second = temp.<span class="hljs-built_in">next</span><br><br>        <span class="hljs-keyword">while</span> second <span class="hljs-keyword">and</span> second.<span class="hljs-built_in">next</span>:<br>            first.<span class="hljs-built_in">next</span> = second.<span class="hljs-built_in">next</span><br>            second.<span class="hljs-built_in">next</span> = second.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span><br>            first.<span class="hljs-built_in">next</span>.<span class="hljs-built_in">next</span> = second<br>            <br>            first = second<br>            second = second.<span class="hljs-built_in">next</span><br>        <span class="hljs-keyword">return</span> temp.<span class="hljs-built_in">next</span><br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>数组及数学</title>
    <link href="/2022/04/28/%E6%95%B0%E7%BB%84%E5%8F%8A%E6%95%B0%E5%AD%A6/"/>
    <url>/2022/04/28/%E6%95%B0%E7%BB%84%E5%8F%8A%E6%95%B0%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h2 id="整数反转">7、整数反转</h2><p><img src="/img/LeetCode/数组及数学/1.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverse</span>(<span class="hljs-params">self, x: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ans = <span class="hljs-number">0</span><br>        flag = <span class="hljs-literal">False</span><br>        <span class="hljs-keyword">if</span> x &lt; <span class="hljs-number">0</span>:<br>            flag = <span class="hljs-literal">True</span><br>            x = -<span class="hljs-number">1</span> * x<br>        a = <span class="hljs-number">0</span>   <span class="hljs-comment"># 除法余数</span><br>        <span class="hljs-keyword">while</span> x != <span class="hljs-number">0</span>:<br>            a = x % <span class="hljs-number">10</span><br>            x = x // <span class="hljs-number">10</span><br>            ans = ans * <span class="hljs-number">10</span> + a<br>        <span class="hljs-keyword">if</span> flag:<br>            ans = -<span class="hljs-number">1</span> * ans<br>        <span class="hljs-keyword">if</span> ans &gt; <span class="hljs-number">2</span> ** <span class="hljs-number">31</span> - <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> ans &lt; -(<span class="hljs-number">2</span> ** <span class="hljs-number">31</span>):<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="回文数">9、回文数</h2><p><img src="/img/LeetCode/数组及数学/2.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">isPalindrome</span>(<span class="hljs-params">self, x: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">bool</span>:<br>        <span class="hljs-keyword">if</span> x &lt; <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br>        a = self.reverse(x) <span class="hljs-comment"># 整数反转</span><br>        <span class="hljs-keyword">if</span> a == x:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">True</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">False</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">reverse</span>(<span class="hljs-params">self, x: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ans = <span class="hljs-number">0</span><br>        a = <span class="hljs-number">0</span>   <span class="hljs-comment"># 除法余数</span><br>        <span class="hljs-keyword">while</span> x != <span class="hljs-number">0</span>:<br>            a = x % <span class="hljs-number">10</span><br>            x = x // <span class="hljs-number">10</span><br>            ans = ans * <span class="hljs-number">10</span> + a<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="盛水最多的容器">11、盛水最多的容器</h2><p><img src="/img/LeetCode/数组及数学/3.png" /></p><p>我们使用双指针的思想，两个指针分别为数组的头尾结点，每轮都进行面积的计算，并且每轮只移动较小结点的指针。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maxArea</span>(<span class="hljs-params">self, height: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(height)<br>        left = <span class="hljs-number">0</span>    <span class="hljs-comment"># 左指针</span><br>        right = n - <span class="hljs-number">1</span>   <span class="hljs-comment"># 右指针</span><br>        max_s = <span class="hljs-number">0</span>   <span class="hljs-comment"># 面积最大值</span><br>        s = <span class="hljs-number">0</span>   <span class="hljs-comment"># 面积</span><br>        l = <span class="hljs-number">0</span>   <span class="hljs-comment"># 边长</span><br>        <span class="hljs-keyword">while</span> right &gt; left:<br>            <span class="hljs-keyword">if</span> height[left] &gt; height[right]:<br>                l = height[right]<br>            <span class="hljs-keyword">else</span>:<br>                l = height[left]<br>            s = (right - left) * l<br>            <span class="hljs-keyword">if</span> s &gt; max_s:<br>                max_s = s<br>            <span class="hljs-keyword">if</span> height[left] &gt; height[right]:<br>                right = right - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">else</span>:<br>                left = left + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> max_s<br></code></pre></div></td></tr></table></figure><h2 id="整数转罗马数字">12、整数转罗马数字</h2><p><img src="/img/LeetCode/数组及数学/4.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    A = [<br>        (<span class="hljs-number">1000</span>, <span class="hljs-string">&quot;M&quot;</span>),<br>        (<span class="hljs-number">900</span>, <span class="hljs-string">&quot;CM&quot;</span>),<br>        (<span class="hljs-number">500</span>, <span class="hljs-string">&quot;D&quot;</span>),<br>        (<span class="hljs-number">400</span>, <span class="hljs-string">&quot;CD&quot;</span>),<br>        (<span class="hljs-number">100</span>, <span class="hljs-string">&quot;C&quot;</span>),<br>        (<span class="hljs-number">90</span>, <span class="hljs-string">&quot;XC&quot;</span>),<br>        (<span class="hljs-number">50</span>, <span class="hljs-string">&quot;L&quot;</span>),<br>        (<span class="hljs-number">40</span>, <span class="hljs-string">&quot;XL&quot;</span>),<br>        (<span class="hljs-number">10</span>, <span class="hljs-string">&quot;X&quot;</span>),<br>        (<span class="hljs-number">9</span>, <span class="hljs-string">&quot;IX&quot;</span>),<br>        (<span class="hljs-number">5</span>, <span class="hljs-string">&quot;V&quot;</span>),<br>        (<span class="hljs-number">4</span>, <span class="hljs-string">&quot;IV&quot;</span>),<br>        (<span class="hljs-number">1</span>, <span class="hljs-string">&quot;I&quot;</span>),<br>    ]<br>    <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">intToRoman</span>(<span class="hljs-params">self, num: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">str</span>:<br>        ans = <span class="hljs-string">&quot;&quot;</span><br>        <span class="hljs-keyword">for</span> value, s <span class="hljs-keyword">in</span> self.A:<br>            <span class="hljs-keyword">while</span> num &gt;= value:<br>                num = num - value<br>                ans = ans + s<br>            <span class="hljs-keyword">if</span> num == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">break</span><br>        <br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="罗马数字转整数">13、罗马数字转整数</h2><p><img src="/img/LeetCode/数组及数学/5.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    A = &#123;<br>        <span class="hljs-string">&#x27;I&#x27;</span>: <span class="hljs-number">1</span>,<br>        <span class="hljs-string">&#x27;V&#x27;</span>: <span class="hljs-number">5</span>,<br>        <span class="hljs-string">&#x27;X&#x27;</span>: <span class="hljs-number">10</span>,<br>        <span class="hljs-string">&#x27;L&#x27;</span>: <span class="hljs-number">50</span>,<br>        <span class="hljs-string">&#x27;C&#x27;</span>: <span class="hljs-number">100</span>,<br>        <span class="hljs-string">&#x27;D&#x27;</span>: <span class="hljs-number">500</span>,<br>        <span class="hljs-string">&#x27;M&#x27;</span>: <span class="hljs-number">1000</span>,<br>    &#125;<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">romanToInt</span>(<span class="hljs-params">self, s: <span class="hljs-built_in">str</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        ans = <span class="hljs-number">0</span><br>        n = <span class="hljs-built_in">len</span>(s)<br>        <span class="hljs-keyword">for</span> i, ch <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(s):<br>            <span class="hljs-keyword">if</span> i &lt; n - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> self.A[ch] &lt; self.A[s[i+<span class="hljs-number">1</span>]]:<br>                ans = ans - self.A[ch]<br>            <span class="hljs-keyword">else</span>:<br>                ans = ans + self.A[ch]<br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="三数之和">15、三数之和</h2><p><img src="/img/LeetCode/数组及数学/15.png" /></p><p>采用双指针的思想，为了不使三元组重复，先对数组进行排序。当确定了第一个元素，只需要对剩下元素进行设置左指针和右指针，如果三元组大与0，右指针就左移一位，反之亦然。如果三元组之和刚好等于0，就需要左指针和右指针都移动一位。这时候还会出现重复问题，所以需要判断此时遍历的元素和之前的是否相同，如果相同就要跳过。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">threeSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        nums.sort()<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        ans = <span class="hljs-built_in">list</span>()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<span class="hljs-comment"># 固定其中一个元素</span><br>            <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> nums[i] != nums[i - <span class="hljs-number">1</span>]:<br>                k = n - <span class="hljs-number">1</span><br>                j = i + <span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> k &gt; j:<span class="hljs-comment"># 进行双指针遍历</span><br>                    <span class="hljs-keyword">if</span> k != n - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[k] == nums[k + <span class="hljs-number">1</span>]:<span class="hljs-comment"># 如果右指针之前出现过就跳过</span><br>                        k = k - <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">continue</span><br>                    <span class="hljs-keyword">if</span> j != i + <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[j] == nums[j - <span class="hljs-number">1</span>]:<span class="hljs-comment"># 如果左指针之前出现过就跳过</span><br>                        j = j + <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">continue</span><br>                    s = nums[i] + nums[j] + nums[k]<br>                    <span class="hljs-keyword">if</span> s == <span class="hljs-number">0</span>:<br>                        ans.append([nums[i], nums[j], nums[k]])<br>                        k = k - <span class="hljs-number">1</span><br>                        j = j + <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> s &gt; <span class="hljs-number">0</span>:<br>                        k = k - <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> s &lt; <span class="hljs-number">0</span>:<br>                        j = j + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="最接近的三数之和">16、最接近的三数之和</h2><p><img src="/img/LeetCode/数组及数学/16.png" /></p><p>采用双指针的思想，先对数组进行排序。当确定了第一个元素，只需要对剩下元素进行设置左指针和右指针，如果三元组之和大于target，我们只需要将右节点左移一位，相反亦然。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">threeSumClosest</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        nums.sort()<br>        min_num = <span class="hljs-number">10</span>**<span class="hljs-number">9</span><br>        ans = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n):<br>            <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] == nums[i - <span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">continue</span><br>            k = n - <span class="hljs-number">1</span><br>            j = i + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> k &gt; j:<br>                s = nums[i] + nums[j] + nums[k]<br>                <span class="hljs-keyword">if</span> s == target:<br>                    <span class="hljs-keyword">return</span> s<br>                <span class="hljs-keyword">if</span> s &gt; target:<br>                    <span class="hljs-keyword">if</span> s - target &lt; min_num:<br>                        min_num = s - target<br>                        ans = s<br>                    k = k - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">elif</span> s &lt; target:<br>                    <span class="hljs-keyword">if</span> target - s &lt; min_num:<br>                        min_num = target - s<br>                        ans = s<br>                    j = j + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="四数之和">18、四数之和</h2><p><img src="/img/LeetCode/数组及数学/18.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">fourSum</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]]:<br>        n = <span class="hljs-built_in">len</span>(nums)<br>        <span class="hljs-keyword">if</span> nums == [] <span class="hljs-keyword">or</span> n &lt; <span class="hljs-number">4</span>:<br>            <span class="hljs-keyword">return</span> []<br>        nums.sort()<br>        ans = <span class="hljs-built_in">list</span>()<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n - <span class="hljs-number">3</span>):<br>            <span class="hljs-keyword">if</span> i != <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] == nums[i - <span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">if</span> nums[i] + nums[i + <span class="hljs-number">1</span>] + nums[i + <span class="hljs-number">2</span>] + nums[i + <span class="hljs-number">3</span>] &gt; target:<br>                <span class="hljs-keyword">break</span><br>            <span class="hljs-keyword">if</span> nums[i] + nums[n - <span class="hljs-number">1</span>] + nums[n - <span class="hljs-number">2</span>] + nums[n - <span class="hljs-number">3</span>] &lt; target:<br>                <span class="hljs-keyword">continue</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i + <span class="hljs-number">1</span>, n - <span class="hljs-number">2</span>):<br>                <span class="hljs-keyword">if</span> j != i + <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[j] == nums[j - <span class="hljs-number">1</span>]:<br>                    <span class="hljs-keyword">continue</span><br>                <span class="hljs-keyword">if</span> nums[i] + nums[j] + nums[j + <span class="hljs-number">1</span>] + nums[j + <span class="hljs-number">2</span>] &gt; target:<br>                    <span class="hljs-keyword">break</span><br>                <span class="hljs-keyword">if</span> nums[i] + nums[n - <span class="hljs-number">1</span>] + nums[n - <span class="hljs-number">2</span>] + nums[n - <span class="hljs-number">3</span>] &lt; target:<br>                    <span class="hljs-keyword">continue</span><br>                k = j + <span class="hljs-number">1</span><br>                l = n - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">while</span> k &lt; l:<br>                    <span class="hljs-built_in">sum</span> = nums[i] + nums[j] + nums[k] + nums[l]<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">sum</span> &lt; target:<br>                        k = k + <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">sum</span> &gt; target:<br>                        l = l - <span class="hljs-number">1</span><br>                    <span class="hljs-keyword">elif</span> <span class="hljs-built_in">sum</span> == target:<br>                        ans.append([nums[i], nums[j], nums[k], nums[l]])<br>                        <span class="hljs-keyword">while</span> k &lt; l <span class="hljs-keyword">and</span> nums[k] == nums[k + <span class="hljs-number">1</span>]:<br>                            k = k + <span class="hljs-number">1</span><br>                        k = k + <span class="hljs-number">1</span><br>                        <span class="hljs-keyword">while</span> k &lt; l <span class="hljs-keyword">and</span> nums[l] == nums[l - <span class="hljs-number">1</span>]:<br>                            l = l - <span class="hljs-number">1</span><br>                        l = l - <span class="hljs-number">1</span> <br><br>        <span class="hljs-keyword">return</span> ans<br></code></pre></div></td></tr></table></figure><h2 id="删除有序数组中的重复项">26、删除有序数组中的重复项</h2><p><img src="/img/LeetCode/数组及数学/26.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">removeDuplicates</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> nums == []:<br>            <span class="hljs-keyword">return</span> <span class="hljs-number">0</span><br>        n = <span class="hljs-built_in">len</span>(nums)<br>        left, right = <span class="hljs-number">1</span>, <span class="hljs-number">1</span><br>        <span class="hljs-keyword">while</span> right &lt; n:<br>            <span class="hljs-keyword">if</span> nums[right] != nums[right - <span class="hljs-number">1</span>]:<br>                nums[left] = nums[right]<br>                left = left + <span class="hljs-number">1</span><br>            right = right + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> left<br></code></pre></div></td></tr></table></figure><h2 id="下一个排列">31、下一个排列</h2><p><img src="/img/LeetCode/数组及数学/31.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">nextPermutation</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">        Do not return anything, modify nums in-place instead.</span><br><span class="hljs-string">        &quot;&quot;&quot;</span><br>        i = <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">2</span><br>        <span class="hljs-keyword">while</span> i &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] &gt;= nums[i + <span class="hljs-number">1</span>]:<br>            i -= <span class="hljs-number">1</span><br>        j = <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> i &gt;= <span class="hljs-number">0</span>:<br>            <span class="hljs-keyword">while</span> j &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> nums[i] &gt;= nums[j]:<br>                j -= <span class="hljs-number">1</span><br>            nums[i], nums[j] = nums[j], nums[i]<br>        left, right = i + <span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>        <span class="hljs-keyword">while</span> left &lt; right:<br>            nums[left], nums[right] = nums[right], nums[left]<br>            left += <span class="hljs-number">1</span><br>            right -= <span class="hljs-number">1</span><br></code></pre></div></td></tr></table></figure><h2 id="搜索旋转排序数组">33、搜索旋转排序数组</h2><p><img src="/img/LeetCode/数组及数学/33.png" /></p><p>看到有序的数组，然后对其查找，一般最先要想到的是二分查找</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">search</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> nums:<br>            <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br>        i = <span class="hljs-number">0</span><br>        <span class="hljs-keyword">if</span> nums[<span class="hljs-number">0</span>] &gt; nums[<span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span>]:<br>            <span class="hljs-keyword">while</span> i &lt; <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span> <span class="hljs-keyword">and</span> nums[i] &lt; nums[i + <span class="hljs-number">1</span>]:<br>             i += <span class="hljs-number">1</span><br>            <span class="hljs-keyword">if</span> target &gt; nums[<span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span>]:<br>                left = <span class="hljs-number">0</span><br>                right = i<br>            <span class="hljs-keyword">elif</span> target &lt; nums[<span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span>]:<br>                left = i + <span class="hljs-number">1</span><br>                right = <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">elif</span> target == nums[<span class="hljs-built_in">len</span>(nums) -<span class="hljs-number">1</span>]:<br>                <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>        <span class="hljs-keyword">else</span>:<br>            left, right = <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>        <br>        <span class="hljs-keyword">while</span> left &lt;= right:<br>            mid = (left + right) // <span class="hljs-number">2</span><br>            <span class="hljs-keyword">if</span> target == nums[mid]:<br>                <span class="hljs-keyword">return</span> mid<br>            <span class="hljs-keyword">elif</span> target &lt; nums[mid]:<br>                right = mid - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">elif</span> target &gt; nums[mid]:<br>                left = mid + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> -<span class="hljs-number">1</span><br></code></pre></div></td></tr></table></figure><h2id="在排序数组中查找元素的第一个和最后一个">34、在排序数组中查找元素的第一个和最后一个</h2><p><img src="/img/LeetCode/数组及数学/34.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">searchRange</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], target: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]:<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> nums:<br>            <span class="hljs-keyword">return</span> [-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">binarySearch</span>(<span class="hljs-params">nums, target</span>):<br>            left, right = <span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(nums) - <span class="hljs-number">1</span><br>            <span class="hljs-keyword">while</span> left &lt;= right:<br>                mid = (left + right) // <span class="hljs-number">2</span><br>                <span class="hljs-keyword">if</span> nums[mid] &gt;= target:<br>                    right = mid - <span class="hljs-number">1</span><br>                <span class="hljs-keyword">elif</span> nums[mid] &lt; target:<br>                    left = mid + <span class="hljs-number">1</span><br>            <span class="hljs-keyword">return</span> left<br>        a = binarySearch(nums, target)<br>        b = binarySearch(nums, target + <span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> a == <span class="hljs-built_in">len</span>(nums) <span class="hljs-keyword">or</span> nums[a] != target:<br>            <span class="hljs-keyword">return</span>[-<span class="hljs-number">1</span>, -<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">return</span>[a, b - <span class="hljs-number">1</span>]<br></code></pre></div></td></tr></table></figure><h2 id="powx-n">50、Pow(x, n)</h2><p><img src="/img/LeetCode/数组及数学/50.png" /></p><p>使用递归的思想，直接循环会导致运行时间过长</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">myPow</span>(<span class="hljs-params">self, x: <span class="hljs-built_in">float</span>, n: <span class="hljs-built_in">int</span></span>) -&gt; <span class="hljs-built_in">float</span>:<br>        <span class="hljs-keyword">def</span> <span class="hljs-title function_">quickPow</span>(<span class="hljs-params">N</span>):<br>            <span class="hljs-keyword">if</span> N == <span class="hljs-number">0</span>:<br>                <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span><br>            y = quickPow(N // <span class="hljs-number">2</span>)<br>            <span class="hljs-keyword">return</span> y * y <span class="hljs-keyword">if</span> N % <span class="hljs-number">2</span> == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> y * y * x<br>        <span class="hljs-keyword">return</span> quickPow(n) <span class="hljs-keyword">if</span> n &gt;= <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1.0</span> / quickPow(-n)<br></code></pre></div></td></tr></table></figure><h2 id="最大子数组和">53、最大子数组和</h2><p><img src="/img/LeetCode/数组及数学/53.png" /></p><p>看到这种区间的，可以想到动态规划</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Solution</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">maxSubArray</span>(<span class="hljs-params">self, nums: <span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>]</span>) -&gt; <span class="hljs-built_in">int</span>:<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(nums)):<br>            nums[i] += <span class="hljs-built_in">max</span>(nums[i - <span class="hljs-number">1</span>], <span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">max</span>(nums)<br></code></pre></div></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>LeetCode</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>pytorch</title>
    <link href="/2022/04/27/pytorch/"/>
    <url>/2022/04/27/pytorch/</url>
    
    <content type="html"><![CDATA[<h1 id="pytorch">PyTorch</h1><h2 id="dataset">1、DataSet</h2><p>DataSet：提供一种方法去获取数据及其label</p><p>使用的数据集为蜜蜂与蚂蚁的图像数据集，分别保存在dataset/train/bees以及dataset/train/ants</p><p>DataSet需要通过继承重载才能使用，使用方法如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> os<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, root_dir, label_dir</span>):<span class="hljs-comment"># read data &amp; preprocess</span><br>        self.root_dir = root_dir    <span class="hljs-comment"># 根目录</span><br>        self.label_dir = label_dir  <span class="hljs-comment"># 标记目录</span><br>        self.path = os.path.join(self.root_dir, self.label_dir)     <span class="hljs-comment"># 地址相加</span><br>        self.img_path = os.listdir(self.path)   <span class="hljs-comment"># 生成图片文件列表</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, index</span>):<span class="hljs-comment"># returns one sample at a time</span><br>        img_name = self.img_path[index]   <span class="hljs-comment"># 文件名</span><br>        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)<br>        img = Image.<span class="hljs-built_in">open</span>(img_item_path)<br>        label = self.label_dir<br>        <span class="hljs-keyword">return</span> img, label<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<span class="hljs-comment"># returns the size of the dataset</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_path)<br><br><br><span class="hljs-comment"># 读取蚂蚁和蜜蜂的图片数据集</span><br>root_dir = <span class="hljs-string">&quot;dataset/train&quot;</span><br>ants_label_dir = <span class="hljs-string">&quot;ants&quot;</span><br>bees_label_dir = <span class="hljs-string">&quot;bees&quot;</span><br>ants_dataset = MyData(root_dir, ants_label_dir)<br>bees_dataset = MyData(root_dir, bees_label_dir)<br><br><span class="hljs-comment"># 将两个数据集进行合并，成为训练数据集</span><br>train_dataset = ants_dataset + bees_dataset<br></code></pre></div></td></tr></table></figure><h2 id="dataloader">2、DataLoader</h2><p>DataSet就相当于是一整个数据集，而DataLoader是取出其中一部分到神经网络中进行使用。</p><p>使用的是torchvision所提供的数据集。DataLoader中的参数分别释义如下：</p><ul><li>dataset：我们所使用的数据集，即dataset类型数据</li><li>batch_size：一次抓取多少个数据</li><li>shuffle：抓取时是否打乱顺序</li><li>num_workers：代表创建了多少个worker进程，0表示只有主进程去加载batch数据，1表示有一个worker进程加载batch数据</li><li>drop_last：无法整除时，最后剩余的几条数据要不要去除</li></ul><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br><span class="hljs-comment"># 准备的测试数据集</span><br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(), download=<span class="hljs-literal">True</span>)<br><br>test_loader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>, num_workers=<span class="hljs-number">0</span>, drop_last=<span class="hljs-literal">False</span>)<br><br>writer = SummaryWriter(<span class="hljs-string">&quot;dataloader&quot;</span>)<br>step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_loader:<br>    imgs, targets = data<br>    writer.add_images(<span class="hljs-string">&quot;test_data&quot;</span>, imgs, step)<br>    step = step + <span class="hljs-number">1</span><br><br>writer.close()<br></code></pre></div></td></tr></table></figure><p>最后显示结果如下：</p><p><img src="/img/pytorch/1.png" /></p><h2 id="tensorboard">3、TensorBoard</h2><p>我们可以通过TensorBoard可以查看图像</p><h3 id="输出函数图像">3.1输出函数图像</h3><p>首先我们通过TensorBoard来绘制y=x的图像，我们需要先生成一个实例，随后通过add_scalar()方法来添加，参数分别为名称，y轴的值，x轴的值。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-comment"># 创建一个实例,存储在logs文件夹下</span><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br><br><span class="hljs-comment"># y = x图像</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&quot;y=x&quot;</span>, i, i)<br><br>writer.close()<br></code></pre></div></td></tr></table></figure><p>完成后，我们可以通过控制台输入如下指令来观看图像</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensorboard --logdir=logs<br></code></pre></div></td></tr></table></figure><p>这时候我们可以通过6006端口来查看图像，但是如果有很多tensorboard都要查看呢，我们可以选择自定义端口查看，例如我们要选择6007端口来查看，指令如下：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensorboard --logdir=logs --port=6007<br></code></pre></div></td></tr></table></figure><p><img src="/img/pytorch/2.png" /></p><p>这时候我们就可以看到y=x的图像</p><p>如果是y=2x的图像则需要修改代码为：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># y = 2x图像</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    writer.add_scalar(<span class="hljs-string">&quot;y=2x&quot;</span>, <span class="hljs-number">2</span>*i, i)<br></code></pre></div></td></tr></table></figure><h3 id="输出图片">3.2输出图片</h3><p>我们也可以通过TensorBoard来显示我们的图片，通过TensorBoard的add_image()方法，参数分别为名称、图片（需要为tensor类型或者numpy类型）、global_step、类型（因为默认的类型为是（3，H，W）即通道(channel)为3，H为高度，W为宽度），代码如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><br><span class="hljs-comment"># 创建一个实例,存储在logs文件夹下</span><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>image_path = <span class="hljs-string">&quot;dataset/train/ants/0013035.jpg&quot;</span><br>img_PIL = Image.<span class="hljs-built_in">open</span>(image_path)<br>img_array = np.array(img_PIL)<br><br>writer.add_image(<span class="hljs-string">&quot;test&quot;</span>, img_array, <span class="hljs-number">1</span>, dataformats=<span class="hljs-string">&#x27;HWC&#x27;</span>)<br><br>writer.close()<br></code></pre></div></td></tr></table></figure><h2 id="神经网络的基本骨架">4、神经网络的基本骨架</h2><p>基本骨架是通过对nn.Module的继承重写实现的，大体实现如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = <span class="hljs-built_in">input</span> + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">return</span> output<br><br>res = MyModel()<br>x = torch.tensor(<span class="hljs-number">1.0</span>)<br>output = res(x)<span class="hljs-comment"># 有__call__方法来调用，所以可以直接将x输入，不用我们来调用forward()方法</span><br><span class="hljs-built_in">print</span>(output)<br><br></code></pre></div></td></tr></table></figure><p>最后我们可以看到控制台输出为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor(2.)<br></code></pre></div></td></tr></table></figure><h2 id="卷积层">5、卷积层</h2><p>首先对于卷积层的学习，要了解卷积层的使用，我们先使用torch.nn.functional中的方法（一般都是使用torch.nn，其对functional进行了封装，这里是为了了解如何卷积）进行学习，首先要对conv2d的stride参数进行了解，stride时计算卷积时移动的步长，我们可以使用代码来了解：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                      [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])<br><br>kernel = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],<br>                       [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>],<br>                       [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])<br><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br>kernel = torch.reshape(kernel, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>))<br><br>output = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output)<br><br>output2 = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(output2)<br></code></pre></div></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[10, 12, 12],<br>          [18, 16, 16],<br>          [13,  9,  3]]]])<br>tensor([[[[10, 12],<br>          [13,  3]]]])<br></code></pre></div></td></tr></table></figure><p>图示为：</p><p><img src="/img/pytorch/4.png" /></p><p>随后我们要学习的是conv2d中的padding参数，它是对输入图像的周围进行填充，并设置填充值：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">output3 = F.conv2d(<span class="hljs-built_in">input</span>, kernel, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(output3)<br></code></pre></div></td></tr></table></figure><p>输出结果为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[ 1,  3,  4, 10,  8],<br>          [ 5, 10, 12, 12,  6],<br>          [ 7, 18, 16, 16,  8],<br>          [11, 13,  9,  3,  4],<br>          [14, 13,  9,  7,  4]]]])<br></code></pre></div></td></tr></table></figure><p>图示为：</p><p><img src="/img/pytorch/5.png" /></p><p>现在对卷积层的知识进行了学习后正式来时学习卷积层是如何搭建的，我们要使用的是torch.nn中的conv2d方法，其方法的参数解释在官方文档中如下：</p><ul><li><strong>in_channels</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)– Number of channels in the input image</li><li><strong>out_channels</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>)– Number of channels produced by the convolution</li><li><strong>kernel_size</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>or</em> <ahref="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a>)– Size of the convolving kernel</li><li><strong>stride</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>or</em> <ahref="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em><em>optional</em>) – Stride of the convolution. Default: 1</li><li><strong>padding</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em><ahref="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>or</em> <ahref="https://docs.python.org/3/library/stdtypes.html#str"><em>str</em></a><em>,</em><em>optional</em>) – Padding added to all four sides of the input.Default: 0</li><li><strong>padding_mode</strong> (*string**,* <em>optional</em>) –<code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code>or <code>'circular'</code>. Default: <code>'zeros'</code></li><li><strong>dilation</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>or</em> <ahref="https://docs.python.org/3/library/stdtypes.html#tuple"><em>tuple</em></a><em>,</em><em>optional</em>) – Spacing between kernel elements. Default: 1</li><li><strong>groups</strong> (<ahref="https://docs.python.org/3/library/functions.html#int"><em>int</em></a><em>,</em><em>optional</em>) – Number of blocked connections from input channelsto output channels. Default: 1</li><li><strong>bias</strong> (<ahref="https://docs.python.org/3/library/functions.html#bool"><em>bool</em></a><em>,</em><em>optional</em>) – If <code>True</code>, adds a learnable bias to theoutput. Default: <code>True</code></li></ul><p><img src="/img/pytorch/3.png" /></p><p>我们简单的对图像进行卷积处理：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Conv2d<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br>dataloader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.conv1 = Conv2d(in_channels=<span class="hljs-number">3</span>, out_channels=<span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>myModel = MyModel()<br><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs&quot;</span>)<br>step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs, targets = data<br>    output = myModel(imgs)<br>    <span class="hljs-comment"># print(imgs.shape)</span><br>    <span class="hljs-comment"># print(output.shape)</span><br><br>    <span class="hljs-comment"># torch.Size([64, 3, 32, 32])</span><br>    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, imgs, step)<br>    <span class="hljs-comment"># torch.Size([64, 6, 32, 32]) -&gt; [xxx, 3, 32, 32]，因为out_channels为6没办法add进去</span><br>    output = torch.reshape(output, (-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">30</span>, <span class="hljs-number">30</span>))<br>    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, output, step)<br><br>    step = step + <span class="hljs-number">1</span><br></code></pre></div></td></tr></table></figure><p>运行结果在tensorboard上显示为：</p><p><img src="/img/pytorch/6.png" /></p><p><img src="/img/pytorch/7.png" /></p><h2 id="池化层">6、池化层</h2><p>我们要使用的是torch.nn中的MaxPool2d方法，其中参数解释为：</p><ul><li><strong>kernel_size</strong> – the size of the window to take a maxover</li><li><strong>stride</strong> – the stride of the window. Default value is<code>kernel_size</code></li><li><strong>padding</strong> – implicit zero padding to be added on bothsides</li><li><strong>dilation</strong> – a parameter that controls the stride ofelements in the window</li><li><strong>return_indices</strong> – if <code>True</code>, will returnthe max indices along with the outputs. Useful for <ahref="https://pytorch.org/docs/stable/generated/torch.nn.MaxUnpool2d.html#torch.nn.MaxUnpool2d"><code>torch.nn.MaxUnpool2d</code></a>later</li><li><strong>ceil_mode</strong> – when True, will use ceil instead offloor to compute the output shape</li></ul><p>对于ceil_mode的两种情况如下图示意：</p><p><img src="/img/pytorch/8.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> MaxPool2d<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],<br>                      [<span class="hljs-number">5</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],<br>                      [<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.float32)<br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>))<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.maxpool1 = MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.maxpool1(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br><br>myModel = MyModel()<br>output = myModel(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></div></td></tr></table></figure><p>运行结果：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[2., 3.],<br>          [5., 1.]]]])<br></code></pre></div></td></tr></table></figure><p>当我们将ceil_mode修改为False后：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">self.maxpool1 = MaxPool2d(kernel_size=<span class="hljs-number">3</span>, ceil_mode=<span class="hljs-literal">False</span>)<br></code></pre></div></td></tr></table></figure><p>结果为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[2.]]]])<br></code></pre></div></td></tr></table></figure><p>我们可以直观的感受一下最大池化后的结果，我们可以将输入换成我们的数据集，在tensorboard上查看输入输出的差别：</p><p><img src="/img/pytorch/9.png" /></p><h2 id="非线性激活">7、非线性激活</h2><p>首先我们要使用的是torch.nn中的ReLU方法</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> ReLU<br><br><span class="hljs-built_in">input</span> = torch.tensor([[<span class="hljs-number">1</span>, -<span class="hljs-number">0.5</span>],<br>                      [-<span class="hljs-number">1</span>, <span class="hljs-number">3</span>]])<br><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>))<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.relu1 = ReLU()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.relu1(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br><br>myModel = MyModel()<br>output = myModel(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-built_in">input</span>)<br><span class="hljs-built_in">print</span>(output)<br></code></pre></div></td></tr></table></figure><p>结果为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[[[ 1.0000, -0.5000],<br>          [-1.0000,  3.0000]]]])<br>tensor([[[[1., 0.],<br>          [0., 3.]]]])<br></code></pre></div></td></tr></table></figure><p>然后我们使用torch.nn中的Sigmoid方法更直观的来查看结果：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> Sigmoid<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br>dataloader = DataLoader(dataset=test_data, batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.sigmoid1 = Sigmoid()<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, <span class="hljs-built_in">input</span></span>):<br>        output = self.sigmoid1(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-keyword">return</span> output<br><br>myModel = MyModel()<br>writer = SummaryWriter(<span class="hljs-string">&quot;logs_sigmoid&quot;</span>)<br>step = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs, target = data<br>    output = myModel(imgs)<br>    writer.add_images(<span class="hljs-string">&quot;input&quot;</span>, imgs, step)<br>    writer.add_images(<span class="hljs-string">&quot;output&quot;</span>, output, step)<br>    step = step + <span class="hljs-number">1</span><br><br>writer.close()<br></code></pre></div></td></tr></table></figure><p>在tensorboard中查看运行结果：</p><p><img src="/img/pytorch/10.png" /></p><h2 id="线性层-及其他层">8、线性层 及其他层</h2><p>主要还是查看官方文档。</p><h2id="搭建小实例和sequential的使用">9、搭建小实例和Sequential的使用</h2><p>我们使用之前学习过的基础来搭建一个简单的神经网络，例如我们来搭建cifar10的模型，其如下图所示</p><p><img src="/img/pytorch/11.png" /></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.conv1 = Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.maxpool1 = MaxPool2d(<span class="hljs-number">2</span>)<br>        self.conv2 = Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.maxpool2 = MaxPool2d(<span class="hljs-number">2</span>)<br>        self.conv3 = Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        self.maxpool3 = MaxPool2d(<span class="hljs-number">2</span>)<br>        self.flatten = Flatten()<br>        self.linear1 = Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>)<br>        self.linear2 = Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br><br>        self.model1 = Sequential(<br>            Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.conv1(x)<br>        x = self.maxpool1(x)<br>        x = self.conv2(x)<br>        x = self.maxpool2(x)<br>        x = self.conv2(x)<br>        x = self.maxpool2(x)<br>        x = self.flatten(x)<br>        x = self.linear1(x)<br>        x = self.linear2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></div></td></tr></table></figure><p>引入了Sequential方法后可以将我们搭建的模型简写出来</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        self.model1 = Sequential(<br>            Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>),<br>            MaxPool2d(<span class="hljs-number">2</span>),<br>            Flatten(),<br>            Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">64</span>),<br>            Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model1(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></div></td></tr></table></figure><h2 id="损失函数与反向传播">10、损失函数与反向传播</h2><p>我们可以使用torch.nn中的L1Loss以及MSELoss方法来构造损失函数：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-built_in">input</span> = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], dtype=torch.float32)<br>target = torch.tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>], dtype=torch.float32)<br><br><span class="hljs-built_in">input</span> = torch.reshape(<span class="hljs-built_in">input</span>, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))<br>target = torch.reshape(target, (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>))<br><br>loss = L1Loss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>)<br>result = loss(<span class="hljs-built_in">input</span>, target)<br><br>loss_mse = nn.MSELoss()<br>result_mse = loss_mse(<span class="hljs-built_in">input</span>, target)<br><br><span class="hljs-built_in">print</span>(result)<br><span class="hljs-built_in">print</span>(result_mse)<br></code></pre></div></td></tr></table></figure><p>运行结果为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor(2.)<br>tensor(1.3333)<br></code></pre></div></td></tr></table></figure><p>我们可以利用交叉熵作为损失函数来进行反向传播，利用backward方法：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">myModel = MyModel()<br>loss = nn.CrossEntropyLoss()<br><span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>    imgs, targets = data<br>    output = myModel(imgs)<br>    result_loss = loss(output, targets)<br>    result_loss.backward()<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;ok&quot;</span>)<br></code></pre></div></td></tr></table></figure><h2 id="优化器">11、优化器</h2><p>使用torch.optim中的优化器:</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">myModel = MyModel()<br>loss = nn.CrossEntropyLoss()<br>optim = torch.optim.SGD(myModel.parameters(), lr=<span class="hljs-number">0.01</span>)<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20</span>):<br>    runing_loss = <span class="hljs-number">0.0</span><br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> dataloader:<br>        imgs, targets = data<br>        output = myModel(imgs)<br>        result_loss = loss(output, targets)<br>        optim.zero_grad()<span class="hljs-comment"># 将梯度初始化为0</span><br>        result_loss.backward()<br>        optim.step()<span class="hljs-comment"># 进行优化</span><br>        runing_loss = runing_loss + result_loss<br>    <span class="hljs-built_in">print</span>(runing_loss)<br></code></pre></div></td></tr></table></figure><p>运行结果：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">tensor(<span class="hljs-number">18666.8984</span>, grad_fn=&lt;AddBackward0&gt;)<br>tensor(<span class="hljs-number">16161.6846</span>, grad_fn=&lt;AddBackward0&gt;)<br>tensor(<span class="hljs-number">15338.8057</span>, grad_fn=&lt;AddBackward0&gt;)<br>...<br></code></pre></div></td></tr></table></figure><p><code>torch.optim.lr_scheduler</code>模块提供了一些根据epoch训练次数来调整学习率（learningrate）的方法。一般情况下我们会设置随着epoch的增大而逐渐减小学习率从而达到更好的训练效果。</p><p><code>torch.optim.lr_scheduler.LambdaLR</code>中大部分调整学习率的方法都是根据epoch训练次数</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">torch</span>.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-<span class="hljs-number">1</span>)<br></code></pre></div></td></tr></table></figure><p>更新策略： <span class="math display">\[new\_lr=\lambda \times initial\_lr\]</span> 其中new_lr是得到的新的学习率，initial_lr是初始的学习率，<spanclass="math inline">\(\lambda\)</span>是通过参数lr_lambda和epoch得到的。</p><p>参数：</p><p>optimizer （Optimizer）：要更改学习率的优化器； lr_lambda（functionor list）：根据epoch计算<spanclass="math inline">\(\lambda\)</span>的函数；或者是一个list的这样的function，分别计算各个parametergroups的学习率更新用到的<span class="math inline">\(\lambda\)</span>；last_epoch（int）：最后一个epoch的index，如果是训练了很多个epoch后中断了，继续训练，这个值就等于加载的模型的epoch。默认为-1表示从头开始训练，即从epoch=1开始。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">net_1 = model()<br><br>optimizer_1 = torch.optim.Adam(net_1.parameters(), lr = initial_lr)<br>scheduler_1 = LambdaLR(optimizer_1, lr_lambda=<span class="hljs-keyword">lambda</span> epoch: <span class="hljs-number">1</span>/(epoch+<span class="hljs-number">1</span>))<br></code></pre></div></td></tr></table></figure><h2 id="现有网络模型的使用与修改">12、现有网络模型的使用与修改</h2><p>我们使用torchvision中现有的vgg16模型进行使用：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">vgg16_true = torchvison.models.vgg16(pretrained=<span class="hljs-literal">True</span>)<br><span class="hljs-comment"># 添加层</span><br>vgg16_true.classifier.add_module(<span class="hljs-string">&#x27;add_linear&#x27;</span>, nn.Linear(<span class="hljs-number">1000</span>, <span class="hljs-number">10</span>))<br><span class="hljs-comment"># 修改层</span><br>vgg16_true.classifier[<span class="hljs-number">6</span>] = nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>)<br></code></pre></div></td></tr></table></figure><h2 id="网络模型的保存与读取">13、网络模型的保存与读取</h2><p>保存方法：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)<br><span class="hljs-comment"># 保存方法1</span><br>torch.save(vgg16, <span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)<br><br><span class="hljs-comment"># 保存方法二，将模型参数保存为字典（官方推荐）</span><br>torch.save(vgg16.state_dict(), <span class="hljs-string">&quot;vgg16_method2.pth&quot;</span>)<br></code></pre></div></td></tr></table></figure><p>读取方法：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 方法1保存的模型进行读取</span><br>model = torch.load(<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>)<br><br><span class="hljs-comment"># 方法2保存的模型进行读取</span><br>vgg16 = vgg16 = torchvision.models.vgg16(pretrained=<span class="hljs-literal">False</span>)<br>vgg16.load_state_dict(torch.load(<span class="hljs-string">&quot;vgg16_method1.pth&quot;</span>))<br></code></pre></div></td></tr></table></figure><h2 id="完整的模型训练">14、完整的模型训练</h2><p>model.py：用于保存网络模型</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br><span class="hljs-comment"># 验证</span><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">&#x27;__main__&#x27;</span>:<br>    myModel = MyModel()<br>    <span class="hljs-built_in">input</span> = torch.ones((<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>    output = myModel(<span class="hljs-built_in">input</span>)<br>    <span class="hljs-built_in">print</span>(output.shape)<br></code></pre></div></td></tr></table></figure><p>train.py：进行网络训练</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torchvision<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><span class="hljs-keyword">from</span> torch.utils.tensorboard <span class="hljs-keyword">import</span> SummaryWriter<br><br><span class="hljs-keyword">from</span> model <span class="hljs-keyword">import</span> *<br><br><span class="hljs-comment"># 准备数据集</span><br>train_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">True</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br>test_data = torchvision.datasets.CIFAR10(<span class="hljs-string">&quot;./dataset&quot;</span>, train=<span class="hljs-literal">False</span>, transform=torchvision.transforms.ToTensor(),<br>                                         download=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># 获取数据集长度</span><br>train_data_length = <span class="hljs-built_in">len</span>(train_data)<br>test_data_length = <span class="hljs-built_in">len</span>(test_data)<br><br><span class="hljs-comment"># 利用DataLoader加载数据</span><br>train_dataloader = DataLoader(train_data, batch_size=<span class="hljs-number">64</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>)<br><br><span class="hljs-comment"># 创建网络模型</span><br>myModel = MyModel()<br><br><span class="hljs-comment"># 损失函数</span><br>loss_fn = nn.CrossEntropyLoss()<br><br><span class="hljs-comment"># 优化器</span><br>learning_rate = <span class="hljs-number">0.01</span><br>optimizer = torch.optim.SGD(myModel.parameters(), lr=learning_rate)<br><br><span class="hljs-comment"># 设置训练网络的一些参数</span><br><span class="hljs-comment"># 记录训练的次数</span><br>total_train_step = <span class="hljs-number">0</span><br><span class="hljs-comment"># 记录测试的次数</span><br>total_test_step = <span class="hljs-number">0</span><br><span class="hljs-comment"># 训练论数</span><br>epoch = <span class="hljs-number">10</span><br><br><span class="hljs-comment"># 添加tensorboard</span><br>writer = SummaryWriter(<span class="hljs-string">&quot;logs_train&quot;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epoch):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;------第%d轮训练开始------&quot;</span> % (i+<span class="hljs-number">1</span>))<br><br>    <span class="hljs-comment"># 训练步骤开始</span><br>    myModel.train()<br>    <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> train_dataloader:<br>        imgs, targets = data<br>        outputs = myModel(imgs)<br>        loss = loss_fn(outputs, targets)<br><br>        <span class="hljs-comment"># 优化器优化模型</span><br>        <span class="hljs-comment"># 利用优化器进行梯度清零</span><br>        optimizer.zero_grad()<br>        <span class="hljs-comment"># 反向传播</span><br>        loss.backward()<br>        optimizer.step()<br><br>        total_train_step = total_train_step + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> total_train_step % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;训练次数：&#123;&#125;，loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_train_step, loss.item()))<br>            writer.add_scalar(<span class="hljs-string">&quot;train_loss&quot;</span>, loss.item(), total_train_step)<br><br>    <span class="hljs-comment"># 测试步骤开始</span><br>    myModel.<span class="hljs-built_in">eval</span>()<br>    total_test_loss = <span class="hljs-number">0</span><br>    total_accuracy = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> test_dataloader:<br>            imgs, targets = data<br>            outputs = myModel(imgs)<br>            loss = loss_fn(outputs, targets)<br>            total_test_loss = total_test_loss + loss.item()<br>            accuracy = (outputs.argmax(<span class="hljs-number">1</span>) == targets).<span class="hljs-built_in">sum</span>()<br>            total_accuracy = total_accuracy + accuracy<br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_test_loss))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="hljs-built_in">format</span>(total_accuracy/test_data_length))<br>    writer.add_scalar(<span class="hljs-string">&quot;test_loss&quot;</span>, total_test_loss, total_test_step)<br>    writer.add_scalar(<span class="hljs-string">&quot;test_accuracy&quot;</span>, total_accuracy/test_data_length, total_test_step)<br>    total_test_step = total_test_step + <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># 保存每轮的数据</span><br>    torch.save(myModel, <span class="hljs-string">&quot;myModel_&#123;&#125;.pth&quot;</span>.<span class="hljs-built_in">format</span>(i))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;模型已保存&quot;</span>)<br><br>writer.close()<br></code></pre></div></td></tr></table></figure><h2 id="使用gpu训练">15、使用GPU训练</h2><h3 id="方法一">方法一</h3><p>我们需要对网络模型、数据、损失函数进行修改</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 构建模型</span><br>myModel = MyModel()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    myModel = myModel.cuda()<br>    <br><span class="hljs-comment"># 损失函数</span><br>loss_fn = nn.CrossEntropyLoss()<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    loss_fn = loss_fn.cuda()<br>    <br>imgs, targets = data<br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>imgs = imgs.cuda()<br>targets = targets.cuda()<br></code></pre></div></td></tr></table></figure><h3 id="方法二">方法二</h3><p>需要在文件最开始定义训练的设备</p><p>此时为将设备设置为cpu</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 定义训练的设备</span><br>device = torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)<br><br><span class="hljs-comment"># 构建模型</span><br>myModel = MyModel()<br>myModel.to(device)<span class="hljs-comment"># 模型和损失函数不用重新赋值</span><br><br><span class="hljs-comment"># 损失函数</span><br>loss_fn = nn.CrossEntropyLoss()<br>loss_fn.to(device)<br><br>imgs, targets = data<br>imgs = imgs.to(device)<span class="hljs-comment"># 数据需要重新赋值</span><br>targets = targets.to(device)<br></code></pre></div></td></tr></table></figure><p>如果要使用gpu需要如下设置：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 定义训练的设备</span><br>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>)<br></code></pre></div></td></tr></table></figure><p>如果有多个gpu，可以按照如下选择设置：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># 定义训练的设备</span><br>device = torch.device(<span class="hljs-string">&quot;cuda：0&quot;</span>)<br></code></pre></div></td></tr></table></figure><h2 id="模型验证套路">16、模型验证套路</h2><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">image_path = <span class="hljs-string">&quot;./img/dog.png&quot;</span><br><br>image = Image.<span class="hljs-built_in">open</span>(image_path)<br>image = image.convert(<span class="hljs-string">&#x27;RGB&#x27;</span>)<br><br>transform = torchvision.transforms.Compose([torchvision.transforms.Resize((<span class="hljs-number">32</span>, <span class="hljs-number">32</span>)),<br>                                            torchvision.transforms.ToTensor()])<br><br>image = transform(image)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyModel</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(MyModel, self).__init__()<br>        self.model = nn.Sequential(<br>            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>),<br>            nn.MaxPool2d(<span class="hljs-number">2</span>),<br>            nn.Flatten(),<br>            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">4</span> * <span class="hljs-number">4</span>, <span class="hljs-number">64</span>),<br>            nn.Linear(<span class="hljs-number">64</span>, <span class="hljs-number">10</span>)<br>        )<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.model(x)<br>        <span class="hljs-keyword">return</span> x<br><br><br>myModel = torch.load(<span class="hljs-string">&quot;myModel_0.pth&quot;</span>)<br><span class="hljs-built_in">print</span>(myModel)<br>image = torch.reshape(image, (<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">32</span>, <span class="hljs-number">32</span>))<br>myModel.<span class="hljs-built_in">eval</span>()<br><span class="hljs-keyword">with</span> torch.no_grad():<br>    output = myModel(image)<br><span class="hljs-built_in">print</span>(output)<br><span class="hljs-built_in">print</span>(output.argmax(<span class="hljs-number">1</span>))<br></code></pre></div></td></tr></table></figure><p>输出为：</p><figure class="highlight shell"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs shell">tensor([[ 0.3308,  0.0307,  0.9064,  0.9474,  0.2715,  0.8123, -0.4077,  0.2044,<br>         -0.4149, -1.0106]])<br>tensor([3])<br></code></pre></div></td></tr></table></figure><p>可以看到最后预测我们的图片属于第三类</p><h2 id="lstm">17、LSTM</h2><p><strong>输入的参数列表包括:</strong></p><ul><li>input_size输入数据的特征维数，通常就是embedding_dim(词向量的维度)</li><li>hidden_size　LSTM中隐层的维度</li><li>num_layers　循环神经网络的层数</li><li>bias　用不用偏置，default=True</li><li>batch_first这个要注意，通常我们输入的数据shape=(batch_size,seq_length,embedding_dim),而batch_first默认是False,所以我们的输入数据最好送进LSTM之前将batch_size与seq_length这两个维度调换</li><li>dropout　默认是0，代表不用dropout</li><li>bidirectional默认是false，代表不用双向LSTM</li></ul><p><strong>输入数据包括input,(h_0,c_0):</strong></p><ul><li>input就是shape=(seq_length,batch_size,input_size)的张量</li><li>h_0是shape=(num_layers*num_directions,batch_size,hidden_size)的张量，它包含了在当前这个batch_size中每个句子的初始隐藏状态。其中num_layers就是LSTM的层数。如果bidirectional=True,num_directions=2,否则就是１，表示只有一个方向。</li><li>c_0和h_0的形状相同，它包含的是在当前这个batch_size中的每个句子的初始细胞状态。h_0,c_0如果不提供，那么默认是０。</li></ul><p><strong>输出数据包括output,(h_n,c_n):</strong></p><ul><li>output的shape=(seq_length,batch_size,num_directions*hidden_size),它包含的是LSTM的最后一时间步的输出特征(h_t),ｔ是batch_size中每个句子的长度。</li><li>h_n.shape==(num_directions * num_layers,batch,hidden_size)</li><li>h_n包含的是句子的最后一个单词（也就是最后一个时间步）的隐藏状态，c_n包含的是句子的最后一个单词的细胞状态，所以它们都与句子的长度seq_length无关。</li><li>output[-1]与h_n是相等的，因为output[-1]包含的正是batch_size个句子中每一个句子的最后一个单词的隐藏状态，注意LSTM中的隐藏状态其实就是输出，cellstate细胞状态才是LSTM中一直隐藏的，记录着信息。</li></ul><h2 id="lstmcell">18、LSTMcell</h2><p>上述的nn.LSTM模块一次构造完若干层的LSTM,但是为了对模型有更加灵活的处nn中还有一个LSTMCell模块，是组成LSTM整个序列计算过程的基本组成单元，也就是进行sequence中一个word的计算。</p><p>同时它的参数也就只有3个：输入维度，隐节点数据维度，是否带有偏置。仅提供最基本一个LSTM单元结构，想要完整的进行一个序列的训练的要还要自己编写传播函数把cell间的输入输出连接起来。但是想要自定义不同cell隐节点维度的多层LSTM的话，这个模块还是挺有用的。</p><h2 id="pad_sequence">19、pad_sequence</h2><p><strong>sequences</strong>：表示输入样本序列，为 list 类型，list中的元素为 tensor 类型。 tensor 的 size 为 L * F 。其中，L为单个序列的长度，F 为序列中每个时间步（timestep）特征的个数，根据任务的不同 F 的维度会有所不同。</p><p><strong>batch_first</strong>：为 True 对应 [batch_size, seq_len,feature]；False 对应[seq_len, batch_size,feature]，从习惯上来讲一般设置为 True 比较符合我们的认知。</p><p><strong>padding_value</strong>：填充值，默认值为 0 。</p><p><strong>说明</strong></p><p>主要用来对样本进行填充，填充值一般为 0。我们在训练网络时，一般会采用一个一个 mini-batch的方式，将训练样本数据喂给网络。在 PyTorch 里面数据都是以 tensor的形式存在，一个 mini-batch 实际上就是一个高维的 tensor，每个序列数据的长度必须相同才能组成一个 tensor 。为了使网络可以处理mini-batch 形式的数据，就必须对序列样本进行填充，保证一个 mini-batch里面的数据长度是相同的。</p><p>在 PyTorch 里面一般是使用 DataLoader 进行数据加载，返回 mini-batch形式的数据，再将此数据喂给网络进行训练。我们一般会自定义一个 collate_fn函数，完成对数据的填充。</p><p><strong>示例</strong></p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset, DataLoader<br><span class="hljs-keyword">from</span> torch.nn.utils.rnn <span class="hljs-keyword">import</span> pad_sequence,pack_padded_sequence,pack_sequence,pad_packed_sequence<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>        self.data = data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.data[idx]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>    data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>, padding_value=<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">return</span> data<br><br>a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br>b = torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>])<br>c = torch.tensor([<span class="hljs-number">7</span>,<span class="hljs-number">8</span>])<br>d = torch.tensor([<span class="hljs-number">9</span>])<br>train_x = [a, b, c, d]<br><br>data = MyData(train_x)<br>data_loader = DataLoader(data, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=collate_fn)<br><span class="hljs-comment"># 采用默认的 collate_fn 会报错</span><br><span class="hljs-comment">#data_loader = DataLoader(data, batch_size=2, shuffle=True) </span><br>batch_x = <span class="hljs-built_in">iter</span>(data_loader).<span class="hljs-built_in">next</span>()<br></code></pre></div></td></tr></table></figure><p>运行程序，得到 batch_x 的值：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>        [<span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br></code></pre></div></td></tr></table></figure><p>从 batch_x 的值可以看出，第二行填充了三个 0，使其长度和第一行保持一致。</p><p>需要说明的是，对于长度不同的序列，使用默认的 collate_fn函数，不自定义 collate_fn 函数完成对序列的填充，上面的程序就会报错。</p><h2 id="pack_padded_sequence">20、pack_padded_sequence</h2><p><strong>参数</strong></p><p><strong>input</strong>：经过 pad_sequence 处理之后的数据。</p><p><strong>lengths</strong>：mini-batch中各个序列的实际长度。</p><p><strong>batch_first</strong>：True 对应 [batch_size, seq_len,feature] ；</p><p>False 对应 [seq_len, batch_size, feature] 。</p><p><strong>enforce_sorted</strong>：如果是 True，则输入应该是按长度降序排序的序列。如果是 False，会在函数内部进行排序。默认值为 True 。</p><p><strong>说明</strong></p><p>这个 pack 的意思可以理解为压紧或压缩，因为数据在经过填充之后，会有很多冗余的padding_value，所以需要压缩一下。</p><p>为什么要使用这个函数呢？</p><p>RNN 读取数据的方式：网络每次吃进去一组同样时间步 （time step）的数据，也就是 mini-batch 的所有样本中下标相同的数据，然后获得一个mini-batch 的输出；再移到下一个时间步 （time step），再读入 mini-batch中所有该时间步的数据，再输出；直到处理完所有的时间步数据。</p><p>第一个时间步：</p><p><img src="/img/pytorch/12.png" /></p><p>第二个时间步：</p><p><img src="/img/pytorch/13.png" /></p><p>mini-batch 中的 0 只是用来做数据对齐的 padding_value ，如果进行forward 计算时，把 padding_value也考虑进去，可能会导致RNN通过了非常多无用的padding_value，这样不仅浪费计算资源，最后得到的值可能还会存在误差。对于上面的序列2 的数据，通过 RNN 网络：</p><p><img src="/img/pytorch/14.png" /></p><p>实际上从第 2 个时间步开始一直到最后的计算都是多余的，输入都是无效的padding_value 而已。</p><p>从上面的分析可以看出，为了使 RNN 可以高效的读取数据进行训练，就需要在pad 之后再使用 pack_padded_sequence 对数据进行处理。</p><p>需要注意的是，默认条件下，我们必须把输入数据按照序列长度从大到小排列后才能送入pack_padded_sequence ，否则会报错。</p><p><strong>示例</strong></p><p>只需要将上面的例子中的 collate_fn函数稍作修改即可，其余部分保持不变。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>    seq_len = [s.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data] <span class="hljs-comment"># 获取数据真实的长度</span><br>    data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>)    <br>    data = pack_padded_sequence(data, seq_len, batch_first=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> data<br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>PackedSequence(data=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), <br>               batch_sizes=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>               sorted_indices=<span class="hljs-literal">None</span>, unsorted_indices=<span class="hljs-literal">None</span>)<br></code></pre></div></td></tr></table></figure><p>可以看出，输出返回一个 PackedSequence 对象，它主要包含两部分：data 和batch_sizes 。</p><p><img src="/img/pytorch/15.png" /></p><p>填充值 0 就被跳过了。batch_size中的值，实际上就是告诉网络每个时间步需要吃进去多少数据。</p><p>如果仔细看，其实输出的 PackedSequence 对象还包含两个部分sorted_indices 和unsorted_indices 。前面说到 pack_padded_sequence还有一个参数 <strong>enforce_sorted</strong> ，如果是 True，则输入应该是按长度降序排序的序列。如果是 False，会在函数内部进行排序。默认值为 True 。也就是说在输入pack_padded_sequence 前，我们也可以不对数据进行排序。</p><p>现在我们将 enforce_sorted 设置为 False，且输入数据不预先进行排序。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">data = [torch.tensor([<span class="hljs-number">9</span>]), <br>        torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]),<br>        torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])]<br><br>seq_len = [s.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data]<br>data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>)    <br>data = pack_padded_sequence(data, seq_len, batch_first=<span class="hljs-literal">True</span>, enforce_sorted=<span class="hljs-literal">False</span>)<br></code></pre></div></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">PackedSequence(data=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), <br>               batch_sizes=tensor([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>               sorted_indices=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>]), <br>               unsorted_indices=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]))<br></code></pre></div></td></tr></table></figure><p>sorted_indices = tensor([1, 2, 0]，表示排序之后的结果与原始 data 中的tensor 的下标对应关系。1 表示原始 data 中 第 1行最长，排序之后排在最前面，其次是第 2 行、第 0 行。</p><p>假设排序之后的结果为：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">sort_data = [torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]),<br>             torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])<br>             torch.tensor([<span class="hljs-number">9</span>]), <br>             ]<br></code></pre></div></td></tr></table></figure><p>unsorted_indices = tensor([2, 0, 1]，表示未排序前结果。2 表示sort_data 的第 2 行对应 data 中第 0 行；0 表示 sort_data 的第 0 行对应data 中的第 1 行；1 表示 sort_data 的第 1 行对应 data 中的第 2 行。</p><h2 id="pack_sequence">21、pack_sequence</h2><p><strong>参数</strong></p><p><strong>sequences</strong>：输入样本序列，为 list 类型，list中的元素为 tensor ；tensor 的 size 为 L * F，其中，L 为单个序列的长度，F为序列中每个时间步（time step）特征的个数，根据任务的不同 F的维度会有所不同。</p><p><strong>enforce_sorted</strong>：如果是 True，则输入应该是按长度降序排序的序列。如果是 False，会在函数内部进行排序。默认值为 True 。</p><p><strong>说明</strong></p><p>我们看看 PyTorch 中的源码：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">pack_sequence</span>(<span class="hljs-params">sequences, enforce_sorted=<span class="hljs-literal">True</span></span>): <br>lengths = torch.as_tensor([v.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> sequences]) <br><span class="hljs-keyword">return</span> pack_padded_sequence(pad_sequence(sequences),lengths,enforce_sorted=enforce_sorted)<br></code></pre></div></td></tr></table></figure><p>可以看出 pack_sequence 实际上就是对 pad_sequence 和pack_padded_sequence操作的一个封装。通过一个函数完成了两步才能完成的工作。</p><p><strong>示例</strong></p><p>前面的 collate_fn 函数可以进一步修改为：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>   <br>    data = pack_sequence(data)<br>    <span class="hljs-comment">#seq_len = [s.size(0) for s in data]</span><br>    <span class="hljs-comment">#data = pad_sequence(data, batch_first=True)    </span><br>    <span class="hljs-comment">#data = pack_padded_sequence(data, seq_len, batch_first=True)</span><br>    <span class="hljs-keyword">return</span> data<br></code></pre></div></td></tr></table></figure><p>输出结果与前面相同：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>PackedSequence(data=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">9</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>]), <br>               batch_sizes=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>               sorted_indices=<span class="hljs-literal">None</span>, unsorted_indices=<span class="hljs-literal">None</span>)<br></code></pre></div></td></tr></table></figure><h2 id="pad_packed_sequence">22、pad_packed_sequence</h2><p><strong>参数</strong></p><p><strong>sequences</strong>：PackedSequence 对象，将要被填充的 batch；</p><p><strong>batch_first</strong>：一般设置为 True，返回的数据格式为[batch_size, seq_len, feature] ；</p><p><strong>padding_value</strong>：填充值；</p><p><strong>total_length</strong>：如果不是<code>None</code>，输出将被填充到长度：<code>total_length</code>。</p><p><strong>说明</strong></p><p>如果在喂给网络数据的时候，用了 pack_sequence 进行打包，pytorch 的 RNN也会把输出 out 打包成一个 PackedSequence 对象。</p><p>这个函数实际上是 pack_padded_sequence函数的逆向操作。就是把压紧的序列再填充回来。</p><p>为啥要填充回来呢？我的理解是，在 collate_fn 函数里面通常也会调用pad_sequence 对 label 进行填充，RNN 的输出结果为了和 label对齐，需要将压紧的序列再填充回来，方便后续的计算。</p><p><strong>示例</strong></p><p>需要说明的是，下面的程序中，为了产生符合 LSTM 输入格式 [batch_size,seq_len, feature] 的数据，使用了函数 unsqueeze 进行升维处理。其中，batch_size 是<strong>样本数</strong>，seq_len是<strong>序列长度</strong>，feature 是<strong>特征数</strong>。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">MyData</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, data</span>):<br>        self.data = data<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.data)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-keyword">return</span> self.data[idx]<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">data</span>):<br>    data.sort(key=<span class="hljs-keyword">lambda</span> x: <span class="hljs-built_in">len</span>(x), reverse=<span class="hljs-literal">True</span>)<br>    seq_len = [s.size(<span class="hljs-number">0</span>) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data]<br>    data = pad_sequence(data, batch_first=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()    <br>    data = data.unsqueeze(-<span class="hljs-number">1</span>)<br>    data = pack_padded_sequence(data, seq_len, batch_first=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">return</span> data<br><br>a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])<br>b = torch.tensor([<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>])<br>c = torch.tensor([<span class="hljs-number">7</span>,<span class="hljs-number">8</span>])<br>d = torch.tensor([<span class="hljs-number">9</span>])<br>train_x = [a, b, c, d]<br><br>data = MyData(train_x)<br>data_loader = DataLoader(data, batch_size=<span class="hljs-number">2</span>, shuffle=<span class="hljs-literal">True</span>, collate_fn=collate_fn)<br>batch_x = <span class="hljs-built_in">iter</span>(data_loader).<span class="hljs-built_in">next</span>()<br><br>rnn = nn.LSTM(<span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>, batch_first=<span class="hljs-literal">True</span>)<br>h0 = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>()<br>c0 = torch.rand(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>).<span class="hljs-built_in">float</span>()<br>out, (h1, c1) = rnn(batch_x, (h0, c0))<br></code></pre></div></td></tr></table></figure><p>得到 out 的结果如下，是一个 PackedSequence 类型的对象，与前面调用pack_padded_sequence 得到的结果类型相同。</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># out</span><br>PackedSequence(data=tensor([[-<span class="hljs-number">1.3302e-04</span>,  <span class="hljs-number">5.7754e-02</span>,  <span class="hljs-number">4.3181e-02</span>,  <span class="hljs-number">6.4226e-02</span>],<br>        [-<span class="hljs-number">2.8673e-02</span>,  <span class="hljs-number">3.9089e-02</span>, -<span class="hljs-number">2.6875e-03</span>,  <span class="hljs-number">4.2686e-03</span>],<br>        [-<span class="hljs-number">1.0216e-01</span>,  <span class="hljs-number">2.5236e-02</span>, -<span class="hljs-number">1.2230e-01</span>,  <span class="hljs-number">5.1524e-02</span>],<br>        [-<span class="hljs-number">1.6211e-01</span>,  <span class="hljs-number">2.1079e-02</span>, -<span class="hljs-number">1.5849e-01</span>,  <span class="hljs-number">5.2800e-02</span>],<br>        [-<span class="hljs-number">1.5774e-01</span>,  <span class="hljs-number">2.6749e-02</span>, -<span class="hljs-number">1.3333e-01</span>,  <span class="hljs-number">4.7894e-02</span>]],<br>       grad_fn=&lt;CatBackward&gt;), <br>       batch_sizes=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]), <br>       sorted_indices=<span class="hljs-literal">None</span>, unsorted_indices=<span class="hljs-literal">None</span>)<br></code></pre></div></td></tr></table></figure><p>对 out 调用 pad_packed_sequence 进行填充：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python">out_pad, out_len = pad_packed_sequence(out, batch_first=<span class="hljs-literal">True</span>)<br></code></pre></div></td></tr></table></figure><p>out_pad 和 out_len 的结果如下：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># out_pad </span><br>tensor([[[-<span class="hljs-number">1.3302e-04</span>,  <span class="hljs-number">5.7754e-02</span>,  <span class="hljs-number">4.3181e-02</span>,  <span class="hljs-number">6.4226e-02</span>],<br>         [-<span class="hljs-number">1.0216e-01</span>,  <span class="hljs-number">2.5236e-02</span>, -<span class="hljs-number">1.2230e-01</span>,  <span class="hljs-number">5.1524e-02</span>],<br>         [-<span class="hljs-number">1.6211e-01</span>,  <span class="hljs-number">2.1079e-02</span>, -<span class="hljs-number">1.5849e-01</span>,  <span class="hljs-number">5.2800e-02</span>],<br>         [-<span class="hljs-number">1.5774e-01</span>,  <span class="hljs-number">2.6749e-02</span>, -<span class="hljs-number">1.3333e-01</span>,  <span class="hljs-number">4.7894e-02</span>]],<br><br>        [[-<span class="hljs-number">2.8673e-02</span>,  <span class="hljs-number">3.9089e-02</span>, -<span class="hljs-number">2.6875e-03</span>,  <span class="hljs-number">4.2686e-03</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>],<br>         [ <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>,  <span class="hljs-number">0.0000e+00</span>]]],<br>       grad_fn=&lt;TransposeBackward0&gt;)<br><br><span class="hljs-comment"># out_len</span><br>tensor([<span class="hljs-number">4</span>, <span class="hljs-number">1</span>])<br></code></pre></div></td></tr></table></figure><p>再回想下我们调用 pad_sequence 填充之后的输入：</p><figure class="highlight python"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs python"><span class="hljs-comment"># batch_x</span><br>tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],<br>        [<span class="hljs-number">9</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])<br></code></pre></div></td></tr></table></figure><p>这个 out_pad 结果其实就和我们填充之后的输入对应起来了。</p><h2 id="cat">22、cat</h2><p>出了两个张量A和B，分别是2行3列，4行3列。即他们都是2维张量。因为只有两维，这样在用torch.cat拼接的时候就有两种拼接方式：按行拼接和按列拼接。即所谓的维数0和维数1.</p><p>C=torch.cat((A,B),0)就表示按维数0（行）拼接A和B，也就是竖着拼接，A上B下。此时需要注意：列数必须一致，即维数1数值要相同，这里都是3列，方能列对齐。拼接后的C的第0维是两个维数0数值和，即2+4=6</p><p>C=torch.cat((A,B),1)就表示按维数1（列）拼接A和B，也就是横着拼接，A左B右。此时需要注意：行数必须一致，即维数0数值要相同，这里都是2行，方能行对齐。拼接后的C的第1维是两个维数1数值和，即3+4=7</p><h2 id="torch.split">23、torch.split</h2><p><code>torch.split(tensor, split_size_or_sections, dim=0)</code></p><p>torch.split()作用将tensor分成块结构。</p><p>参数：</p><ul><li>tesnor：input，待分输入</li><li>split_size_or_sections：需要切分的大小(int or list )</li><li>dim：切分维度</li><li>output：切分后块结构 &lt;class 'tuple'&gt;</li><li>当split_size_or_sections为<strong>int</strong>时，tenor结构和split_size_or_sections，正好匹配，那么ouput就是大小相同的块结构。如果按照split_size_or_sections结构，tensor不够了，那么就把剩下的那部分做一个块处理。</li><li>当split_size_or_sections为<strong>list</strong>时，那么tensor结构会一共切分成len(list)这么多的小块，每个小块中的大小按照list中的大小决定，其中list中的数字总和应等于该维度的大小，否则会报错（注意这里与split_size_or_sections为int时的情况不同）。</li></ul><h2id="torch.squeeze和torch.unsqueeze">24、torch.squeeze和torch.unsqueeze</h2><p>⼀、先看torch.squeeze()这个函数主要对数据的维度进⾏压缩，去掉维数为1的的维度，⽐如是⼀⾏或者⼀列这种，⼀个⼀⾏三列（1,3）的数去掉第⼀个维数为⼀的维度之后就变成（3）⾏。</p><ul><li>1.squeeze(a)就是将a中所有为1的维度删掉。不为1的维度没有影响。</li><li>2.a.squeeze(N) 就是去掉a中指定的维数为⼀的维度。</li><li>3.还有⼀种形式就是b=torch.squeeze(a，N)a中去掉指定的维数N为⼀的维度。</li></ul><p>⼆、再看torch.unsqueeze()这个函数主要是对数据维度进⾏扩充。</p><ul><li>1.给指定位置加上维数为⼀的维度，⽐如原本有个三⾏的数据（3），在0的位置加了⼀维就变成⼀⾏三列（1,3）。a.unsqueeze(N)就是在a中指定位置N加上⼀个维数为1的维度。</li><li>2.还有⼀种形式就是b=torch.unsqueeze(a，N)a就是在a中指定位置N加上⼀个维数为1的维度</li></ul><h2 id="nn.embedding">25、nn.Embedding</h2><p><code>torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None,max_norm=None,  norm_type=2.0,   scale_grad_by_freq=False, sparse=False,  _weight=None)</code></p><p>其为一个简单的存储固定大小的词典的嵌入向量的查找表，意思就是说，给一个编号，嵌入层就能返回这个编号对应的嵌入向量，嵌入向量反映了各个编号代表的符号之间的语义关系。</p><p>输入为一个编号列表，输出为对应的符号嵌入向量列表。</p><p><strong>参数解释</strong></p><ul><li>num_embeddings (python:int) –词典的大小尺寸，比如总共出现5000个词，那就输入5000。此时index为（0-4999）</li><li>embedding_dim (python:int) –嵌入向量的维度，即用多少维来表示一个符号。</li><li>padding_idx (python:int, optional) –填充id，比如，输入长度为100，但是每次的句子长度并不一样，后面就需要用统一的数字填充，而这里就是指定这个数字，这样，网络在遇到填充id时，就不会计算其与其它符号的相关性。<strong>（初始化为0）</strong></li><li>max_norm (python:float, optional) –最大范数，如果嵌入向量的范数超过了这个界限，就要进行再归一化。</li><li>norm_type (python:float, optional) –指定利用什么范数计算，并用于对比max_norm，默认为2范数。</li><li>scale_grad_by_freq (boolean, optional) –根据单词在mini-batch中出现的频率，对梯度进行放缩。默认为False.</li><li>sparse (bool, optional) –若为True,则与权重矩阵相关的梯度转变为稀疏张量。</li></ul><h2 id="torch.bmm">26、torch.bmm</h2><p>计算两个torch的矩阵乘法</p><p>函数定义：</p><p><code>def bmm(self: Tensor,mat2: Tensor,*,out: Optional[Tensor] = None) -&gt; Tensor</code></p><p>函数的传入参数很简单，两个三维<ahref="https://so.csdn.net/so/search?q=矩阵&amp;spm=1001.2101.3001.7020">矩阵</a>而已，只是要注意这两个矩阵的shape有一些要求：</p><p><code>res = torch.bmm(ma, mb)</code></p><p><code>ma: [a, b, c]</code></p><p><code>mb: [a, c, d]</code></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>线性模型</title>
    <link href="/2022/04/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    <url>/2022/04/26/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="线性模型">线性模型</h1><h2 id="线性回归">1、线性回归</h2><p>假如咱们有一个数据集，里面的数据是俄勒冈州波特兰市的 <spanclass="math inline">\(47\)</span> 套房屋的面积和价格：</p><table><thead><tr class="header"><th style="text-align: center;">居住面积（平方英尺）</th><th style="text-align: center;">价格（千美元）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(2104\)</span></td><td style="text-align: center;"><spanclass="math inline">\(400\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(1600\)</span></td><td style="text-align: center;"><spanclass="math inline">\(330\)</span></td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(2400\)</span></td><td style="text-align: center;"><spanclass="math inline">\(369\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(1416\)</span></td><td style="text-align: center;"><spanclass="math inline">\(232\)</span></td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(3000\)</span></td><td style="text-align: center;"><spanclass="math inline">\(540\)</span></td></tr><tr class="even"><td style="text-align: center;">……</td><td style="text-align: center;">……</td></tr></tbody></table><p><img src="/img/线性模型/1.png" /></p><p>如果来了一个新的面积，假设在销售价钱的记录中没有的，我们怎么办呢？</p><p>我们可以用一条曲线去尽量准的拟合这些数据，然后如果有新的输入过来，我们可以将曲线上这个点对应的值返回。这就是线性回归的思想。</p><p>为了让我们的房屋案例更有意思，咱们稍微对数据集进行一下补充，增加上每一个房屋的卧室数目：</p><table><thead><tr class="header"><th style="text-align: center;">居住面积（平方英尺）</th><th style="text-align: center;">卧室数目</th><th style="text-align: center;">价格（千美元）</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(2104\)</span></td><td style="text-align: center;"><spanclass="math inline">\(3\)</span></td><td style="text-align: center;"><spanclass="math inline">\(400\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(1600\)</span></td><td style="text-align: center;"><spanclass="math inline">\(3\)</span></td><td style="text-align: center;"><spanclass="math inline">\(330\)</span></td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(2400\)</span></td><td style="text-align: center;"><spanclass="math inline">\(3\)</span></td><td style="text-align: center;"><spanclass="math inline">\(369\)</span></td></tr><tr class="even"><td style="text-align: center;"><spanclass="math inline">\(1416\)</span></td><td style="text-align: center;"><spanclass="math inline">\(2\)</span></td><td style="text-align: center;"><spanclass="math inline">\(232\)</span></td></tr><tr class="odd"><td style="text-align: center;"><spanclass="math inline">\(3000\)</span></td><td style="text-align: center;"><spanclass="math inline">\(4\)</span></td><td style="text-align: center;"><spanclass="math inline">\(540\)</span></td></tr><tr class="even"><td style="text-align: center;">……</td><td style="text-align: center;">……</td><td style="text-align: center;">……</td></tr></tbody></table><p>也是同样的方法，此时的自变量就变成了二维向量。</p><p>下面是一个典型的机器学习的过程，首先给出一个输入数据，我们的算法会通过一系列的过程得到一个估计的函数，这个函数有能力对没有见过的新数据给出一个新的估计，也被称为构建一个模型。就如同上面的线性回归函数。</p><p><img src="/img/线性模型/2.png" /></p><p>线性回归假设特征和结果满足线性关系。其实线性关系的表达能力非常强大，每个特征对结果的影响强弱可以有前面的参数体现，而且每个特征变量可以首先映射到一个函数，然后再参与线性计算。这样就可以表达特征与结果之间的非线性关系。</p><p>我们用<span class="math inline">\(x_1\)</span>，<spanclass="math inline">\(x_2\)</span>去描述feature里面的分量，比如 <spanclass="math inline">\(x_1\)</span>=居住面积，<spanclass="math inline">\(x_2\)</span>=卧室数目，我们可以做出一个估计函数：<span class="math display">\[h_\theta  (x) = \theta_0 + \theta_1 \times x_1 + \theta_2 \times x_2\]</span> 简化一下就为： <span class="math display">\[h(x) = \sum^n_{i=0}  \theta_i x_i = \theta^T x\]</span> 我们程序也需要一个机制去评估我们<spanclass="math inline">\(\theta\)</span>是否比较好，所以说需要对我们做出的<spanclass="math inline">\(h\)</span>函数进行评估，一般这个函数称为损失函数（loss function）或者错误函数(errorfunction)，描述<span class="math inline">\(h\)</span>函数不好的程度，在下面，我们称这个函数为<spanclass="math inline">\(J\)</span>函数： <span class="math display">\[J(\theta) = \frac 12 \sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})^2\]</span> 如何调整<span class="math inline">\(\theta\)</span>以使得<spanclass="math inline">\(J(\theta)\)</span>取得最小值有很多方法，其中有最小二乘法(leastsquares method)和梯度下降法等。</p><h3 id="最小二乘法">1.1 最小二乘法</h3><p>给定一个训练集，把<strong>设计矩阵（design matrix）</strong> <spanclass="math inline">\(x\)</span> 设置为一个 <spanclass="math inline">\(m\times n\)</span>矩阵（实际上，如果考虑到截距项，也就是 <spanclass="math inline">\(\theta_0\)</span> 那一项，就应该是 <spanclass="math inline">\(m\times (n+1)\)</span>矩阵），这个矩阵里面包含了训练样本的输入值作为每一行：</p><p><span class="math display">\[X =\begin{bmatrix}(x^{(1)}) ^T\\(x^{(2)}) ^T\\\vdots \\(x^{(m)}) ^T\\\end{bmatrix}\]</span></p><p>然后，咱们设 <span class="math inline">\(\vec{y}\)</span> 是一个<span class="math inline">\(m\)</span> 维向量（m-dimensionalvector），其中包含了训练集中的所有目标值：</p><p><span class="math display">\[Y =\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots \\y^{(m)}\\\end{bmatrix}\]</span></p><p>假设<span class="math inline">\(h_\theta(x_1,x_2,...,x_m)=\theta_0+\theta_1 \times x_1 + \theta_2 \times x_2 + ... +\theta \timesx_m\)</span>​ 的矩阵表达式为 <span class="math display">\[h_\theta(x)=X_\theta\]</span> 损失函数定义为： <span class="math display">\[\frac \partial {\partial\theta_j}J(\theta) = \frac12 (X_\theta - Y) ^ T(X_\theta - T)\]</span> 这时候我们要对这个损失函数的 <spanclass="math inline">\(\theta\)</span> 向量进行求导取0，结果如下式：<span class="math display">\[\theta = (X^T X)^{-1} X^T Y\]</span></p><h3 id="梯度下降算法">1.2 梯度下降算法</h3><p>我们希望选择一个能让 <span class="math inline">\(J(\theta)\)</span>最小的 <span class="math inline">\(\theta\)</span>值。怎么做呢，咱们先用一个搜索的算法，从某一个对 <spanclass="math inline">\(\theta\)</span> 的“初始猜测值”，然后对 <spanclass="math inline">\(\theta\)</span> 值不断进行调整，来让 <spanclass="math inline">\(J(\theta)\)</span>逐渐变小，最好是直到我们能够达到一个使 <spanclass="math inline">\(J(\theta)\)</span> 最小的 <spanclass="math inline">\(\theta\)</span>。具体来说，咱们可以考虑使用梯度下降法（gradientdescent algorithm），这个方法就是从某一个 <spanclass="math inline">\(\theta\)</span>的初始值开始，然后逐渐重复更新：</p><p><span class="math display">\[\theta_j := \theta_j - \alpha \frac \partial {\partial\theta_j}J(\theta)\]</span></p><p>（上面的这个更新要同时对应从 <span class="math inline">\(0\)</span>到 <span class="math inline">\(n\)</span> 的所有<spanclass="math inline">\(j\)</span> 值进行。）这里的 <spanclass="math inline">\(\alpha\)</span>也称为学习速率。这个算法是很自然的，逐步重复朝向 <spanclass="math inline">\(J\)</span> 降低最快的方向移动。</p><p>要实现这个算法，咱们需要解决等号右边的导数项。首先来解决只有一组训练样本<span class="math inline">\((x, y)\)</span>的情况，这样就可以忽略掉等号右边对 <spanclass="math inline">\(J\)</span> 的求和项目了。公式就简化下面这样：</p><p><span class="math display">\[\begin{aligned}\frac \partial {\partial\theta_j}J(\theta) &amp; = \frac \partial{\partial\theta_j} \frac  12(h_\theta(x)-y)^2\\&amp; = 2 \cdot\frac 12(h_\theta(x)-y)\cdot \frac \partial{\partial\theta_j}  (h_\theta(x)-y) \\&amp; = (h_\theta(x)-y)\cdot \frac \partial{\partial\theta_j}(\sum^n_{i=0} \theta_ix_i-y) \\&amp; = (h_\theta(x)-y) x_j\end{aligned}\]</span></p><p>对单个训练样本，更新规则如下所示：</p><p><span class="math display">\[\theta_j := \theta_j + \alpha (y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}\]</span></p><p>第一种就是下面这个算法：</p>$<span class="math display">\[\begin{aligned}&amp;\qquad 重复直到收敛 \{ \\&amp;\qquad\qquad\theta_j := \theta_j + \alpha\sum^m_{i=1}(y^{(i)}-h_\theta (x^{(i)}))x_j^{(i)}\quad(对每个j) \\&amp;\qquad\}\end{aligned}\]</span><p>$</p><p>读者很容易能证明，在上面这个更新规则中求和项的值就是<spanclass="math inline">\(\frac {\partial J(\theta)}{\partial\theta_j}\)</span> （这是因为对 <span class="math inline">\(J\)</span>的原始定义）。所以这个更新规则实际上就是对原始的成本函数<spanclass="math inline">\(J\)</span>进行简单的梯度下降。这一方法在每一个步长内检查所有整个训练集中的所有样本，也叫做<strong>批量梯度下降法（batchgradientdescent</strong>）。这里要注意，因为梯度下降法容易被局部最小值影响，而我们要解决的这个线性回归的优化问题只能有一个全局的而不是局部的最优解；因此，梯度下降法应该总是收敛到全局最小值（假设学习速率<span class="math inline">\(\alpha\)</span> 不设置的过大）。</p><p>对咱们之前的房屋数据集进行批量梯度下降来拟合 <spanclass="math inline">\(\theta\)</span>，把房屋价格当作房屋面积的函数来进行预测，我们得到的结果是 <spanclass="math inline">\(\theta_0 = 71.27, \theta_1 =0.1345\)</span>。如果把 <spanclass="math inline">\(h_{\theta}(x)\)</span> 作为一个定义域在 <spanclass="math inline">\(x\)</span>上的函数来投影，同时也投上训练集中的已有数据点，会得到下面这幅图：</p><p><img src="/img/线性模型/3.png" /></p><p>如果在数据集中添加上卧室数目作为输入特征，那么得到的结果就是 <spanclass="math inline">\(\theta_0 = 89.60, \theta_1 = 0.1392, \theta_2 =−8.738\)</span>​。这个结果就是用批量梯度下降法来获得的。</p><p>此外还有另外一种方法能够替代批量梯度下降法，这种方法效果也不错。如下所示：</p>$<span class="math display">\[\begin{aligned}&amp;\qquad循环：\{ \\&amp;\qquad\qquad i从1到m,\{   \\&amp;\qquad\qquad\qquad\theta_j :=\theta_j  +\alpha(y^{(i)}-h_{\theta}(x^{(i)}))x_j^{(i)} \qquad(对每个 j)\\&amp;\qquad\qquad\}  \\&amp;\qquad\}\end{aligned}\]</span><p>$</p><p>在这个算法里，我们对整个训练集进行了循环遍历，每次遇到一个训练样本，根据每个单一训练样本的误差梯度来对参数进行更新。这个算法叫做<strong>随机梯度下降法（stochasticgradient descent）</strong>，或者叫<strong>增量梯度下降法（incrementalgradientdescent）</strong>。批量梯度下降法要在运行第一步之前先对整个训练集进行扫描遍历，当训练集的规模<span class="math inline">\(m\)</span>变得很大的时候，引起的性能开销就很不划算了；随机梯度下降法就没有这个问题，而是可以立即开始，对查询到的每个样本都进行运算。通常情况下，随机梯度下降法查找到足够接近最低值的<span class="math inline">\(\theta\)</span>的速度要比批量梯度下降法更快一些。（也要注意，也有可能会一直无法收敛（converge）到最小值，这时候<span class="math inline">\(\theta\)</span> 会一直在 <spanclass="math inline">\(J(\theta)\)</span>最小值附近震荡；不过通常情况下在最小值附近的这些值大多数其实也足够逼近了，足以满足咱们的精度要求，所以也可以用。）由于这些原因，特别是在训练集很大的情况下，随机梯度下降往往比批量梯度下降更受青睐。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
  </entry>
  
  
  
  
</search>
